set.seed(131)
x = rnorm(100)
y = 3 * x^2 + 4 + rnorm(100)
train = sample(100, 50)
y[train] = y[train] + 3
y[-train] = y[-train] - 3
# Plot using different colors
plot(x[train], y[train], pch="+", lwd=4, col="red", ylim=c(-4, 20), xlab="X", ylab="Y")
points(x[-train], y[-train], pch="o", lwd=4, col="blue")
set.seed(131)
x = rnorm(100)
y = 3 * x^3 + 4 + rnorm(100)
train = sample(100, 50)
y[train] = y[train] + 3
y[-train] = y[-train] - 3
# Plot using different colors
plot(x[train], y[train], pch="+", lwd=4, col="red", ylim=c(-4, 20), xlab="X", ylab="Y")
points(x[-train], y[-train], pch="o", lwd=4, col="blue")
set.seed(131)
x = rnorm(100)
y = 7 * x^2 + 4 + rnorm(100)
train = sample(100, 50)
y[train] = y[train] + 3
y[-train] = y[-train] - 3
# Plot using different colors
plot(x[train], y[train], pch="+", lwd=4, col="red", ylim=c(-4, 20), xlab="X", ylab="Y")
points(x[-train], y[-train], pch="o", lwd=4, col="blue")
set.seed(131)
x = rnorm(100)
y = 7 * x^2 + rnorm(100)
train = sample(100, 50)
y[train] = y[train] + 3
y[-train] = y[-train] - 3
# Plot using different colors
plot(x[train], y[train], pch="+", lwd=4, col="red", ylim=c(-4, 20), xlab="X", ylab="Y")
points(x[-train], y[-train], pch="o", lwd=4, col="blue")
set.seed(131)
x = rnorm(100)
y = 7 * x^2 + rnorm(100)
train = sample(100, 50)
y[train] = y[train] + 3
y[-train] = y[-train] - 3
plot(x[train], y[train], pch="+", lwd=4, col="red", ylim=c(-4, 20), xlab="X", ylab="Y")
points(x[-train], y[-train], pch="o", lwd=4, col="blue")
set.seed(1)
x = rnorm(100)
y = 7*x^2 + rnorm(100)
train = sample(100, 50)
y[train] = y[train] + 3
y[-train] = y[-train] - 3
plot(x[train], y[train], pch="+", lwd=4, col="red", ylim=c(-4, 20), xlab="X", ylab="Y")
points(x[-train], y[-train], pch="o", lwd=4, col="blue")
set.seed(1)
x = rnorm(100)
y = 7*x^2 + rnorm(100)
train = sample(100, 50)
y[train] = y[train] + 3
y[-train] = y[-train] - 3
plot(x[train], y[train], pch="+", lwd=4, col="red", ylim=c(-4, 20), xlab="X", ylab="Y")
points(x[-train], y[-train], pch="o", lwd=4, col="blue")
set.seed(1)
x = rnorm(100)
y = 7*x^2 + rnorm(100)
train = sample(100, 50)
y[train] = y[train] + 3
y[-train] = y[-train] - 3
plot(x[train], y[train], pch="+", lwd=4, col="red", ylim=c(-4, 20), xlab="X", ylab="Y")
points(x[-train], y[-train], pch="o", lwd=4, col="blue")
library(e1071)
# train set
set.seed(1)
x = rnorm(100)
y = 7*x^2 + rnorm(100)
train = sample(100, 50)
y[train] = y[train] + 3
y[-train] = y[-train] - 3
plot(x[train], y[train], pch="+", lwd=4, col="red", ylim=c(-4, 20), xlab="X", ylab="Y")
points(x[-train], y[-train], pch="o", lwd=4, col="blue")
# train vs test ->z 생성
set.seed(315)
z = rep(0, 100)
z[train] = 1
final.train = c(sample(train, 25), sample(setdiff(1:100, train), 25))
data.train = data.frame(x=x[final.train], y=y[final.train], z=as.factor(z[final.train]))
data.test = data.frame(x=x[-final.train], y=y[-final.train], z=as.factor(z[-final.train]))
library(e1071)
# train set
set.seed(1)
x = rnorm(100)
y = 7*x^2 + rnorm(100)
train = sample(100, 50)
y[train] = y[train] + 3
y[-train] = y[-train] - 3
plot(x[train], y[train], pch="+", lwd=4, col="red", ylim=c(-4, 20), xlab="X", ylab="Y")
points(x[-train], y[-train], pch="o", lwd=4, col="blue")
# split variable z
set.seed(315)
z = rep(0, 100)
z[train] = 1
final.train = c(sample(train, 25), sample(setdiff(1:100, train), 25))
# train, test
data.train = data.frame(x=x[final.train], y=y[final.train], z=as.factor(z[final.train]))
data.test = data.frame(x=x[-final.train], y=y[-final.train], z=as.factor(z[-final.train]))
# linear kernel
svm.linear = svm(z~., data=data.train, kernel="linear", cost=10)
plot(svm.linear, data.train)
table(z[final.train], predict(svm.linear, data.train))
library(e1071)
# train set
set.seed(1)
x = rnorm(100)
y = 7*x^2 + rnorm(100)
train = sample(100, 50)
y[train] = y[train] + 3
y[-train] = y[-train] - 3
plot(x[train], y[train], pch="+", lwd=4, col="red", ylim=c(-4, 20), xlab="X", ylab="Y")
points(x[-train], y[-train], pch="o", lwd=4, col="blue")
# split variable z
set.seed(315)
z = rep(0, 100)
z[train] = 1
final.train = c(sample(train, 25), sample(setdiff(1:100, train), 25))
# train, test
data.train = data.frame(x=x[final.train], y=y[final.train], z=as.factor(z[final.train]))
data.test = data.frame(x=x[-final.train], y=y[-final.train], z=as.factor(z[-final.train]))
# linear kernel
svm.linear = svm(z~., data=data.train, kernel="linear", cost=10)
plot(svm.linear, data.train)
linear.table = table(z[final.train], predict(svm.linear, data.train))
linear.table[1,2] + linear.table[2,1]
library(e1071)
# train set
set.seed(1)
x = rnorm(100)
y = 7*x^2 + rnorm(100)
train = sample(100, 50)
y[train] = y[train] + 3
y[-train] = y[-train] - 3
plot(x[train], y[train], pch="+", lwd=4, col="red", ylim=c(-4, 20), xlab="X", ylab="Y")
points(x[-train], y[-train], pch="o", lwd=4, col="blue")
# split variable z
set.seed(315)
z = rep(0, 100)
z[train] = 1
final.train = c(sample(train, 25), sample(setdiff(1:100, train), 25))
# train, test
data.train = data.frame(x=x[final.train], y=y[final.train], z=as.factor(z[final.train]))
data.test = data.frame(x=x[-final.train], y=y[-final.train], z=as.factor(z[-final.train]))
### kernel ###
# linear kernel
svm.linear = svm(z~., data=data.train, kernel="linear", cost=10)
plot(svm.linear, data.train)
linear.table = table(z[final.train], predict(svm.linear, data.train))
linear.table[1,2] + linear.table[2,1] # number of error
# polynomial kernel
set.seed(1)
svm.poly = svm(z~., data=data.train, kernel="polynomial", cost=10)
plot(svm.poly, data.train)
poly.table = table(z[final.train], predict(svm.poly, data.train))
poly.table[1,2] + poly.table[2,1] # number of error
# radial kernel
set.seed(1)
svm.radial = svm(z~., data=data.train, kernel="radial", gamma=1, cost=10)
plot(svm.radial, data.train)
radial.table = table(z[final.train], predict(svm.radial, data.train))
radial.table[1,2] + radial.table[2,1] # number of error
library(e1071)
# train set
set.seed(1)
x = rnorm(100)
y = 7*x^2 + rnorm(100)
train = sample(100, 50)
y[train] = y[train] + 3
y[-train] = y[-train] - 3
plot(x[train], y[train], pch="+", lwd=4, col="red", ylim=c(-4, 20), xlab="X", ylab="Y")
points(x[-train], y[-train], pch="o", lwd=4, col="blue")
# split variable z
set.seed(315)
z = rep(0, 100)
z[train] = 1
final.train = c(sample(train, 25), sample(setdiff(1:100, train), 25))
# train, test
data.train = data.frame(x=x[final.train], y=y[final.train], z=as.factor(z[final.train]))
data.test = data.frame(x=x[-final.train], y=y[-final.train], z=as.factor(z[-final.train]))
### train set - 3 kernel fit ###
# linear kernel
svm.linear = svm(z~., data=data.train, kernel="linear", cost=10)
plot(svm.linear, data.train)
linear.table = table(z[final.train], predict(svm.linear, data.train))
linear.table[1,2] + linear.table[2,1] # number of error
# polynomial kernel
set.seed(1)
svm.poly = svm(z~., data=data.train, kernel="polynomial", cost=10)
plot(svm.poly, data.train)
poly.table = table(z[final.train], predict(svm.poly, data.train))
poly.table[1,2] + poly.table[2,1] # number of error
# radial kernel
set.seed(1)
svm.radial = svm(z~., data=data.train, kernel="radial", gamma=1, cost=10)
plot(svm.radial, data.train)
radial.table = table(z[final.train], predict(svm.radial, data.train))
radial.table[1,2] + radial.table[2,1] # number of error # perfect !!!!
### test set - 3 kernel fit ###
plot(svm.linear, data.test)
test1 = table(z[-final.train], predict(svm.linear, data.test))
test1[1,2] + test1[2,1]
plot(svm.poly, data.test)
test2 = table(z[-final.train], predict(svm.poly, data.test))
test2[1,2] + test2[2,1]
plot(svm.radial, data.test)
test3 = table(z[-final.train], predict(svm.radial, data.test))
test3[1,2] + test3[2,1]
library(e1071)
### data setting ###
# train set
set.seed(1)
x = rnorm(100)
train = sample(100, 50)
y[train] = y[train] + 3
y[-train] = y[-train] - 3
points(x[-train], y[-train], pch="o", lwd=4, col="blue")
# split variable z
set.seed(315)
z = rep(0, 100)
z[train] = 1
final.train = c(sample(train, 25), sample(setdiff(1:100, train), 25))
# train, test
data.train = data.frame(x=x[final.train], y=y[final.train], z=as.factor(z[final.train]))
data.test = data.frame(x=x[-final.train], y=y[-final.train], z=as.factor(z[-final.train]))
### train set - 3 kernel fit ###
# linear kernel
svm.linear = svm(z~., data=data.train, kernel="linear", cost=10)
plot(svm.linear, data.train)
linear.table = table(z[final.train], predict(svm.linear, data.train))
linear.table
linear.table[1,2] + linear.table[2,1] # number of error
# polynomial kernel
set.seed(1)
plot(svm.poly, data.train)
poly.table = table(z[final.train], predict(svm.poly, data.train))
poly.table
poly.table[1,2] + poly.table[2,1] # number of error
svm.radial = svm(z~., data=data.train, kernel="radial", gamma=1, cost=10)
# a
set.seed(1)
x1 = runif(500) - 0.5
x2 = runif(500) - 0.5
y = 1 * (x1^2 - x2^2 > 0)
# b
plot(x1[y == 0], x2[y == 0], col = "red", xlab = "X1", ylab = "X2", pch = "+")
points(x1[y == 1], x2[y == 1], col = "blue", pch = 4)
# c
lm.fit = glm(y ~ x1 + x2, family = binomial)
summary(lm.fit)
# d
# e
# f
# g
# h
summary(lm.fit)
# a
set.seed(1)
x1 = runif(500) - 0.5
x2 = runif(500) - 0.5
y = 1 * (x1^2 - x2^2 > 0)
# b
plot(x1[y == 0], x2[y == 0], col = "red", xlab = "X1", ylab = "X2", pch = "+")
points(x1[y == 1], x2[y == 1], col = "blue", pch = 4)
# non-linear !!!!
# c
lm.fit = glm(y ~ x1 + x2, family = binomial)
summary(lm.fit)
# x1, x2 : insignificant!!!
# d
ata = data.frame(x1 = x1, x2 = x2, y = y)
lm.prob = predict(lm.fit, data, type = "response")
lm.prob = predict(lm.fit, data, type = "response")
?predict
# a
set.seed(1)
x1 = runif(500) - 0.5
x2 = runif(500) - 0.5
y = 1 * (x1^2 - x2^2 > 0)
# b
plot(x1[y == 0], x2[y == 0], col = "red", xlab = "X1", ylab = "X2", pch = "+")
points(x1[y == 1], x2[y == 1], col = "blue", pch = 4)
# non-linear !!!!
# c
lm.fit = glm(y ~ x1 + x2, family = binomial)
summary(lm.fit)
# x1, x2 : insignificant!!!
# d
ata = data.frame(x1 = x1, x2 = x2, y = y)
lm.prob = predict(lm.fit, data)
# a
set.seed(1)
x1 = runif(500) - 0.5
x2 = runif(500) - 0.5
y = 1 * (x1^2 - x2^2 > 0)
# b
plot(x1[y == 0], x2[y == 0], col = "red", xlab = "X1", ylab = "X2", pch = "+")
points(x1[y == 1], x2[y == 1], col = "blue", pch = 4)
# non-linear !!!!
# c
lm.fit = glm(y ~ x1 + x2, family = binomial)
summary(lm.fit)
# x1, x2 : insignificant!!!
# d
data = data.frame(x1 = x1, x2 = x2, y = y)
lm.prob = predict(lm.fit, data, type = "response")
lm.pred = ifelse(lm.prob > 0.5, 1, 0)
data.pos = data[lm.pred == 1, ]
data.neg = data[lm.pred == 0, ]
plot(data.pos$x1, data.pos$x2, col = "blue", xlab = "X1", ylab = "X2", pch = "+")
points(data.neg$x1, data.neg$x2, col = "red", pch = 4)
# e
# f
# g
# h
set.seed(1)
x1 = runif(500) - 0.5
x2 = runif(500) - 0.5
y = 1 * (x1^2 - x2^2 > 0)
# b
plot(x1[y == 0], x2[y == 0], col = "red", xlab = "X1", ylab = "X2", pch = "+")
points(x1[y == 1], x2[y == 1], col = "blue", pch = 4)
# non-linear !!!!
# c
lm.fit = glm(y ~ x1 + x2, family = binomial)
summary(lm.fit)
# x1, x2 : insignificant!!!
data = data.frame(x1 = x1, x2 = x2, y = y)
lm.prob = predict(lm.fit, data, type = "response")
lm.pred = ifelse(lm.prob > 0.5, 1, 0)
data.pos = data[lm.pred == 1, ]
data.neg = data[lm.pred == 0, ]
plot(data.pos$x1, data.pos$x2, col = "blue", xlab = "X1", ylab = "X2", pch = "+")
points(data.neg$x1, data.neg$x2, col = "red", pch = 4)
data.pos
data.neg
data = data.frame(x1 = x1, x2 = x2, y = y)
lm.prob = predict(lm.fit, data, type = "response")
lm.pred = ifelse(lm.prob > 0.7, 1, 0)
data.pos = data[lm.pred == 1, ]
data.neg = data[lm.pred == 0, ]
plot(data.pos$x1, data.pos$x2, col = "blue", xlab = "X1", ylab = "X2", pch = "+")
# d
data = data.frame(x1 = x1, x2 = x2, y = y)
lm.pred = ifelse(lm.prob > 0.7, 1, 0)
data.pos = data[lm.pred == 1, ]
data.neg = data[lm.pred == 0, ]
plot(data.pos$x1, data.pos$x2, col = "blue", xlab = "X1", ylab = "X2", pch = "+")
?plot
# a
library(ISLR)
set.seed(1)
train = sample(dim(OJ)[1], 800)
OJ.train = OJ[train, ]
OJ.test = OJ[-train, ]
# b
library(e1071)
svm.linear = svm(Purchase ~ ., kernel = "linear", data = OJ.train, cost = 0.01)
summary(svm.linear)
# c
# d
# e
# f
# g
# h
library(ISLR)
set.seed(1)
train = sample(dim(OJ)[1], 800)
OJ.train = OJ[train, ]
OJ.test = OJ[-train, ]
# b
library(e1071)
svm.linear = svm(Purchase ~ ., kernel = "linear", data = OJ.train, cost = 0.01)
summary(svm.linear)
# 432개 support vector 생성, CH:215개  MM:217개
# c
train.pred = predict(svm.linear, OJ.train)
test.pred = predict(svm.linear, OJ.test)
t1 =table(OJ.train$Purchase, train.pred)
t2 =table(OJ.test$Purchase, test.pred)
(t1[1,2] + t1[2,1])/sum(t1)
set.seed(1)
tune.out = tune(svm, Purchase ~ ., data = OJ.train, kernel = "linear",
ranges = list(cost = 10^seq(-2,1, by = 0.25)))
summary(tune.out)
write.csv ( round(summary(lm.fit)$coef,2), "C:/Users/jeeyeon/Desktop/데마/HW8/m1.csv")
summary(lm.fit)$coef
summary(lm.fit)$coefficient
lm.fit = glm(y ~ x1 + x2, family = binomial)
summary(lm.fit)
set.seed(400)
x1 = runif(500) - 0.5
x2 = runif(500) - 0.5
y = 1 * (x1^2 - x2^2 > 0)
# b
plot(x1[y == 0], x2[y == 0], col = "red", xlab = "X1", ylab = "X2", pch = "+")
points(x1[y == 1], x2[y == 1], col = "blue", pch = 4)
# non-linear !!!!
# c
lm.fit = glm(y ~ x1 + x2, family = binomial)
summary(lm.fit)
a = summary(lm.fit)
a$coefficients
write.csv ( round(summary(lm.fit)$coefficients,2), "C:/Users/jeeyeon/Desktop/데마/HW8/m1.csv")
lm.fit = glm(y ~ poly(x1, 2) + poly(x2, 2) + I(x1 * x2) + log(x2), data = data, family = binomial)
summary(lm.fit)
write.csv ( round(summary(lm.fit)$coefficients,2), "C:/Users/jeeyeon/Desktop/데마/HW8/m1.csv")
write.csv ( round(summary(lm.fit)$coefficients,2), "C:/Users/jeeyeon/Desktop/데마/HW8/m2.csv")
# e
lm.fit = glm(y ~ poly(x1, 2) + poly(x2, 2) + I(x1 * x2) + log(x2), data = data, family = binomial)
summary(lm.fit)
# e
lm.fit = glm(y ~ poly(x1, 2) + poly(x2, 2) + I(x1 * x2) + log(x2), data = data, family = binomial)
summary(lm.fit)
# e
lm.fit = glm(y ~ poly(x1, 2) + poly(x2, 2) + I(x1 * x2) + log(x2), data = data, family = binomial)
# a
set.seed(400)
x1 = runif(500) - 0.5
x2 = runif(500) - 0.5
y = 1 * (x1^2 - x2^2 > 0)
# b
plot(x1[y == 0], x2[y == 0], col = "red", xlab = "X1", ylab = "X2", pch = "+")
points(x1[y == 1], x2[y == 1], col = "blue", pch = 4)
# non-linear !!!!
# c
lm.fit = glm(y ~ x1 + x2, family = binomial)
summary(lm.fit)
# x1, x2 : insignificant!!!
# d
data = data.frame(x1 = x1, x2 = x2, y = y)
lm.prob = predict(lm.fit, data, type = "response")
lm.pred = ifelse(lm.prob > 0.5, 1, 0)
data.pos = data[lm.pred == 1, ]
data.neg = data[lm.pred == 0, ]
plot(data.pos$x1, data.pos$x2, col = "blue", xlab = "X1", ylab = "X2", pch = "+")
points(data.neg$x1, data.neg$x2, col = "red", pch = 4)
# linear boundary!!!
# e
lm.fit = glm(y ~ poly(x1, 2) + poly(x2, 2) + I(x1 * x2) + log(x2), data = data, family = binomial)
summary(lm.fit)
write.csv ( round(summary(lm.fit)$coefficients,2), "C:/Users/jeeyeon/Desktop/데마/HW8/m3.csv")
