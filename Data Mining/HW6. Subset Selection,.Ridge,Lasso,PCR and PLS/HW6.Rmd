---
title: "HW6"
author: "jy"
date: "2019년 4월 22일"
output: html_document
---

### Chaper 6 Lab1: Subset Selection Methods

```{r setup}
library(ISLR)
library(MASS)
library(tidyverse)
library(leaps)
library(glmnet)  
library(pls)

options(scipen = 100)

```

# 6.5.1 Best Subset Selection

Hitters 데이터를 사용하여 야구선수의 Salary를 예측해보자

```{r }

# fix(Hitters)
names(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary)) # Salary 값이 누락된 행 확인

Hitters = na.omit(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary)) # Salary 값 누락된 행 제거 확인

```

```{r}

regfit.full = regsubsets(Salary ~ ., data = Hitters) # regsubsets :Best subset selection
summary(regfit.full)

regfit.full = regsubsets(Salary ~ ., data = Hitters, nvmax = 19) 
reg.summary = summary(regfit.full)

```

위 결과는 변수가 8개까지 포함된 모델만 보여줌 
"*"는 그 해당 변수가 포함된다는 의미임 
2 행은 2개 변수를 사용하여 적합한 모델을 의미하고, 이 때 Hits, CRBI 변수만 포함되어 있음을 의미함 
nvmax option 사용하면 원하는 변수만큼 포함된 결과가 출력됨 

```{r}
names(reg.summary)
reg.summary$rsq # R-suqare

par(mfrow= c(2,2))
plot(reg.summary$rss, xlab = "Number of Variables", ylab = "RSS",type = "l") # 단조감소 
minloc_rss = which.min(reg.summary$rss) 
points(minloc_rss, reg.summary$rss[minloc_rss], 
       col = "red", cex = 2, pch = 20) 

plot(reg.summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted R-square",type = "l")  # 단조증가 
maxloc_adjr2 = which.max(reg.summary$adjr2) # Adjusted R-square 값이 가장 최대가 되는 지점
points(maxloc_adjr2, reg.summary$adjr2[maxloc_adjr2], 
       col = "red", cex = 2, pch = 20)  

plot(reg.summary$cp, xlab = "Number of Variables", ylab = "Cp",type = "l")
minloc_cp = which.min(reg.summary$cp) 
points(minloc_cp, reg.summary$cp[minloc_cp], 
       col = "red", cex = 2, pch = 20)  

plot(reg.summary$bic, xlab = "Number of Variables", ylab = "Bic",type = "l")
minloc_bic = which.min(reg.summary$bic) 
points(minloc_bic, reg.summary$bic[minloc_bic], 
       col = "red", cex = 2, pch = 20)  
par(mfrow= c(1,1))

```

points 함수 : 이미 생성된 그래프에 점을 그려 넣을 수 있음 


```{r}

plot(regfit.full, scale = "r2") # r2 기준에 따라 포함되는 변수를 알려줌 
plot(regfit.full, scale = "adjr2")
plot(regfit.full, scale = "Cp")
plot(regfit.full, scale = "bic")# bic 기준 가장 낮은 값을 가지는 변수는 그래프의 맨 위 6개 변수만을 포함함 

coef(regfit.full, 6)

```

regsubset 함수는 plot 내장되어 있으며, 어떤 변수가 포함되어 있는지를 나타내 줌 ?plot.regsebsets 

# 6.5.2 Forward and Backward Stepwise Selection

```{r}

regfit.fwd = regsubsets(Salary ~., data = Hitters, nvmax = 19, method = "forward")
summary(regfit.fwd)
regfit.bwd = regsubsets(Salary ~., data = Hitters, nvmax = 1, method = "backward")
summary(regfit.bwd)

# p = 7 비교  
coef(regfit.full, 7)
coef(regfit.fwd, 7)
coef(regfit.bwd, 7)

```

regsubsets() 함수에서 method = "forward" 또는 "backward"옵션 주면 됨 
forward, backward 해봤을 때 p =6 까지는 결과 같은데 p= 7부터 달라지므로 비교해보기 위해 coef 함수 사용해서 모형확인 

# 6.5.3 Choosing Among Models Using the Validation Set Approach and Cross-Validation

```{r}

# 1. train vs test 분할 
  set.seed(1)
  train = sample(c(TRUE, FALSE), nrow(Hitters), rep = TRUE) # replace = TRUE 복원추출
  sum(train)/ nrow(Hitters) # 1:1 로 분할
  test = (!train) # 반대 값

# 2. Best subeset selction 이용한 모델적합 
  regfit.best = regsubsets(Salary ~., data = Hitters[train,], nvmax = 19) # train set 만 적합한 모델 
  test.mat = model.matrix(Salary~., data = Hitters[test,]) # model.matrix : 데이터로부터 X 행렬 구성함 
  head(test.mat) # 만들어진 X matrix 확인  

# 3. for 문 이용해서 p 갯수 별로 best 모델 선택

  # 쉽게 이해가능하도록 p = 3일때 best model 에 대한 test MSE 를 구하면 ? 
  temp_coef = coef(regfit.best, id = 3) # id 옵션 이용해서 p 결정해 줄 수 있음, coef 값 저장 
  temp_pred = test.mat[,names(temp_coef)] %*% temp_coef 
  temp_pred # test 데이터랑 training 으로 부터 만들어진 coef값이랑 곱하면 각 row당 하나의 예측치가 구해짐 즉 y_hat 값  
  sum((Hitters$Salary[test] - temp_pred)^2) # test MSE
  
  # for 문 이용해서 모든 p 에 대해 error 구함
  val.errors = rep(NA, 19)
  for(i in 1 :19){
    coefi = coef(regfit.best, id = i)
    pred = test.mat[,names(coefi)] %*% coefi
    val.errors[i] = mean((Hitters$Salary[test] - pred)^2)
  }
  val.errors
  
  val.errors %>% min
  which.min(val.errors)
  
  coef(regfit.best, 10) # regsubsets 함수에 대한 predict 함수가 없어서 이렇게 구함 

# 4. Cross- Validation 
    
    predict.regsubsets = function(object, newdata, id, ...) {
      form = as.formula(object$call[[2]])
      mat = model.matrix(form, newdata)
      coefi = coef(object, id = id)
      mat[, names(coefi)] %*% coefi
    }
    
  k = 10
  set.seed(1)
  folds = sample(1:k, nrow(Hitters), replace = TRUE) # k개로 쪼개기 위해서 1~k까지 숫자 생성함 
  table(folds)

  # 쉽게 이해가능하도록 K=10일 때 그 중 3번째 그룹을 test 로 사용하는 경우 
  temp_best.fit = regsubsets(Salary ~., data =Hitters[folds!=3,], nvmax =19) #folds=3인 경우는test 니까 모델적합x
  temp_pred = predict(temp_best.fit, Hitters[folds == 3,], id =2)
  temp_cv.errors = mean((Hitters$Salary[folds == 3] - temp_pred)^2)
  
  
  # for 문 돌려보자 
  cv.errors = matrix(NA,k,19, dimnames = list(NULL, paste(1:19))) # NA 값을 행렬에 넣고 차원은 k행 19열
                                                                  # dimnames = list(행이름, 열이름)
  for(j in 1:k){
    
    best.fit = regsubsets(Salary ~., data =Hitters[folds!=j,], nvmax =19) #folds=j인 경우는test 니까 모델적합x
    
    for(i in 1:19){ # p 달리해가면서 best model 찾기 
    pred = predict(best.fit, Hitters[folds == j,], id =i)
    cv.errors[j,i] = mean( (Hitters$Salary[folds==j]-pred )^2)
    }
  }

  cv.errors
  
  mean.cv.errors = apply(cv.errors,2,mean) # p 에 따른 testMSE 평균 
  mean.cv.errors
  par(mfrow = c(1,1))
  plot(mean.cv.errors, type = "b")
  # p = 11 일 때가 가장 작은 error 가짐 !! 
  
  reg.best = regsubsets(Salary ~., data =Hitters, nvmax = 19)
  coef(reg.best, 11)

```

### Chaper 6 Lab 2: Ridge Regression and the Lasso

# 6.6.1 Ridge Regression -> glmnet(alpha=0)
```{r}

  x = model.matrix(Salary ~., Hitters)[,-1] # model.matrix 함수는 각각의 설명변수들에 대응하여 행렬 만들어주고,  질적변수를 가변수로 변환함
  y = Hitters$Salary
  
  library(glmnet)
  grid = 10^seq(10, -2, length = 100) 
  ridge.mod = glmnet(x,y,alpha = 0, lambda = grid) # alpha=0: Ridge, alpha=1 : Lasso 
                # glmnet 함수는 변수를 자동 scaling 함 ! standardize = FALSE 하면 끌 수 있음 
  dim( coef(ridge.mod) )


# lambda 값에 따라서 l2_norm 값 비교  
  ridge.mod$lambda[50] 
  coef(ridge.mod)[,50] # lambda = ridge.mod$lambda[50]= 11497 일 때 계수 
  sqrt( sum (coef(ridge.mod)[-1,50]^2 ) ) # lambda = ridge.mod$lambda[50]= 11497 일 때 l2 norm
  
  ridge.mod$lambda[60] 
  coef(ridge.mod)[,60] # lambda = ridge.mod$lambda[60]= 705 일 때 계수 
  sqrt( sum (coef(ridge.mod)[-1,60]^2 ) ) # lambda = ridge.mod$lambda[60]= 705 일 때 l2 norm
  # labmda 값이 작을 때 l2 norm 값이 훨 크다 

  predict(ridge.mod, s = 50, type = "coefficients") 
  predict(ridge.mod, s = 50, type = "coefficients")[1:20,] # lambda = 50 에 대한 coefficient 

# Cross- Validation
  # 1:n 사이 숫자를 random 으로 선택해서 index 생성함 
  set.seed(1)
  train = sample(1:nrow(x), nrow(x)/2)
  test = (-train)
  y.test = y[test]
  
  ## train data set 이용해 적합 
  ridge.mod = glmnet(x[train,], y[train], alpha = 0, lambda = grid, thresh =1e-12)
  
  #1
  ridge.pred = predict(ridge.mod, s=4, newx = x[test,]) # lambda = 4 , newx 바꿈 
  mean( (ridge.pred - y.test)^2 )  # MSE lambda = 4
  #2
  mean( (mean(y[train]) - y.test )^2 )  # E[ (Y_bar_train - Y_bar_test)^2 ] # 절편만 있는 회귀에서는 y_hat = mean(y)
  #3
  ridge.pred = predict(ridge.mod, s = 1e10, newx = x[test,]) 
  mean( (ridge.pred - y.test)^2 )  # MSE lambda = 1e10(매우큰 값) #2 와 같은 결과
  
  # 결과를 살펴보면 #1 처럼 lambda = 4 일때 MSE 가 가장 작음 
  
# LSE model(lambda =0) 대신에 위와 같이 lambda = 4 인 Ridge Regression 을 이용하면 어떤 장점이 있을까?   
  
  ridge.pred=predict(ridge.mod,s=0,newx=x[test,],exact=T,x=x[train,],y=y[train])
  mean((ridge.pred - y.test)^2)
  
  lm(y~x, subset = train)$coefficients
  predict(ridge.mod, s=0, exact=T, type = "coefficients", x=x[train,],y=y[train])[1:20,]
  
# lambda = 4말고 최적의 lambda 를 찾아보자 : cv.glmnet 함수가 찾아줌, default가 10fold이고 nfolds option주면 됨 
  set.seed(1)
  cv.out = cv.glmnet(x[train,], y[train], alpha = 0) # cv.glmnet -> best lambda 알 수 o
  plot(cv.out)
  bestlam = cv.out$lambda.min
  ridge.pred = predict(ridge.mod, s=bestlam, newx = x[test,])
  MSE_bestlam = mean((ridge.pred - y.test)^2)
  
  c(bestlam,MSE_bestlam )  # best lambda, best lambda 넣은 MSE (best lambda에서 최소화)
  # 앞에서 lambda = 4일때 MSE = 101036.8, 여기서 lambda = 211.7416일 때 MSE = 96015.5127
  
  out = glmnet(x,y, alpha =0)
  predict(out, type = "coefficients", s = bestlam)[1:20,] 
  # 보면 값이 0인 없다!! 즉 Ridge 는 변수선택 하지 않는다 
  
```

즉 Ridge Regression 은 lambda 값을 잘 선택하면 Least Squared Model 보다 MSE를 낮게 추정해서 성능이 더 좋을 수 있다는 것을 알았다.

# 6.6.2 The Lasso -> glmnet(alpha=1)
```{r}
# glmnet 함수에서 alpha =0 일때는 Ridge 였지만 alpha =1로 바꿔주면 Lasso 가 됨 
  lasso.mod = glmnet(x[train,] , y[train], alpha = 1, lambda = grid )
  plot(lasso.mod)

# Cross-Validation
  set.seed(1)
  cv.out = cv.glmnet(x[train,], y[train], alpha =1)
  plot(cv.out)
  bestlam = cv.out$lambda.min
  lasso.pred = predict(lasso.mod, s=bestlam, newx =x[test,]) 
  mean( (lasso.pred - y.test)^2 ) # 앞서 Ridge 보다 더 작은 MSE 추정
  
  out = glmnet(x,y,alpha=1,lambda =grid)
  lasso.coef = predict(out, type = "coefficients", s =bestlam)[1:20,]
  lasso.coef # 결과 보면 19 개 중 12 개가 0 이고 7개의 변수만 0이 아닌 값을 가진다 
  
```

###  Chaper 6 Lab 3: PCR and PLS Regression

# 6.7.1 Principal Components Regression
```{r}
  
  library(pls)
  set.seed(2)
  pcr.fit = pcr(Salary ~., data =Hitters, scale = TRUE, validation = "CV")
              # scale = TRUE : 주성분 생성 전 표준화 
              # validation = "CV" : 주성분 M개에 대한 10-fold CV
              # CV 값은 M=0부터 M=4까지 출력 가능 
              # MSE 값 말고 RMSE 값 제공하기 때문에 제곱해 줘야 함 
  summary(pcr.fit)
              # 결과에 보면 TRAINING: % variance explained 있는데 
              #이건 M=1 일 때 설명변수 내 모든 분산or정보의 38.31%얻을 수 있음
              # M=6일 때 88.63%를 설명 가능 
  
  validationplot(pcr.fit, val.type = "MSEP") 
              # MSE 값 y축에 표현해줌 
              # M=16일 때 가장 작은 값 가지지만 M=19일 때(LSmodel)일 때와 거의 차이가 없음 
              # 그래프 보면 M=1일 때보다 MSE 값이 많이 감소하지 않음 따라서 적은 주성분으로 설명 가능함
  
# test set 대해서 PCR 을 수행하고 성능을 평가하자 
  set.seed(1)
  pcr.fit = pcr(Salary ~., data = Hitters, subset = train, scale = TRUE, validation = "CV")
  validationplot(pcr.fit, val.type = "MSEP")
              # M = 7 일 때 test MSE 가장 작음
  pcr.pred = predict(pcr.fit, x[test,], ncomp = 7)
  mean((pcr.pred - y.test)^2)
              # 따라서 M = 7일 때 MSE 값 
  
  pcr.fit = pcr(y ~ x, scale = TRUE, ncomp = 7)
  summary(pcr.fit)

```

# 6.7.2 Partial Least Squares
```{r}
  set.seed(1)
  pls.fit = plsr(Salary~., data = Hitters, subset = train, scale = TRUE, validation = "CV")
  summary(pls.fit)
  validationplot(pls.fit, val.type = "MSEP")
              # M = 2 일 때 test MSE 가장 작음
  
  pls.pred = predict(pls.fit, x[test,], ncomp=2)
  mean( (pls.pred - y.test)^2) # MSE 값이 약간 높지만 ridge, lasso, PCR 과 비슷함 
  
  pls.fit = plsr(Salary~., data = Hitters, scale = TRUE, ncomp =2)
  summary(pls.fit)
              # 두 개의 주성분 1comps, 2comps 가 설명하는 Salary 내의 분산 비율은 46.40 % 
              # 앞에서 PCR 은 7개의주성분으로 46.69% 설명한 것과 비슷한 값
  
              # 비교 
              # PCR :  설명변수에서 설명되는 분산의 양만 최대로 하려고 함 
              # PLS : 설명변수, 반응변수 둘 다의 분산을 설명하는 방향을 찾으려고 함 
```



### 연습문제 풀기 과제 


#EX 5
```{r}




```


#EX 9
```{r}

# a
dim(College)

set.seed(1)
trainid = sample(1:nrow(College), nrow(College)/2)
train = College[trainid,]
test = College[-trainid,]
y.train = train$Apps
y.test = test$Apps

# b
# Least Squar Model 
lm.fit = lm(Apps~., data=train)
lm.pred = predict(lm.fit,test) # predict function 안에 data = 이런거 넣지 않기 
lm.mse = mean((lm.pred - y.test)^2)
par(mfrow= c(1,1))
plot(test$Apps,lm.pred, main="Least Square Regression", xlab="Y", ylab ="Least Square predicted value")

# c
# Ridge Regression Model
x = model.matrix(Apps ~., College)[,-1] 
y = College$Apps

set.seed(1)
ridge.fit = cv.glmnet(x[trainid,], y.train, alpha = 0) 
plot(ridge.fit)
ridge.bestlam = ridge.fit$lambda.min
ridge.pred = predict(ridge.fit, s = ridge.bestlam, newx = x[-trainid,])
ridge.mse = mean((ridge.pred - y.test)^2)
plot(test$Apps,ridge.pred, main="Ridge Regression", xlab="Y", ylab ="Ridge predicted value")

# d
# Lasso Model
set.seed(1)
lasso.fit = cv.glmnet(x[trainid,], y.train, alpha = 1)
plot(lasso.fit)
lasso.bestlam = lasso.fit$lambda.min
lasso.pred = predict(lasso.fit, s = lasso.bestlam, newx = x[-trainid,])
lasso.mse = mean((lasso.pred - y.test)^2)
lasso.coef = predict(lasso.fit, type = "coefficients", s =lasso.bestlam)[1:ncol(College),]
lasso.coef 
length(lasso.coef[lasso.coef != 0])
length(lasso.coef)
plot(test$Apps,lasso.pred, main="Lasso Regression", xlab="Y", ylab ="Lasso predicted value")

# e 
# PCR Model
set.seed(1)
pcr.fit = pcr(Apps ~., data =train, scale = TRUE, validation = "CV")
validationplot(pcr.fit, val.type = "MSEP") 
summary(pcr.fit)       # CV 값이 16comps 일 때 1273로 가장 작음 
write.csv(summary(pcr.fit), "C:/Users/jeeyeon/Desktop/데마/HW6/table1.csv")
pcr.pred = predict(pcr.fit, x[-trainid,], ncomp = 16)
pcr.mse = mean((pcr.pred - y.test)^2)   # 따라서 M = 16일 때 MSE 값 
plot(test$Apps,pcr.pred, main="PCR Regression", xlab="Y", ylab ="PCR predicted value")

# f
# PLS model
set.seed(1)
pls.fit = plsr(Apps~., data = train, scale = TRUE, validation = "CV")
validationplot(pls.fit, val.type = "MSEP")
summary(pls.fit)       # CV 값이 11comps일 때 1279로 가장 작음
pls.pred = predict(pls.fit, x[-trainid,], ncomp=11)
pls.mse = mean( (pls.pred - y.test)^2) 

pls.fit = plsr(Apps~., data = test, scale = TRUE, ncomp =11)
summary(pls.fit)
plot(test$Apps,pls.pred, main="PLS Regression", xlab="Y", ylab ="PLS predicted value")

# 비교 
# PCR :  설명변수에서 설명되는 분산의 양만 최대로 하려고 함 
# PLS : 설명변수, 반응변수 둘 다의 분산을 설명하는 방향을 찾으려고 함 

# g
err.all =c(lm.mse, ridge.mse,lasso.mse,pcr.mse,pls.mse)
method = c("lm","ridge","lasso","pcr","pls")
table = data.frame(method, err.all)

ggplot(data=table, aes(x=method, y=err.all)) + geom_bar(stat="identity")+ theme_bw()

# BEST : lasso 

```



#EX 11
```{r}

# train, test split
  set.seed(1)
  trainid = sample(1:nrow(Boston), nrow(Boston)/2)
  train = Boston[trainid,]
  test = Boston[-trainid,]
  y.train = train$crim
  y.test = test$crim

# Best subeset selction 이용한 모델적합 
  regfit.best = regsubsets(crim ~.,data=train,nvmax=ncol(Boston)-1) # train set 적합한 모델 #nvmax:maximum size of subsets to examine
  test.mat = model.matrix(crim~., data = test) # model.matrix : 데이터로부터 X 행렬 구성함 
  head(test.mat) # 만들어진 X matrix 확인  

# Cross- Validation 
  # regsubsets 함수를 predict 사용하기 위해 predict function 정의
    predict.regsubsets = function(object, newdata, id, ...) {
      form = as.formula(object$call[[2]])
      mat = model.matrix(form, newdata)
      coefi = coef(object, id = id)
      mat[, names(coefi)] %*% coefi
    }
    
#   # for 문 돌려보자 
#   k = 10 
#   ncol(Boston)-1
#   cv.errors = matrix(NA,k,13, dimnames = list(NULL, paste(1:13)))
#   for(j in 1:k){
#     best.fit = regsubsets(crim ~., data =Boston[folds!=j,], nvmax =13) #folds=j인 경우는test 니까 모델적합x
#     for(i in 1:13){  # p 달리해가면서 best model 찾기 
#     pred = predict(best.fit, Boston[folds == j,], id =i)
#     cv.errors[j,i] = mean( (Boston$crim[folds==j]-pred )^2)
#     }
#   }
#   cv.errors
#   mean.cv.errors = apply(cv.errors,2,mean) # p 에 따른 testMSE 평균 
#   mean.cv.errors
#   par(mfrow = c(1,1))
#   plot(mean.cv.errors, type = "b") # p = 12 일 때가 가장 작은 error 가짐 !! 
#   reg.best = regsubsets(Salary ~., data =Hitters, nvmax = 13)
#   coef(reg.best, 12)
#   
# # Forward Selection 
#   cv.errors = matrix(NA,k,13, dimnames = list(NULL, paste(1:13)))
#   for(j in 1:k){
#     best.fit = regsubsets(crim ~., data =Boston[folds!=j,], nvmax =13, method = "forward") #folds=j인 경우는test 니까 모델적합x
#     for(i in 1:13){  # p 달리해가면서 best model 찾기 
#     pred = predict(best.fit, Boston[folds == j,], id =i)
#     cv.errors[j,i] = mean( (Boston$crim[folds==j]-pred )^2)
#     }
#   }
#   cv.errors
#   mean.cv.errors = apply(cv.errors,2,mean) # p 에 따른 testMSE 평균 
#   mean.cv.errors
#   par(mfrow = c(1,1))
#   plot(mean.cv.errors, type = "b") # p = 12 일 때가 가장 작은 error 가짐 !! 
#   
# # Backward Elimination
#   cv.errors = matrix(NA,k,13, dimnames = list(NULL, paste(1:13)))
#   for(j in 1:k){
#     best.fit = regsubsets(crim ~., data =Boston[folds!=j,], nvmax =13, method = "backward") #folds=j인 경우는test 니까 모델적합x
#     for(i in 1:13){  # p 달리해가면서 best model 찾기 
#     pred = predict(best.fit, Boston[folds == j,], id =i)
#     cv.errors[j,i] = mean( (Boston$crim[folds==j]-pred )^2)
#     }
#   }
#   cv.errors
#   mean.cv.errors = apply(cv.errors,2,mean) # p 에 따른 testMSE 평균 
#   mean.cv.errors
#   par(mfrow = c(1,1))
#   plot(mean.cv.errors, type = "b") # p = 12 일 때가 가장 작은 error 가짐 !! 
  
# forward selection
  fit.fwd <- regsubsets(crim~., data=train, nvmax=ncol(Boston)-1, method = "forward")
  (fwd.summary <- summary(fit.fwd))
  err.fwd <- rep(NA, ncol(Boston)-1)
  for(i in 1:(ncol(Boston)-1)) {
    pred.fwd <- predict(fit.fwd, test, id=i)
    err.fwd[i] <- mean((test$crim - pred.fwd)^2)
  }
  plot(err.fwd, type="b", main="Test MSE for Forward Selection", xlab="Number of Predictors")
  which.min(err.fwd)
  
# backward selection
  fit.bwd <- regsubsets(crim~., data=train, nvmax=ncol(Boston)-1, method = "backward")
  (bwd.summary <- summary(fit.bwd))
  err.bwd <- rep(NA, ncol(Boston)-1)
  for(i in 1:(ncol(Boston)-1)) {
    pred.bwd <- predict(fit.bwd, test, id=i)
    err.bwd[i] <- mean((test$crim - pred.bwd)^2)
  }
  plot(err.bwd, type="b", main="Test MSE for Backward Selection", xlab="Number of Predictors")
  which.min(err.bwd)
  
### plot 
  par(mfrow= c(4,2))
  
  plot(fwd.summary$rss, xlab = "Number of Variables", main = "Forward RSS",type = "l")
  minloc_rss = which.min(fwd.summary$rss) 
  points(minloc_rss, fwd.summary$rss[minloc_rss], col = "red", cex = 2, pch = 20) 
  plot(bwd.summary$rss, xlab = "Number of Variables", main = "Backward RSS",type = "l")
  minloc_rss = which.min(bwd.summary$rss) 
  points(minloc_rss, bwd.summary$rss[minloc_rss], col = "red", cex = 2, pch = 20) 
  
  plot(fwd.summary$adjr2, xlab = "Number of Variables", main = "Forward Adjusted R-square",type = "l")  
  maxloc_adjr2 = which.max(fwd.summary$adjr2)
  points(maxloc_adjr2, fwd.summary$adjr2[maxloc_adjr2], col = "red", cex = 2, pch = 20)  
  plot(bwd.summary$adjr2, xlab = "Number of Variables", main = "Backward Adjusted R-square",type = "l")  
  maxloc_adjr2 = which.max(bwd.summary$adjr2)
  points(maxloc_adjr2, bwd.summary$adjr2[maxloc_adjr2], col = "red", cex = 2, pch = 20) 
  
  plot(fwd.summary$cp, xlab = "Number of Variables", main = "Forward Cp",type = "l")
  minloc_cp = which.min(fwd.summary$cp) 
  points(minloc_cp, fwd.summary$cp[minloc_cp], col = "red", cex = 2, pch = 20)  
  plot(bwd.summary$cp, xlab = "Number of Variables", main = "Backward Cp",type = "l")
  minloc_cp = which.min(bwd.summary$cp) 
  points(minloc_cp, bwd.summary$cp[minloc_cp], col = "red", cex = 2, pch = 20)  
  
  plot(fwd.summary$bic, xlab = "Number of Variables", main = "Forward Bic",type = "l")
  minloc_bic = which.min(fwd.summary$bic) 
  points(minloc_bic, fwd.summary$bic[minloc_bic], col = "red", cex = 2, pch = 20)  
  plot(bwd.summary$bic, xlab = "Number of Variables", main = "Backward Bic",type = "l")
  minloc_bic = which.min(bwd.summary$bic) 
  points(minloc_bic, bwd.summary$bic[minloc_bic], col = "red", cex = 2, pch = 20)  

  par(mfrow= c(1,1))

# -----------------------------------------------------------------------
# Least Squar Model 
  lm.fit = lm(crim~., data=train)
  lm.pred = predict(lm.fit,test) 
  lm.mse = mean((lm.pred - y.test)^2)
  plot(test$crim,lm.pred)

# Ridge Regression Model
  x = model.matrix(crim ~., Boston)[,-1] 
  y = Boston$crim
  
  set.seed(1)
  ridge.fit = cv.glmnet(x[trainid,], y.train, alpha = 0) 
  plot(ridge.fit)
  ridge.bestlam = ridge.fit$lambda.min
  ridge.pred = predict(ridge.fit, s = ridge.bestlam, newx = x[-trainid,])
  ridge.mse = mean((ridge.pred - y.test)^2)

# Lasso Model
  set.seed(1)
  lasso.fit = cv.glmnet(x[trainid,], y.train, alpha = 1)
  plot(lasso.fit)
  lasso.bestlam = lasso.fit$lambda.min
  lasso.pred = predict(lasso.fit, s = lasso.bestlam, newx = x[-trainid,])
  lasso.mse = mean((lasso.pred - y.test)^2)
  lasso.coef = predict(lasso.fit, type = "coefficients", s =lasso.bestlam)[1:ncol(Boston),]
  length(lasso.coef[lasso.coef != 0])
 
# PCR Model
  set.seed(1)
  pcr.fit = pcr(crim ~., data =train, scale = TRUE, validation = "CV")
  validationplot(pcr.fit, val.type = "MSEP") 
  summary(pcr.fit)       # CV 값이 13comps 일 때 7.025 가장 작음 

  pcr.pred = predict(pcr.fit, x[-trainid,], ncomp = 13)
  pcr.mse = mean((pcr.pred - y.test)^2)   # 따라서 M = 13일 때 MSE 값 

# PLS model
  set.seed(1)
  pls.fit = plsr(crim~., data = train, scale = TRUE, validation = "CV")
  validationplot(pls.fit, val.type = "MSEP")
  summary(pls.fit)       # CV 값이 10comps일 때 7.025 가장 작음
  
  pls.pred = predict(pls.fit, x[-trainid,], ncomp=10)
  pls.mse = mean( (pls.pred - y.test)^2) 

# b
  err.all =c(min(err.fwd), min(err.bwd), lm.mse, ridge.mse,lasso.mse,pcr.mse,pls.mse)
  method = c("forward","backward","lm","ridge","lasso","pcr","pls")
  table = data.frame(method, err.all)
  
  ggplot(data=table, aes(x=method, y=err.all)) + geom_bar(stat="identity", ylim =c(20,40)) + theme_bw()+ coord_cartesian(ylim=c(30,40))
  
  # BEST : lass
# c
  lasso.coef # age, tax 변수는 0 , 모두 사용되지 않음 
  
```


