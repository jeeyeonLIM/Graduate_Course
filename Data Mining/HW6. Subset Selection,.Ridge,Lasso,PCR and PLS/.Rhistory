minloc_rss = which.min(bwd.summary$rss)
points(minloc_rss, bwd.summary$rss[minloc_rss], col = "red", cex = 2, pch = 20)
plot(fwd.summary$adjr2, xlab = "Number of Variables", ylab = "Forward Adjusted R-square",type = "l")
maxloc_adjr2 = which.max(fwd.summary$adjr2)
points(maxloc_adjr2, fwd.summary$adjr2[maxloc_adjr2], col = "red", cex = 2, pch = 20)
plot(bwd.summary$adjr2, xlab = "Number of Variables", ylab = "Backward Adjusted R-square",type = "l")
maxloc_adjr2 = which.max(bwd.summary$adjr2)
points(maxloc_adjr2, bwd.summary$adjr2[maxloc_adjr2], col = "red", cex = 2, pch = 20)
plot(fwd.summary$cp, xlab = "Number of Variables", ylab = "Forward Cp",type = "l")
minloc_cp = which.min(fwd.summary$cp)
points(minloc_cp, fwd.summary$cp[minloc_cp], col = "red", cex = 2, pch = 20)
plot(bwd.summary$cp, xlab = "Number of Variables", ylab = "Backward Cp",type = "l")
minloc_cp = which.min(bwd.summary$cp)
points(minloc_cp, bwd.summary$cp[minloc_cp], col = "red", cex = 2, pch = 20)
plot(fwd.summary$bic, xlab = "Number of Variables", ylab = "Forward Bic",type = "l")
minloc_bic = which.min(fwd.summary$bic)
points(minloc_bic, fwd.summary$bic[minloc_bic], col = "red", cex = 2, pch = 20)
plot(bwd.summary$bic, xlab = "Number of Variables", ylab = "Backward Bic",type = "l")
minloc_bic = which.min(bwd.summary$bic)
points(minloc_bic, bwd.summary$bic[minloc_bic], col = "red", cex = 2, pch = 20)
par(mfrow= c(1,1))
### plot
par(mfrow= c(4,2))
plot(fwd.summary$rss, xlab = "Number of Variables", main = "Forward RSS",type = "l")
minloc_rss = which.min(fwd.summary$rss)
points(minloc_rss, fwd.summary$rss[minloc_rss], col = "red", cex = 2, pch = 20)
plot(bwd.summary$rss, xlab = "Number of Variables", main = "Backward RSS",type = "l")
minloc_rss = which.min(bwd.summary$rss)
points(minloc_rss, bwd.summary$rss[minloc_rss], col = "red", cex = 2, pch = 20)
plot(fwd.summary$adjr2, xlab = "Number of Variables", main = "Forward Adjusted R-square",type = "l")
maxloc_adjr2 = which.max(fwd.summary$adjr2)
points(maxloc_adjr2, fwd.summary$adjr2[maxloc_adjr2], col = "red", cex = 2, pch = 20)
plot(bwd.summary$adjr2, xlab = "Number of Variables", main = "Backward Adjusted R-square",type = "l")
maxloc_adjr2 = which.max(bwd.summary$adjr2)
points(maxloc_adjr2, bwd.summary$adjr2[maxloc_adjr2], col = "red", cex = 2, pch = 20)
plot(fwd.summary$cp, xlab = "Number of Variables", main = "Forward Cp",type = "l")
minloc_cp = which.min(fwd.summary$cp)
points(minloc_cp, fwd.summary$cp[minloc_cp], col = "red", cex = 2, pch = 20)
plot(bwd.summary$cp, xlab = "Number of Variables", main = "Backward Cp",type = "l")
minloc_cp = which.min(bwd.summary$cp)
points(minloc_cp, bwd.summary$cp[minloc_cp], col = "red", cex = 2, pch = 20)
plot(fwd.summary$bic, xlab = "Number of Variables", main = "Forward Bic",type = "l")
minloc_bic = which.min(fwd.summary$bic)
points(minloc_bic, fwd.summary$bic[minloc_bic], col = "red", cex = 2, pch = 20)
plot(bwd.summary$bic, xlab = "Number of Variables", main = "Backward Bic",type = "l")
minloc_bic = which.min(bwd.summary$bic)
points(minloc_bic, bwd.summary$bic[minloc_bic], col = "red", cex = 2, pch = 20)
par(mfrow= c(1,1))
par(mfrow=c(3,2))
min.cp <- which.min(fwd.summary$cp)
plot(fwd.summary$cp, xlab="Number of Poly(X)", ylab="Forward Selection Cp", type="l")
points(min.cp, fwd.summary$cp[min.cp], col="red", pch=4, lwd=5)
min.cp <- which.min(bwd.summary$cp)
plot(bwd.summary$cp, xlab="Number of Poly(X)", ylab="Backward Selection Cp", type="l")
points(min.cp, bwd.summary$cp[min.cp], col="red", pch=4, lwd=5)
min.bic <- which.min(fwd.summary$bic)
plot(fwd.summary$bic, xlab="Number of Poly(X)", ylab="Forward Selection BIC", type="l")
points(min.bic, fwd.summary$bic[min.bic], col="red", pch=4, lwd=5)
min.bic <- which.min(bwd.summary$bic)
plot(bwd.summary$bic, xlab="Number of Poly(X)", ylab="Backward Selection BIC", type="l")
points(min.bic, bwd.summary$bic[min.bic], col="red", pch=4, lwd=5)
min.adjr2 <- which.max(fwd.summary$adjr2)
plot(fwd.summary$adjr2, xlab="Number of Poly(X)", ylab="Forward Selection Adjusted R^2", type="l")
points(min.adjr2, fwd.summary$adjr2[min.adjr2], col="red", pch=4, lwd=5)
min.adjr2 <- which.max(bwd.summary$adjr2)
plot(bwd.summary$adjr2, xlab="Number of Poly(X)", ylab="Backward Selection Adjusted R^2", type="l")
points(min.adjr2, bwd.summary$adjr2[min.adjr2], col="red", pch=4, lwd=5)
err.fwd
err.bwd
err.fwd
err.bwd
err.fwd
err.bwd
ridge.mse
lasso.mse
min(err.fwd)
min(err.fwd)
min(err.bwd)
ridge.mse
lasso.mse
# 비교
err.all =c(min(err.fwd), min(err.bwd), lm.mse, ridge.mse,lasso.mse,pcr.mse,pls.mse)
method = c("forward","backward","lm","ridge","lasso","pcr","pls")
table = data.frame(method, err.all)
ggplot(data=table, aes(x=method, y=err.all)) + geom_bar(stat="identity") + theme_bw()
# BEST : lass
ggplot(data=table, aes(x=method, y=err.all)) + geom_bar(stat="identity") + theme_bw() +ylim(20,40)
ggplot(data=table, aes(x=method, y=err.all)) + geom_bar(stat="identity") + theme_bw() +ylim(c(20,40))
ggplot(data=table, aes(x=method, y=err.all)) + geom_bar(stat="identity") + theme_bw() +ylim(c(10,40))
ggplot(data=table, aes(x=method, y=err.all)) + geom_bar(stat="identity") + theme_bw() +ylim(10,40)
ggplot(data=table, aes(x=method, y=err.all)) + geom_bar(stat="identity") + theme_bw()
ggplot(data=table, aes(x=method, y=err.all)) + geom_bar(stat="identity", ylim =c(20,40)) + theme_bw()
# BEST : lass
?geom_barplot
# BEST : lass
?geom_bar
ggplot(data=table, aes(x=method, y=err.all)) + geom_bar(stat="identity", ylim =c(20,40)) + theme_bw()+ scale_y_continuous(limits=c(20,40),oob = rescale_none)
ggplot(data=table, aes(x=method, y=err.all)) + geom_bar(stat="identity", ylim =c(20,40)) + theme_bw()+ scale_y_continuous(limits=c(20,40))
ggplot(data=table, aes(x=method, y=err.all)) + geom_bar(stat="identity", ylim =c(20,40)) + theme_bw()+ coord_cartesian(ylim=c(5,40))
ggplot(data=table, aes(x=method, y=err.all)) + geom_bar(stat="identity", ylim =c(20,40)) + theme_bw()+ coord_cartesian(ylim=c(20,40))
ggplot(data=table, aes(x=method, y=err.all)) + geom_bar(stat="identity", ylim =c(20,40)) + theme_bw()+ coord_cartesian(ylim=c(30,40))
lasso.coef
#======================================================================
# train, test split
set.seed(1)
trainid = sample(1:nrow(Boston), nrow(Boston)/2)
train = Boston[trainid,]
test = Boston[-trainid,]
y.train = train$crim
y.test = test$crim
# Best subeset selction 이용한 모델적합
regfit.best = regsubsets(crim ~.,data=Boston[train,],nvmax=ncol(Boston)-1) # train set 적합한 모델 #nvmax:maximum size of subsets to examine
test.mat = model.matrix(crim~., data = Boston[test,]) # model.matrix : 데이터로부터 X 행렬 구성함
# Best subeset selction 이용한 모델적합
regfit.best = regsubsets(crim ~.,data=Boston[train,],nvmax=ncol(Boston)-1) # train set 적합한 모델 #nvmax:maximum size of subsets to examine
set.seed(1)
trainid = sample(1:nrow(Boston), nrow(Boston)/2)
train = Boston[trainid,]
test = Boston[-trainid,]
y.train = train$crim
y.test = test$crim
# Best subeset selction 이용한 모델적합
regfit.best = regsubsets(crim ~.,data=train,nvmax=ncol(Boston)-1) # train set 적합한 모델 #nvmax:maximum size of subsets to examine
test.mat = model.matrix(crim~., data = test) # model.matrix : 데이터로부터 X 행렬 구성함
head(test.mat) # 만들어진 X matrix 확인
#======================================================================
# train, test split
set.seed(1)
trainid = sample(1:nrow(Boston), nrow(Boston)/2)
train = Boston[trainid,]
test = Boston[-trainid,]
y.train = train$crim
y.test = test$crim
# Best subeset selction 이용한 모델적합
regfit.best = regsubsets(crim ~.,data=train,nvmax=ncol(Boston)-1) # train set 적합한 모델 #nvmax:maximum size of subsets to examine
test.mat = model.matrix(crim~., data = test) # model.matrix : 데이터로부터 X 행렬 구성함
head(test.mat) # 만들어진 X matrix 확인
# Cross- Validation
# regsubsets 함수를 predict 사용하기 위해 predict function 정의
predict.regsubsets = function(object, newdata, id, ...) {
form = as.formula(object$call[[2]])
mat = model.matrix(form, newdata)
coefi = coef(object, id = id)
mat[, names(coefi)] %*% coefi
}
# forward selection
fit.fwd <- regsubsets(crim~., data=train, nvmax=ncol(Boston)-1, method = "forward")
(fwd.summary <- summary(fit.fwd))
err.fwd <- rep(NA, ncol(Boston)-1)
for(i in 1:(ncol(Boston)-1)) {
pred.fwd <- predict(fit.fwd, test, id=i)
err.fwd[i] <- mean((test$crim - pred.fwd)^2)
}
plot(err.fwd, type="b", main="Test MSE for Forward Selection", xlab="Number of Predictors")
which.min(err.fwd)
# backward selection
fit.bwd <- regsubsets(crim~., data=train, nvmax=ncol(Boston)-1, method = "backward")
(bwd.summary <- summary(fit.bwd))
err.bwd <- rep(NA, ncol(Boston)-1)
for(i in 1:(ncol(Boston)-1)) {
pred.bwd <- predict(fit.bwd, test, id=i)
err.bwd[i] <- mean((test$crim - pred.bwd)^2)
}
plot(err.bwd, type="b", main="Test MSE for Backward Selection", xlab="Number of Predictors")
which.min(err.bwd)
### plot
par(mfrow= c(4,2))
plot(fwd.summary$rss, xlab = "Number of Variables", main = "Forward RSS",type = "l")
minloc_rss = which.min(fwd.summary$rss)
points(minloc_rss, fwd.summary$rss[minloc_rss], col = "red", cex = 2, pch = 20)
plot(bwd.summary$rss, xlab = "Number of Variables", main = "Backward RSS",type = "l")
minloc_rss = which.min(bwd.summary$rss)
points(minloc_rss, bwd.summary$rss[minloc_rss], col = "red", cex = 2, pch = 20)
plot(fwd.summary$adjr2, xlab = "Number of Variables", main = "Forward Adjusted R-square",type = "l")
maxloc_adjr2 = which.max(fwd.summary$adjr2)
points(maxloc_adjr2, fwd.summary$adjr2[maxloc_adjr2], col = "red", cex = 2, pch = 20)
plot(bwd.summary$adjr2, xlab = "Number of Variables", main = "Backward Adjusted R-square",type = "l")
maxloc_adjr2 = which.max(bwd.summary$adjr2)
points(maxloc_adjr2, bwd.summary$adjr2[maxloc_adjr2], col = "red", cex = 2, pch = 20)
minloc_cp = which.min(fwd.summary$cp)
plot(fwd.summary$cp, xlab = "Number of Variables", main = "Forward Cp",type = "l")
points(minloc_cp, fwd.summary$cp[minloc_cp], col = "red", cex = 2, pch = 20)
minloc_cp = which.min(bwd.summary$cp)
points(minloc_cp, bwd.summary$cp[minloc_cp], col = "red", cex = 2, pch = 20)
plot(fwd.summary$bic, xlab = "Number of Variables", main = "Forward Bic",type = "l")
plot(bwd.summary$cp, xlab = "Number of Variables", main = "Backward Cp",type = "l")
# a
dim(College)
set.seed(1)
trainid = sample(1:nrow(College), nrow(College)/2)
train = College[trainid,]
test = College[-trainid,]
y.train = train$Apps
y.test = test$Apps
lm.mse
plot(test$Apps,lm.pred)
# Least Squar Model
lm.fit = lm(Apps~., data=train)
lm.pred = predict(lm.fit,test) # predict function 안에 data = 이런거 넣지 않기
lm.mse = mean((lm.pred - y.test)^2)
plot(test$Apps,lm.pred)
# a
dim(College)
set.seed(1)
trainid = sample(1:nrow(College), nrow(College)/2)
train = College[trainid,]
test = College[-trainid,]
y.train = train$Apps
y.test = test$Apps
# b
# Least Squar Model
lm.fit = lm(Apps~., data=train)
lm.pred = predict(lm.fit,test) # predict function 안에 data = 이런거 넣지 않기
lm.mse = mean((lm.pred - y.test)^2)
plot(test$Apps,lm.pred)
par(mfrow= c(1,1))
plot(test$Apps,lm.pred)
# c
# Ridge Regression Model
x = model.matrix(Apps ~., College)[,-1]
y = College$Apps
set.seed(1)
ridge.fit = cv.glmnet(x[trainid,], y.train, alpha = 0)
plot(ridge.fit)
ridge.bestlam = ridge.fit$lambda.min
ridge.pred = predict(ridge.fit, s = ridge.bestlam, newx = x[-trainid,])
ridge.mse = mean((ridge.pred - y.test)^2)
plot(test$Apps,ridge.pred)
plot(test$Apps,lm.pred)
plot(ridge.fit)
plot(test$Apps,ridge.pred)
set.seed(1)
lasso.fit = cv.glmnet(x[trainid,], y.train, alpha = 1)
plot(lasso.fit)
lasso.bestlam = lasso.fit$lambda.min
lasso.pred = predict(lasso.fit, s = lasso.bestlam, newx = x[-trainid,])
lasso.mse = mean((lasso.pred - y.test)^2)
lasso.coef = predict(lasso.fit, type = "coefficients", s =lasso.bestlam)[1:ncol(College),]
lasso.coef
length(lasso.coef[lasso.coef != 0])
plot(test$Apps,lasso.pred, main="Lasso Regression", xlab="Y", ylab ="Lasso predicted value")
plot(test$Apps,pcr.pred, main="Lasso Regression", xlab="Y", ylab ="Lasso predicted value")
set.seed(1)
pcr.fit = pcr(Apps ~., data =train, scale = TRUE, validation = "CV")
validationplot(pcr.fit, val.type = "MSEP")
summary(pcr.fit)       # CV 값이 16comps 일 때 1273로 가장 작음
pcr.pred = predict(pcr.fit, x[-trainid,], ncomp = 16)
pcr.mse = mean((pcr.pred - y.test)^2)   # 따라서 M = 16일 때 MSE 값
plot(test$Apps,pcr.pred, main="Lasso Regression", xlab="Y", ylab ="Lasso predicted value")
lm.fit = lm(Apps~., data=train)
lm.pred = predict(lm.fit,test) # predict function 안에 data = 이런거 넣지 않기
lm.mse = mean((lm.pred - y.test)^2)
par(mfrow= c(1,1))
plot(test$Apps,lm.pred, main="Least Square Regression", xlab="Y", ylab ="Least Square predicted value")
# c
# Ridge Regression Model
x = model.matrix(Apps ~., College)[,-1]
y = College$Apps
set.seed(1)
ridge.fit = cv.glmnet(x[trainid,], y.train, alpha = 0)
plot(ridge.fit)
ridge.bestlam = ridge.fit$lambda.min
ridge.pred = predict(ridge.fit, s = ridge.bestlam, newx = x[-trainid,])
ridge.mse = mean((ridge.pred - y.test)^2)
plot(test$Apps,ridge.pred, main="Ridge Regression", xlab="Y", ylab ="Ridge predicted value")
# d
# Lasso Model
set.seed(1)
lasso.fit = cv.glmnet(x[trainid,], y.train, alpha = 1)
plot(lasso.fit)
lasso.bestlam = lasso.fit$lambda.min
lasso.pred = predict(lasso.fit, s = lasso.bestlam, newx = x[-trainid,])
lasso.mse = mean((lasso.pred - y.test)^2)
lasso.coef = predict(lasso.fit, type = "coefficients", s =lasso.bestlam)[1:ncol(College),]
lasso.coef
length(lasso.coef[lasso.coef != 0])
plot(test$Apps,lasso.pred, main="Lasso Regression", xlab="Y", ylab ="Lasso predicted value")
# e
# PCR Model
set.seed(1)
pcr.fit = pcr(Apps ~., data =train, scale = TRUE, validation = "CV")
validationplot(pcr.fit, val.type = "MSEP")
summary(pcr.fit)       # CV 값이 16comps 일 때 1273로 가장 작음
pcr.pred = predict(pcr.fit, x[-trainid,], ncomp = 16)
pcr.mse = mean((pcr.pred - y.test)^2)   # 따라서 M = 16일 때 MSE 값
plot(test$Apps,pcr.pred, main="PCR Regression", xlab="Y", ylab ="PCR predicted value")
# f
# PLS model
set.seed(1)
pls.fit = plsr(Apps~., data = train, scale = TRUE, validation = "CV")
validationplot(pls.fit, val.type = "MSEP")
summary(pls.fit)       # CV 값이 11comps일 때 1279로 가장 작음
pls.pred = predict(pls.fit, x[-trainid,], ncomp=11)
pls.mse = mean( (pls.pred - y.test)^2)
pls.fit = plsr(Apps~., data = test, scale = TRUE, ncomp =11)
summary(pls.fit)
plot(test$Apps,pls.pred, main="PLS Regression", xlab="Y", ylab ="PLS predicted value")
# g
err.all =c(lm.mse, ridge.mse,lasso.mse,pcr.mse,pls.mse)
method = c("lm","ridge","lasso","pcr","pls")
table = data.frame(method, err.all)
ggplot(data=table, aes(x=method, y=err.all)) + geom_bar(stat="identity")+ theme_bw()
lm.mse
ridge.bestlam
ridge.mse
ridge.mse
lasso.mse
length(lasso.coef[lasso.coef != 0])
set.seed(1)
lasso.fit = cv.glmnet(x[trainid,], y.train, alpha = 1)
plot(lasso.fit)
lasso.bestlam = lasso.fit$lambda.min
lasso.pred = predict(lasso.fit, s = lasso.bestlam, newx = x[-trainid,])
lasso.mse = mean((lasso.pred - y.test)^2)
lasso.coef = predict(lasso.fit, type = "coefficients", s =lasso.bestlam)[1:ncol(College),]
lasso.coef
length(lasso.coef[lasso.coef != 0])
plot(test$Apps,lasso.pred, main="Lasso Regression", xlab="Y", ylab ="Lasso predicted value")
lasso.coef
length(lasso.coef)
lasso.bestlam
summary(pcr.fit)       # CV 값이 16comps 일 때 1273로 가장 작음
write.csv(summary(pcr.fit), "C:/Users/jeeyeon/Desktop/데마/HW6/table1.csv")
write.csv(summary(pcr.fit), "C:/Users/jeeyeon/Desktop/데마/HW6/table1.csv")
ggplot(data=table, aes(x=method, y=err.all)) + geom_bar(stat="identity")+ theme_bw()
set.seed(1)
trainid = sample(1:nrow(Boston), nrow(Boston)/2)
train = Boston[trainid,]
test = Boston[-trainid,]
y.train = train$crim
y.test = test$crim
# Best subeset selction 이용한 모델적합
regfit.best = regsubsets(crim ~.,data=train,nvmax=ncol(Boston)-1) # train set 적합한 모델 #nvmax:maximum size of subsets to examine
test.mat = model.matrix(crim~., data = test) # model.matrix : 데이터로부터 X 행렬 구성함
head(test.mat) # 만들어진 X matrix 확인
# Cross- Validation
# regsubsets 함수를 predict 사용하기 위해 predict function 정의
predict.regsubsets = function(object, newdata, id, ...) {
form = as.formula(object$call[[2]])
mat = model.matrix(form, newdata)
coefi = coef(object, id = id)
mat[, names(coefi)] %*% coefi
}
# forward selection
fit.fwd <- regsubsets(crim~., data=train, nvmax=ncol(Boston)-1, method = "forward")
(fwd.summary <- summary(fit.fwd))
err.fwd <- rep(NA, ncol(Boston)-1)
for(i in 1:(ncol(Boston)-1)) {
pred.fwd <- predict(fit.fwd, test, id=i)
err.fwd[i] <- mean((test$crim - pred.fwd)^2)
}
plot(err.fwd, type="b", main="Test MSE for Forward Selection", xlab="Number of Predictors")
which.min(err.fwd)
# backward selection
fit.bwd <- regsubsets(crim~., data=train, nvmax=ncol(Boston)-1, method = "backward")
(bwd.summary <- summary(fit.bwd))
err.bwd <- rep(NA, ncol(Boston)-1)
for(i in 1:(ncol(Boston)-1)) {
pred.bwd <- predict(fit.bwd, test, id=i)
err.bwd[i] <- mean((test$crim - pred.bwd)^2)
}
plot(err.bwd, type="b", main="Test MSE for Backward Selection", xlab="Number of Predictors")
which.min(err.bwd)
### plot
par(mfrow= c(4,2))
plot(fwd.summary$rss, xlab = "Number of Variables", main = "Forward RSS",type = "l")
minloc_rss = which.min(fwd.summary$rss)
points(minloc_rss, fwd.summary$rss[minloc_rss], col = "red", cex = 2, pch = 20)
plot(bwd.summary$rss, xlab = "Number of Variables", main = "Backward RSS",type = "l")
minloc_rss = which.min(bwd.summary$rss)
points(minloc_rss, bwd.summary$rss[minloc_rss], col = "red", cex = 2, pch = 20)
plot(fwd.summary$adjr2, xlab = "Number of Variables", main = "Forward Adjusted R-square",type = "l")
maxloc_adjr2 = which.max(fwd.summary$adjr2)
points(maxloc_adjr2, fwd.summary$adjr2[maxloc_adjr2], col = "red", cex = 2, pch = 20)
plot(bwd.summary$adjr2, xlab = "Number of Variables", main = "Backward Adjusted R-square",type = "l")
maxloc_adjr2 = which.max(bwd.summary$adjr2)
points(maxloc_adjr2, bwd.summary$adjr2[maxloc_adjr2], col = "red", cex = 2, pch = 20)
plot(fwd.summary$cp, xlab = "Number of Variables", main = "Forward Cp",type = "l")
minloc_cp = which.min(fwd.summary$cp)
points(minloc_cp, fwd.summary$cp[minloc_cp], col = "red", cex = 2, pch = 20)
plot(bwd.summary$cp, xlab = "Number of Variables", main = "Backward Cp",type = "l")
minloc_cp = which.min(bwd.summary$cp)
points(minloc_cp, bwd.summary$cp[minloc_cp], col = "red", cex = 2, pch = 20)
plot(fwd.summary$bic, xlab = "Number of Variables", main = "Forward Bic",type = "l")
minloc_bic = which.min(fwd.summary$bic)
points(minloc_bic, fwd.summary$bic[minloc_bic], col = "red", cex = 2, pch = 20)
plot(bwd.summary$bic, xlab = "Number of Variables", main = "Backward Bic",type = "l")
minloc_bic = which.min(bwd.summary$bic)
points(minloc_bic, bwd.summary$bic[minloc_bic], col = "red", cex = 2, pch = 20)
par(mfrow= c(1,1))
# Least Squar Model
lm.fit = lm(crim~., data=train)
lm.pred = predict(lm.fit,test)
lm.mse = mean((lm.pred - y.test)^2)
plot(test$crim,lm.pred)
# Ridge Regression Model
x = model.matrix(crim ~., Boston)[,-1]
y = Boston$crim
set.seed(1)
ridge.fit = cv.glmnet(x[trainid,], y.train, alpha = 0)
plot(ridge.fit)
ridge.bestlam = ridge.fit$lambda.min
ridge.pred = predict(ridge.fit, s = ridge.bestlam, newx = x[-trainid,])
ridge.mse = mean((ridge.pred - y.test)^2)
# Lasso Model
set.seed(1)
lasso.fit = cv.glmnet(x[trainid,], y.train, alpha = 1)
plot(lasso.fit)
lasso.bestlam = lasso.fit$lambda.min
lasso.pred = predict(lasso.fit, s = lasso.bestlam, newx = x[-trainid,])
lasso.mse = mean((lasso.pred - y.test)^2)
lasso.coef = predict(lasso.fit, type = "coefficients", s =lasso.bestlam)[1:ncol(Boston),]
length(lasso.coef[lasso.coef != 0])
# PCR Model
set.seed(1)
pcr.fit = pcr(crim ~., data =train, scale = TRUE, validation = "CV")
validationplot(pcr.fit, val.type = "MSEP")
summary(pcr.fit)       # CV 값이 13comps 일 때 7.025 가장 작음
pcr.pred = predict(pcr.fit, x[-trainid,], ncomp = 13)
pcr.mse = mean((pcr.pred - y.test)^2)   # 따라서 M = 13일 때 MSE 값
# PLS model
set.seed(1)
pls.fit = plsr(crim~., data = train, scale = TRUE, validation = "CV")
validationplot(pls.fit, val.type = "MSEP")
summary(pls.fit)       # CV 값이 10comps일 때 7.025 가장 작음
pls.pred = predict(pls.fit, x[-trainid,], ncomp=10)
pls.mse = mean( (pls.pred - y.test)^2)
# b
err.all =c(min(err.fwd), min(err.bwd), lm.mse, ridge.mse,lasso.mse,pcr.mse,pls.mse)
method = c("forward","backward","lm","ridge","lasso","pcr","pls")
table = data.frame(method, err.all)
ggplot(data=table, aes(x=method, y=err.all)) + geom_bar(stat="identity", ylim =c(20,40)) + theme_bw()+ coord_cartesian(ylim=c(30,40))
# Lasso Model
set.seed(1)
lasso.fit = cv.glmnet(x[trainid,], y.train, alpha = 1)
plot(lasso.fit)
lasso.bestlam = lasso.fit$lambda.min
lasso.pred = predict(lasso.fit, s = lasso.bestlam, newx = x[-trainid,])
lasso.mse = mean((lasso.pred - y.test)^2)
lasso.coef = predict(lasso.fit, type = "coefficients", s =lasso.bestlam)[1:ncol(Boston),]
length(lasso.coef[lasso.coef != 0])
# Ridge Regression Model
x = model.matrix(crim ~., Boston)[,-1]
y = Boston$crim
set.seed(1)
ridge.fit = cv.glmnet(x[trainid,], y.train, alpha = 0)
plot(ridge.fit)
ridge.bestlam = ridge.fit$lambda.min
ridge.pred = predict(ridge.fit, s = ridge.bestlam, newx = x[-trainid,])
ridge.mse = mean((ridge.pred - y.test)^2)
plot(ridge.fit)
# Lasso Model
set.seed(1)
lasso.fit = cv.glmnet(x[trainid,], y.train, alpha = 1)
plot(lasso.fit)
# PCR Model
set.seed(1)
pcr.fit = pcr(crim ~., data =train, scale = TRUE, validation = "CV")
validationplot(pcr.fit, val.type = "MSEP")
summary(pcr.fit)       # CV 값이 13comps 일 때 7.025 가장 작음
pcr.pred = predict(pcr.fit, x[-trainid,], ncomp = 13)
pcr.mse = mean((pcr.pred - y.test)^2)   # 따라서 M = 13일 때 MSE 값
summary(pcr.fit)       # CV 값이 13comps 일 때 7.025 가장 작음
# Lasso Model
set.seed(1)
lasso.fit = cv.glmnet(x[trainid,], y.train, alpha = 1)
plot(lasso.fit)
lasso.bestlam = lasso.fit$lambda.min
lasso.pred = predict(lasso.fit, s = lasso.bestlam, newx = x[-trainid,])
lasso.mse = mean((lasso.pred - y.test)^2)
lasso.coef = predict(lasso.fit, type = "coefficients", s =lasso.bestlam)[1:ncol(Boston),]
length(lasso.coef[lasso.coef != 0])
plot(lasso.fit)
# PCR Model
set.seed(1)
pcr.fit = pcr(crim ~., data =train, scale = TRUE, validation = "CV")
validationplot(pcr.fit, val.type = "MSEP")
summary(pcr.fit)       # CV 값이 13comps 일 때 7.025 가장 작음
pcr.pred = predict(pcr.fit, x[-trainid,], ncomp = 13)
pcr.mse = mean((pcr.pred - y.test)^2)   # 따라서 M = 13일 때 MSE 값
# PLS model
set.seed(1)
pls.fit = plsr(crim~., data = train, scale = TRUE, validation = "CV")
validationplot(pls.fit, val.type = "MSEP")
summary(pls.fit)       # CV 값이 10comps일 때 7.025 가장 작음
pls.pred = predict(pls.fit, x[-trainid,], ncomp=10)
pls.mse = mean( (pls.pred - y.test)^2)
length(lasso.coef[lasso.coef != 0])
set.seed(1)
ridge.fit = cv.glmnet(x[trainid,], y.train, alpha = 0)
plot(ridge.fit)
ridge.bestlam = ridge.fit$lambda.min
ridge.pred = predict(ridge.fit, s = ridge.bestlam, newx = x[-trainid,])
ridge.mse = mean((ridge.pred - y.test)^2)
# Lasso Model
set.seed(1)
lasso.fit = cv.glmnet(x[trainid,], y.train, alpha = 1)
plot(lasso.fit)
lasso.bestlam = lasso.fit$lambda.min
lasso.pred = predict(lasso.fit, s = lasso.bestlam, newx = x[-trainid,])
lasso.mse = mean((lasso.pred - y.test)^2)
lasso.coef = predict(lasso.fit, type = "coefficients", s =lasso.bestlam)[1:ncol(Boston),]
length(lasso.coef[lasso.coef != 0])
length(lasso.coef)
ridge.bestlam
lasso.bestlam
# BEST : lass
# c
lasso.coef # age, tax 변수는 0 , 모두 사용되지 않음
length(lasso.coef[lasso.coef != 0])
length(lasso.coef)
validationplot(pls.fit, val.type = "MSEP")
