{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 영어 텍스트 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 원시 텍스트 얻기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 원시 텍스트 파일 읽기 - 가장 많이사용하는 형태 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 외부 파일 불러오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------Output from Raw Text file-----------\n",
      "\n",
      "\n",
      "ABSTRACT\n",
      "\n",
      "1. An abstract, or summary, is published together with a research article, giving the reader a \"preview\" of what's to come. Such abstracts may also be published separately in bibliographical sources, such as Biologic al Abstracts. They allow other scientists to quickly scan the large scientific literature, and decide which articles they want to read in depth. The abstract should be a little less technical than the article itself; you don't want to dissuade your potent ial audience fro\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Get raw data form file\n",
    "\n",
    "# fileread 라는 함수를 사용하면 rawtextcorpus 파일 읽어줌\n",
    "def fileread():\n",
    "    file_contents = open(r\"rawtextcorpus.txt\", \"r\").read()   # rawtextcorpus 파일 불러들이기 \n",
    "    #print(file_contents)\n",
    "    return file_contents\n",
    "\n",
    "# file 잘 읽었는지 볼까\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\") \n",
    "    print(\"----------Output from Raw Text file-----------\")\n",
    "    print(\"\")\n",
    "    filecontentdetails = fileread() # 파일 읽어라 \n",
    "    print(filecontentdetails[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 셀 내에서 원시 텍스트 데이터 정의 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 텍스트를 직접 입력한 것을 읽어줌 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------Output from assigned variable-------\n",
      "\n",
      " one paragraph, of 100-250 words, which summarizes the purpose, methods, results and conclusions of the paper.\n",
      "    It is not easy to include all this information in just a few words. Start by writing a summary that includes whatever you think is important,\n",
      "    and then gradually prune it down to size by removing unnecessary words, while still retaini ng the necessary concepts.\n",
      "    Don't use abbreviations or citations in the abstract. It should be able to stand alone without any footnotes. Fig 1.1.1 shows below.\n"
     ]
    }
   ],
   "source": [
    "# assign text data \n",
    "\n",
    "# localtextvalue 라는 함수 사용해 text 값 돌려놓기 \n",
    "def localtextvalue():\n",
    "    text = \"\"\" one paragraph, of 100-250 words, which summarizes the purpose, methods, results and conclusions of the paper.\n",
    "    It is not easy to include all this information in just a few words. Start by writing a summary that includes whatever you think is important,\n",
    "    and then gradually prune it down to size by removing unnecessary words, while still retaini ng the necessary concepts.\n",
    "    Don't use abbreviations or citations in the abstract. It should be able to stand alone without any footnotes. Fig 1.1.1 shows below.\"\"\"\n",
    "    return text\n",
    "\n",
    "# file 잘 읽었는지 볼까\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\")\n",
    "    print(\"-------Output from assigned variable-------\")\n",
    "    print(\"\")\n",
    "    localveriabledata = localtextvalue()\n",
    "    print(localveriabledata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) NLTK에서 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  [NLTK 자연어 처리 패키지]\n",
    "- 말뭉치 : 자연어 분석 작업을 위해 만든 샘플 문서 집합 , nltk.download(\"book\") 문장으로 nltk패키지에서 대부분의 말뭉치를 다운로드 가능함\n",
    "- 토큰 생성 : 자연어 문서를 분석하기 위해서 긴 문자열을 작은 단위로 나눠야 하는 과정, 토큰생성함수는 문자열을 입력받아서 토큰문자열 리스트를 출력\n",
    "- 형태소분석 : 형태소는 일정한 의미가 있는 가장 작은 말의 단위인데 단어로부터 어근,접두사,접미사,품사 등의 언어적 속성 파악하여 형태소를 찾아냄, 어간추출,원형복원,품사부착 등이 이에 해당함 \n",
    "- 품사 태깅 : 품사란 낱말을 문법적인 기능이나 형태,뜻에 따라 구분한 것임\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\jeeyeon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dutenberg 말뭉치는 저작권 말소된 문학작품 포함되어 있음 , 분석을 위해 다운로드 하자 \n",
    "\n",
    "nltk.download('gutenberg') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Emma by Jane Austen 1816]\n",
      "\n",
      "VOLUME I\n",
      "\n",
      "CHAPTER I\n",
      "\n",
      "\n",
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
      "and happy disposition, seemed to unite some of the best blessings\n",
      "of existence; and had lived nearly twenty-one years in the world\n",
      "with very little to distress or vex her.\n",
      "\n",
      "She was the youngest of the two daughters of a most affectionate,\n",
      "indulgent father; and had, in consequence of her sister's marriage,\n",
      "been mistress of his house from a very early period.  Her mother\n",
      "had died t\n"
     ]
    }
   ],
   "source": [
    "# 예를들어 오스틴 엠마 문서 살펴보자 \n",
    "\n",
    "# nltk.corpus 모듈에서 gustenberg 꺼 불러와서 cg 라고 해보자 \n",
    "from nltk.corpus import gutenberg as cg  \n",
    "\n",
    "emma_raw = cg.raw(\"austen-emma.txt\")\n",
    "print(emma_raw[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------Output Corpus data--------------\n",
      "\n",
      "[The Adventures of Buster Bear by Thornton W. Burgess 1920]\r\n",
      "\r\n",
      "I\r\n",
      "\r\n",
      "BUSTER BEAR GOES FISHING\r\n",
      "\r\n",
      "\r\n",
      "Buster Bear yawned as he lay on his comfortable bed of leaves and\r\n",
      "watched the first early morning sunbeams creeping through the Green\r\n",
      "Forest to chase out the Black Shadows. Once more he yawned, and slowly\r\n",
      "got to his feet and shook himself. Then he walked over to a big\r\n",
      "pine-tree, stood up on his hind legs, reached as high up on the trunk of\r\n",
      "the tree as he could, and scratched the bark with his g\n"
     ]
    }
   ],
   "source": [
    "# 예를들어 burgess-busterbrown 문서 살펴보자 \n",
    "\n",
    "# Use NLTK corpus \n",
    "\n",
    "# nltk.corpus 모듈에서 gustenberg 꺼 불러와서 cg 라고 해보자 \n",
    "from nltk.corpus import gutenberg as cg  \n",
    "\n",
    "def readcorpus():\n",
    "    raw_content_cg = cg.raw(\"burgess-busterbrown.txt\")\n",
    "    return raw_content_cg[:500]\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\") \n",
    "    print(\"-------Output Corpus data--------------\")\n",
    "    print(\"\") \n",
    "    fromcorpusdata = readcorpus()\n",
    "    print(fromcorpusdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전처리[1] 문서 -> 문장 단위로 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 토큰 : 긴 문자열을 분석을 위한 작은 단위로 나누는 과정, 토큰함수를 이용하면 문자열을 입력받아 토큰 문자열의 리스트를 출력함 \n",
    "- nltk.tokenize 라는 모듈 안에 sent_tokenize 함수 사용해서 문장(sentence) 단위로 토큰화 할 수 있음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jeeyeon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nABSTRACT\\n\\n1.', 'An abstract, or summary, is published together with a research article, giving the reader a \"preview\" of what\\'s to come.', 'Such abstracts may also be published separately in bibliographical sources, such as Biologic al Abstracts.', 'They allow other scientists to quickly scan the large scientific literature, and decide which articles they want to read in depth.', \"The abstract should be a little less technical than the article itself; you don't want to dissuade your potent ial audience from reading your paper.\", '2.', 'Your abstract should be one paragraph, of 100-250 words, which summarizes the purpose, methods, results and conclusions of the paper.', '3.', 'It is not easy to include all this information in just a few words.', 'Start by writing a summary that includes whatever you think is important, and then gradually prune it down to size by removing unnecessary words, while still retaini ng the necessary concepts.']\n",
      " \n",
      "80\n"
     ]
    }
   ],
   "source": [
    "# nltk.tokenize 모듈에서 sent_tokenize 함수를 st라는 약어로 사용하자 \n",
    "# sent_tokenize : sentence tokenize \n",
    "from nltk.tokenize import sent_tokenize as st\n",
    "\n",
    "st_list_rawfile = st(filecontentdetails) # 위에서 rawtextcorpus 읽어서 filecontentdetails 로 선언한 파일을 \"문장\" 단위로 토큰화 \n",
    "# print(st_list_rawfile) \n",
    "print(st_list_rawfile[:10]) \n",
    "print(\" \")\n",
    "print(len(st_list_rawfile))\n",
    "\n",
    "# 문장 단위로 잘 잘렸군.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Welcome readers.', 'I hope you find it interesting.', 'Please do reply.']\n"
     ]
    }
   ],
   "source": [
    "# text 가 가지고 있는 3개의 문장을 sent_tokenize 함수를 이용해서 잘 토큰화 되는지 살펴보자 \n",
    "\n",
    "text=\"Welcome readers. I hope you find it interesting. Please do reply.\"\n",
    "print(st(text))  \n",
    "\n",
    "# 문장 단위로 잘 잘렸군.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전처리 [1] 문서 -> 단어 단위로 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nltk.tokenize 라는 모듈 안에 word_tokenize 함수 사용해서 단어(word)단위로 토큰화 할 수 있음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jeeyeon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Welcome', 'readers', '.', 'I', 'hope', 'you', 'find', 'it', 'interesting', '.', 'Please', 'do', 'reply', '.']\n"
     ]
    }
   ],
   "source": [
    "# nltk.tokenize 모듈에서 word_tokenize 함수를 wt라는 약어로 사용하자 \n",
    "# word_tokenize : word tokenize \n",
    "from nltk.tokenize import word_tokenize as wt\n",
    "\n",
    "text = wt(\"Welcome readers. I hope you find it interesting. Please do reply.\")\n",
    "print(text)\n",
    "\n",
    "# 단어 단위로 잘 잘렸군 .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'paragraph', ',', 'of', '100-250', 'words', ',', 'which', 'summarizes', 'the', 'purpose', ',', 'methods', ',', 'results', 'and', 'conclusions', 'of', 'the', 'paper', '.', 'It', 'is', 'not', 'easy', 'to', 'include', 'all', 'this', 'information', 'in', 'just', 'a', 'few', 'words', '.', 'Start', 'by', 'writing', 'a', 'summary', 'that', 'includes', 'whatever', 'you', 'think', 'is', 'important', ',', 'and', 'then', 'gradually', 'prune', 'it', 'down', 'to', 'size', 'by', 'removing', 'unnecessary', 'words', ',', 'while', 'still', 'retaini', 'ng', 'the', 'necessary', 'concepts', '.', 'Do', \"n't\", 'use', 'abbreviations', 'or', 'citations', 'in', 'the', 'abstract', '.', 'It', 'should', 'be', 'able', 'to', 'stand', 'alone', 'without', 'any', 'footnotes', '.', 'Fig', '1.1.1', 'shows', 'below', '.']\n",
      "\n",
      "96\n"
     ]
    }
   ],
   "source": [
    "text = nltk.word_tokenize(\"\"\" one paragraph, of 100-250 words, which summarizes the purpose, methods, results and conclusions of the paper.\n",
    "    It is not easy to include all this information in just a few words. Start by writing a summary that includes whatever you think is important,\n",
    "    and then gradually prune it down to size by removing unnecessary words, while still retaini ng the necessary concepts.\n",
    "    Don't use abbreviations or citations in the abstract. It should be able to stand alone without any footnotes. Fig 1.1.1 shows below.\"\"\")\n",
    "\n",
    "print(text)\n",
    "print(\"\")\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'paragraph', ',', 'of', '100-250', 'words', ',', 'which', 'summarizes', 'the', 'purpose', ',', 'methods', ',', 'results', 'and', 'conclusions', 'of', 'the', 'paper', '.', 'It', 'is', 'not', 'easy', 'to', 'include', 'all', 'this', 'information', 'in', 'just', 'a', 'few', 'words', '.', 'Start', 'by', 'writing', 'a', 'summary', 'that', 'includes', 'whatever', 'you', 'think', 'is', 'important', ',', 'and', 'then', 'gradually', 'prune', 'it', 'down', 'to', 'size', 'by', 'removing', 'unnecessary', 'words', ',', 'while', 'still', 'retaini', 'ng', 'the', 'necessary', 'concepts', '.', 'Do', \"n't\", 'use', 'abbreviations', 'or', 'citations', 'in', 'the', 'abstract', '.', 'It', 'should', 'be', 'able', 'to', 'stand', 'alone', 'without', 'any', 'footnotes', '.', 'Fig', '1.1.1', 'shows', 'below', '.']\n",
      "\n",
      "96\n"
     ]
    }
   ],
   "source": [
    "# 위와 같은 결과 \n",
    "text = wt(\"\"\" one paragraph, of 100-250 words, which summarizes the purpose, methods, results and conclusions of the paper.\n",
    "    It is not easy to include all this information in just a few words. Start by writing a summary that includes whatever you think is important,\n",
    "    and then gradually prune it down to size by removing unnecessary words, while still retaini ng the necessary concepts.\n",
    "    Don't use abbreviations or citations in the abstract. It should be able to stand alone without any footnotes. Fig 1.1.1 shows below.\"\"\")\n",
    "\n",
    "print(text)\n",
    "print(\"\")\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word tokenize 는 몇 가지 방법이 있는데 그 중 TreebankWordTokenizer, WordPunctTokenizer 방법을 살펴보자.    두 가지 방법은 어떻게 분해하는지에 대한 약간의 차이가 존재한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. TreebankWordTokenizer 클래스\n",
    "- 축약형을 분해한다는 특징이 있음 ex) Don't -> Do 와 n't 로 분해함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "print(tokenizer.tokenize(\"Don't hesitate to ask questions\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 2. WordPunctTokenizer 클래스는 \n",
    "- 모든 구두점 단위로 분해함 ex) Don't -> Don , ' , t 로 분해함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tokenizer=WordPunctTokenizer()\n",
    "print(tokenizer.tokenize(\"Don't hesitate to ask questions\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전처리[2] : 불용어 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 불용어란 ? \n",
    "- 문장에서는 자주 등장하지만 실제 의미 분석을 하는데는 거의 기여하는 바가 없는 단어를 의미 \n",
    "- 유의미한 단어 토큰만을 선별하기 위해 의미 없는 토큰 제거하는 작업이 필요 \n",
    "- ex) I, my, me, over, 조사, 접미사\n",
    "- 힘들게 다운받은 nltk 패키지에서는 영어단어 중 불용어를 정의해 놓아서 쉽게 사용할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK에 내장된 영어의 불용어 목록 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jeeyeon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# nltk가 영어에서 어떤 단어를 불용어라고 정의하고 있는지 살펴보자 \n",
    "\n",
    "import nltk  \n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords  \n",
    "print(stopwords.words('english'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Don't\", 'hesitate', 'to', 'ask', 'questions']\n",
      "[\"Don't\", 'hesitate', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "# 실제로 불용어 제거되는지 적용해보자 \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "words=[\"Don't\", 'hesitate','to','ask','questions']\n",
    "\n",
    "print(words)\n",
    "print([word for word in words if word not in stops]) # words 중에서 불용어에 포함되어 있지 않은 단어만을 뽑아줘라 \n",
    "\n",
    "# 결과적으로 불용어 \"to\"가 제거되었다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don't\n",
      "hesitate\n",
      "ask\n",
      "questions\n"
     ]
    }
   ],
   "source": [
    "for word in words:          # words 안에 있는 word 에 대해서 \n",
    "    if word not in stops:   # stops 에 존재하지 않는 word 에 대해서\n",
    "        print(word)         # 출력하라 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK에 불용어 목록이 저장된 언어를 확인해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arabic', 'azerbaijani', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish', 'turkish']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.fileids())  # korean 은 없어 ㅠ ㅠ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전처리[3] : 정규화(Normalization)  -오타수정 느낌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 소문자/대문자 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardwork is key to success\n",
      "HARDWORK IS KEY TO SUCCESS\n"
     ]
    }
   ],
   "source": [
    "text='HARdWork IS KEy to SUCCESS'\n",
    "print(text.lower())  # 전부 소문자로 \n",
    "print(text.upper())  # 전부 대문자로 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 특수문자, 부호 삭제방법(1) :replace() 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She go She go.\n",
      "She go She go\n"
     ]
    }
   ],
   "source": [
    "text = \"She go? She go.\"\n",
    "\n",
    "text_r = text.replace(\"?\", \"\")        # ? 라는 문자를 공백으로 바꾸시오 \n",
    "print(text_r)\n",
    "\n",
    "text_r1 = text_r.replace(\".\", \"\")     # . 라는 문자를 공백으로 바꾸시오 \n",
    "print(text_r1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 특수문자, 부호 삭제방법(2) : 정규식(re) 모듈 사용법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 정규표현식 : 텍스트에서 특정 문자열을 검색하거나 치환할 때 사용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' It is a pleasant evening.', 'Guests, who came from US arrived at the venue', 'Food was tasty.']\n"
     ]
    }
   ],
   "source": [
    "# 1 word_tokenize 이용해서 단어 단위로 토큰화 해보자 \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text=[\" It is a pleasant evening.\",\"Guests, who came from US arrived at the venue\",\"Food was tasty.\"]\n",
    "#tokenized_docs = [word_tokenize(doc) for doc in text]  \n",
    "tokenized_docs = [doc for doc in text]                               # 의미 : 그냥 쓰면 text 를 문장단위로 잘라줌 \n",
    "\n",
    "print(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['It', 'is', 'a', 'pleasant', 'evening', '.'], ['Guests', ',', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Food', 'was', 'tasty', '.']]\n"
     ]
    }
   ],
   "source": [
    "# 1 word_tokenize 이용해서 단어 단위로 토큰화 해보자 \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text=[\" It is a pleasant evening.\",\"Guests, who came from US arrived at the venue\",\"Food was tasty.\"]\n",
    "tokenized_docs = [word_tokenize(doc) for doc in text]               # 의미 : word 단위로 토큰화해서 출력할것이다 \n",
    "#tokenized_docs = [doc for doc in text]  \n",
    "\n",
    "print(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['It', 'is', 'a', 'pleasant', 'evening', '.'], ['Guests', ',', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Food', 'was', 'tasty', '.']]\n"
     ]
    }
   ],
   "source": [
    "# 위와 같은 단어단위 토큰화해서 list 생성하는 걸 이렇게도 할 수 있다~ 빈리스트사용하기 \n",
    "\n",
    "tokenized_docs=[]\n",
    "for doc in text:\n",
    "    word = word_tokenize(doc)\n",
    "    tokenized_docs.append(word)\n",
    "    \n",
    "print(tokenized_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위에서는 tokenized_docs가 어떤 형태로 구성되어있는 건지 본 것이고 여기서 구두점을 제외하기 위해 아래와 같은 절차 사용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['It', 'is', 'a', 'pleasant', 'evening', '.'], ['Guests', ',', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Food', 'was', 'tasty', '.']]\n",
      "=========\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "=========\n",
      "--------------\n",
      "[['It', 'is', 'a', 'pleasant', 'evening'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Food', 'was', 'tasty']]\n"
     ]
    }
   ],
   "source": [
    "import re    # 정규식 사용하기 위해 re 모듈을 가져오고  \n",
    "import string \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text=[\" It is a pleasant evening.\",\"Guests, who came from US arrived at the venue\",\"Food was tasty.\"]\n",
    "tokenized_docs=[word_tokenize(doc) for doc in text]   # 단어단위로 토큰화해서 tokenized_docs 리스트에 저장\n",
    "print(tokenized_docs)\n",
    "\n",
    "print(\"=========\")\n",
    "print(string.punctuation)\n",
    "print(\"=========\")\n",
    "x = re.compile( '[%s]' % re.escape(string.punctuation) )\n",
    "           # re.compile :정규표현식을 compile(파이썬한테 전해주는거),찾고자하는 패턴이 빈번한 경우 미리 컴파일해놓으면 속도,편의성good\n",
    "           # [] : 이 안에 문자를 넣으면 한 개의 문자와 매치를 의미함, \n",
    "           # re.escape ; \n",
    "# 즉 x 라는 것은 string.punctuation 에 있는 걸 없애주는 걸 의미함! \n",
    "            \n",
    "            \n",
    "            \n",
    "tokenized_docs_no_punctuation = []  \n",
    "\n",
    "for review in tokenized_docs:  \n",
    "    #print(review)      # review 형태는 -> ['It', 'is', 'a', 'pleasant', 'evening', '.']\n",
    "    new_review = []\n",
    "    for token in review: \n",
    "        #print(token)       # token 형태는  It is a .... \n",
    "        new_token = x.sub(u'', token)   # new_token 에는 \n",
    "        if not new_token == u'':\n",
    "            new_review.append(new_token)\n",
    "        #print(new_token)   # new_token 형태는 위에 x 에서 compile 한거에 있는 건 뺀거 \n",
    "        #print(new_review)   # 오잉 신기한 형태군,, ['It'], ['It', 'is'] ... 이렇게 반복 \n",
    "    tokenized_docs_no_punctuation.append(new_review) \n",
    "    \n",
    "print(\"--------------\")\n",
    "print(tokenized_docs_no_punctuation)\n",
    "\n",
    "# 요렇게 하면 최종적으로 string.punctuation 에 있는 . , 이런 구두점을 제외해줌 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 약어 전개 방법 (1) replace() 메소드 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She must've gone to the market but she didn't go\n",
      "She must've gone to the market but she did not go\n"
     ]
    }
   ],
   "source": [
    "# 왜 여러개의 패턴을 모듈로 생성해 사용하는 것이 필요한지 예시\n",
    "\n",
    "text = \"She must've gone to the market but she didn't go\"\n",
    "\n",
    "text_r = text.replace(\"t've\" , \"t have\")  # t've 를 t have 로 바꿀거야 \n",
    "text_r = text.replace(\"n't\", \" not\")      \n",
    "\n",
    "print(text)\n",
    "print(text_r)  \n",
    "\n",
    "# 오잉 첫번째꺼 replace가 반영 안됨 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 약어 전개 방법 (2) 기존에 특정 패턴을 지정해 모듈로 생성하여 재사용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do not hesitate to ask questions\n",
      "She must have gone to the market but she did not go\n"
     ]
    }
   ],
   "source": [
    "from replacers import RegexpReplacer\n",
    "\n",
    "replacer = RegexpReplacer()\n",
    "\n",
    "print(replacer.replace(\"Don't hesitate to ask questions\"))\n",
    "# print(RegexpReplacer().replace(\"Don't hesitate to ask questions\")) # 이렇게 직접사용가능\n",
    "print(replacer.replace(\"She must've gone to the market but she didn't go\"))\n",
    "\n",
    "# RegexpReplacer :축약형을 원래 형태로 바꿔주는 모듈 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_patterns = [\n",
    "\t(r'won\\'t', 'will not'),\n",
    "\t(r'can\\'t', 'cannot'),\n",
    "\t(r'i\\'m', 'i am'),\n",
    "\t(r'ain\\'t', 'is not'),\n",
    "\t(r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "\t(r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "\t(r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "\t(r'(\\w+)\\'s', '\\g<1> is'),\n",
    "\t(r'(\\w+)\\'re', '\\g<1> are'),\n",
    "\t(r'(\\w+)\\'d', '\\g<1> would'),\n",
    "]\n",
    "# 요렇게 구성이 되어있음 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전처리[4] : 어간추출(Stemming) - 언어의 압축\n",
    "- 어간 : 단어의 의미를 담고 있는 핵심부분 \n",
    "- 영어 어간추출에는 PoterStemmer 가 많이 활용됨 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work\n",
      "happi\n",
      "pair\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "stemmerporter = PorterStemmer()\n",
    "\n",
    "print(stemmerporter.stem('working'))\n",
    "print(stemmerporter.stem('happiness'))\n",
    "print(stemmerporter.stem('pairing'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 어간추출(Stemming) : 단어에서 접사를 제거하는 것\n",
    "###  vs \n",
    "### 원형복원(lemmatization) : 어원(root word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jeeyeon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "happi\n",
      "happiness\n",
      "============\n",
      "believ\n",
      "belief\n",
      "============\n",
      "cook\n",
      "cooking\n",
      "============\n",
      "cookeri\n",
      "cookery\n",
      "============\n",
      "working\n",
      "work\n",
      "work\n",
      "work\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer_output=PorterStemmer()\n",
    "lemmatizer_output=WordNetLemmatizer()\n",
    "\n",
    "print(stemmer_output.stem('happiness'))\n",
    "print(lemmatizer_output.lemmatize('happiness'))\n",
    "print(\"============\")\n",
    "print(stemmer_output.stem('believes')) \n",
    "print(lemmatizer_output.lemmatize('believes'))\n",
    "print(\"============\")\n",
    "print(stemmer_output.stem('cooking'))\n",
    "print(lemmatizer_output.lemmatize('cooking'))\n",
    "print(\"============\")\n",
    "print(stemmerporter.stem('cookery'))\n",
    "print(lemmatizer_output.lemmatize('cookery'))\n",
    "print(\"============\")\n",
    "print(lemmatizer_output.lemmatize('working'))\n",
    "print(lemmatizer_output.lemmatize('working', pos = 'v'))\n",
    "print(lemmatizer_output.lemmatize('works'))\n",
    "print(lemmatizer_output.lemmatize('works', pos = 'v'))\n",
    "# POS(Part of Speech) = v(동사,verb)\n",
    "\n",
    "\n",
    "# 즉 어간추출을 해주는 PorterStemmer는 뒤에 es를 걍 잘라주고 \n",
    "# 원형복원을 해주는 WordNetLemmatizer는 원어를 잘라서 결과를 보여준다 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전처리[5] : 정규화(Normalization)  -오타수정 느낌이니까"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lot\n",
      "oh\n",
      "ooh\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from replacers import RepeatReplacer\n",
    "\n",
    "replacer=RepeatReplacer()\n",
    "print(replacer.replace('lotttt'))\n",
    "print(replacer.replace('ohhhhh'))\n",
    "print(replacer.replace('ooohhhhh'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. 한글 텍스트 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'눈이 부시게'가 가뿐하게 지상파 월화극을 따돌리며 6%를 돌파했다. \n",
      "27일 시청률 조사회사 닐슨 코리아에 따르면 26일 방송된 JTBC 월화극 '눈이 부시게'는 6.567%(전국 유료가구 기준)의 시청률을 기록했다. 5회 연속 자체 최고 시청률을 찍으며 멈출 줄 모르는 상승세를 이어가고 있다.\n",
      "동시에 첫 6%대 돌파였다. 동 시간대 방송된 지상파 3사 월화극 SBS '해치' KBS 2TV '동네변호사 조들호2:죄와 벌' MBC '아이템'을 따돌리고 우위를 점했다. tvN '왕이 된 남자'(9.5%)를 잇는 월화극 전체 2위에 이름을 올렸다. '왕이 된 남자'의 경우 종영을 앞두고 있기에 '눈이 부시게'가 어디까지 상승할 수 있을지 주목된다.  \n",
      "이날 방송에는 김혜자(김혜자)가 방송 말미 시간을 되돌리는 시계를 발견하는 모습이 그려졌다. 전무송이 이 시계를 차고 있었고 시계를 본 후 눈빛이 심하게 흔들린 김혜자의 모습을 통해 다시금 시간을 되돌릴 수 있을지 여부에 관심이 쏠렸다. \n"
     ]
    }
   ],
   "source": [
    "input_file_name = r\"textdata.txt\"\n",
    "\n",
    "with open(input_file_name, \"r\", encoding = \"EUC-KR\") as input_file: # input_file_name 에 있는걸 read 모드로 열건데 input_file 이라고 할게\n",
    "    #print(input_file)        # 요렇게 해주면 설명밖에 안나와 \n",
    "    text = input_file.read()  # 파일.read() 해줘야지 파일 읽을 수 있어\n",
    "    print(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `read()` 메소드를 사용해서 파일을 통채로 불러오면 토큰화 수행이 불가능, 따라서 for 문을 이용해 파일에 입력된 값을 line으로 읽어와야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'눈이\", \"부시게'가\", '가뿐하게', '지상파', '월화극을', '따돌리며', '6%를', '돌파했다.']\n",
      "['27일', '시청률', '조사회사', '닐슨', '코리아에', '따르면', '26일', '방송된', 'JTBC', '월화극', \"'눈이\", \"부시게'는\", '6.567%(전국', '유료가구', '기준)의', '시청률을', '기록했다.', '5회', '연속', '자체', '최고', '시청률을', '찍으며', '멈출', '줄', '모르는', '상승세를', '이어가고', '있다.']\n",
      "['동시에', '첫', '6%대', '돌파였다.', '동', '시간대', '방송된', '지상파', '3사', '월화극', 'SBS', \"'해치'\", 'KBS', '2TV', \"'동네변호사\", '조들호2:죄와', \"벌'\", 'MBC', \"'아이템'을\", '따돌리고', '우위를', '점했다.', 'tvN', \"'왕이\", '된', \"남자'(9.5%)를\", '잇는', '월화극', '전체', '2위에', '이름을', '올렸다.', \"'왕이\", '된', \"남자'의\", '경우', '종영을', '앞두고', '있기에', \"'눈이\", \"부시게'가\", '어디까지', '상승할', '수', '있을지', '주목된다.']\n",
      "['이날', '방송에는', '김혜자(김혜자)가', '방송', '말미', '시간을', '되돌리는', '시계를', '발견하는', '모습이', '그려졌다.', '전무송이', '이', '시계를', '차고', '있었고', '시계를', '본', '후', '눈빛이', '심하게', '흔들린', '김혜자의', '모습을', '통해', '다시금', '시간을', '되돌릴', '수', '있을지', '여부에', '관심이', '쏠렸다.']\n"
     ]
    }
   ],
   "source": [
    "input_file_name = r\"textdata.txt\"\n",
    "\n",
    "with open(input_file_name, \"r\", encoding = \"EUC-KR\") as input_file:\n",
    "    \n",
    "    for line in input_file:  # input_file 을 for 문을 이용해서 문장을 하나씩 분리해줌 \n",
    "        #print(line)\n",
    "        line = line.split()  # split 함수를 이용해서 for 문에서 문장 분리했던 걸 다시 단어로 분리해줌 \n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## strip() 메소드 사용 \n",
    "- `strip() 메소드`는 해당 문자열의 앞뒤에 붙은 화이트 스페이스 문자(공백 문자, 탭 문자, 줄바꿈 문자)를 모두 없앤다. \n",
    "- 아래 예제를 통해 이 동작을 확인할 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* \t    aaaaa bbbb ccccc ddddd efef \t \n",
      "   *\n",
      "=========================\n",
      "*aaaaa bbbb ccccc ddddd efef*\n"
     ]
    }
   ],
   "source": [
    "a = \" \\t    aaaaa bbbb ccccc ddddd efef \\t \\n   \"\n",
    "\n",
    "print(\"*\" + a + \"*\")\n",
    "# a 변수가 공백문자, 탭문자(\\t), 줄바꿈문자(\\n)를 포함하고 있다\n",
    "\n",
    "\n",
    "print(\"=========================\")\n",
    "b = a.strip()\n",
    "print(\"*\" + b + \"*\")\n",
    "# strip()메소드 사용후 b의 변수가 공백문자, 탭문자(\\t), 줄바꿈문자(\\n)가 삭제되었음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split() 메소드 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `split()` 메소드는 문자열에 대한 메소드로, 주어진 문자열을 임의의 구분자를 기준으로 분절한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['이', '화', '여', '자', '대', '학', '교']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= \"이 화 여 자 대 학 교\"    \n",
    "\n",
    "a.split(\" \")\n",
    "a.split() # 같은 방법 , 즉 공백 기준으로 분절 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['이', '화', '여', '자', '대', '학', '교']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b= \"이/화/여/자/대/학/교\"\n",
    "\n",
    "b.split(\"/\")  # / 기준으로 분절 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이', '화', '여', '자', '대', '학', '교']\n",
      "['이', '화', '여', '자', '대', '학', '교']\n"
     ]
    }
   ],
   "source": [
    "a= \"이 화 여 자 대 학 교\"    \n",
    "\n",
    "# 방법1 \n",
    "a1= a.split()\n",
    "print(a1) # 공백을 기준으로 분절\n",
    "\n",
    "# 방법2\n",
    "a2 =[]\n",
    "for word in a1:\n",
    "    #print(word) 한글자 한글자 분리\n",
    "    a2 += word \n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문장 토큰화 [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'눈이 부시게'가 가뿐하게 지상파 월화극을 따돌리며 6%를 돌파했다. \n",
      "\n",
      "27일 시청률 조사회사 닐슨 코리아에 따르면 26일 방송된 JTBC 월화극 '눈이 부시게'는 6.567%(전국 유료가구 기준)의 시청률을 기록했다. 5회 연속 자체 최고 시청률을 찍으며 멈출 줄 모르는 상승세를 이어가고 있다.\n",
      "\n",
      "동시에 첫 6%대 돌파였다. 동 시간대 방송된 지상파 3사 월화극 SBS '해치' KBS 2TV '동네변호사 조들호2:죄와 벌' MBC '아이템'을 따돌리고 우위를 점했다. tvN '왕이 된 남자'(9.5%)를 잇는 월화극 전체 2위에 이름을 올렸다. '왕이 된 남자'의 경우 종영을 앞두고 있기에 '눈이 부시게'가 어디까지 상승할 수 있을지 주목된다.  \n",
      "\n",
      "이날 방송에는 김혜자(김혜자)가 방송 말미 시간을 되돌리는 시계를 발견하는 모습이 그려졌다. 전무송이 이 시계를 차고 있었고 시계를 본 후 눈빛이 심하게 흔들린 김혜자의 모습을 통해 다시금 시간을 되돌릴 수 있을지 여부에 관심이 쏠렸다. \n"
     ]
    }
   ],
   "source": [
    "# Step 1 파일확인 \n",
    "input_file_name = r\"textdata.txt\"\n",
    "\n",
    "with open(input_file_name, \"r\", encoding=\"EUC-KR\") as input_file:\n",
    "    text_words=[]\n",
    "    for line in input_file:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'눈이 부시게'가 가뿐하게 지상파 월화극을 따돌리며 6%를 돌파했다.\n",
      "27일 시청률 조사회사 닐슨 코리아에 따르면 26일 방송된 JTBC 월화극 '눈이 부시게'는 6.567%(전국 유료가구 기준)의 시청률을 기록했다. 5회 연속 자체 최고 시청률을 찍으며 멈출 줄 모르는 상승세를 이어가고 있다.\n",
      "동시에 첫 6%대 돌파였다. 동 시간대 방송된 지상파 3사 월화극 SBS '해치' KBS 2TV '동네변호사 조들호2:죄와 벌' MBC '아이템'을 따돌리고 우위를 점했다. tvN '왕이 된 남자'(9.5%)를 잇는 월화극 전체 2위에 이름을 올렸다. '왕이 된 남자'의 경우 종영을 앞두고 있기에 '눈이 부시게'가 어디까지 상승할 수 있을지 주목된다.\n",
      "이날 방송에는 김혜자(김혜자)가 방송 말미 시간을 되돌리는 시계를 발견하는 모습이 그려졌다. 전무송이 이 시계를 차고 있었고 시계를 본 후 눈빛이 심하게 흔들린 김혜자의 모습을 통해 다시금 시간을 되돌릴 수 있을지 여부에 관심이 쏠렸다.\n"
     ]
    }
   ],
   "source": [
    "# Step 2 strip 이용\n",
    "input_file_name = r\"textdata.txt\"\n",
    "\n",
    "with open(input_file_name, \"r\", encoding=\"EUC-KR\") as input_file:\n",
    "    text_words=[]\n",
    "    for line in input_file:\n",
    "        #print(line)\n",
    "        line = line.strip()     # strip 함수 이용해서 화이트 스페이스 문자(공백 문자, 탭 문자, 줄바꿈 문자) 없앰\n",
    "        print(line)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "[\"'눈이 부시게'가 가뿐하게 지상파 월화극을 따돌리며 6%를 돌파했다.\", \"27일 시청률 조사회사 닐슨 코리아에 따르면 26일 방송된 JTBC 월화극 '눈이 부시게'는 6.567%(전국 유료가구 기준)의 시청률을 기록했다\", '5회 연속 자체 최고 시청률을 찍으며 멈출 줄 모르는 상승세를 이어가고 있다.', '동시에 첫 6%대 돌파였다', \"동 시간대 방송된 지상파 3사 월화극 SBS '해치' KBS 2TV '동네변호사 조들호2:죄와 벌' MBC '아이템'을 따돌리고 우위를 점했다\", \"tvN '왕이 된 남자'(9.5%)를 잇는 월화극 전체 2위에 이름을 올렸다\", \"'왕이 된 남자'의 경우 종영을 앞두고 있기에 '눈이 부시게'가 어디까지 상승할 수 있을지 주목된다.\", '이날 방송에는 김혜자(김혜자)가 방송 말미 시간을 되돌리는 시계를 발견하는 모습이 그려졌다', '전무송이 이 시계를 차고 있었고 시계를 본 후 눈빛이 심하게 흔들린 김혜자의 모습을 통해 다시금 시간을 되돌릴 수 있을지 여부에 관심이 쏠렸다.']\n"
     ]
    }
   ],
   "source": [
    "# Step 3 split 이용   \". \" 기준으로 값 분리 \n",
    "input_file_name = r\"textdata.txt\"\n",
    "\n",
    "with open(input_file_name, \"r\", encoding=\"EUC-KR\") as input_file:\n",
    "    text_words=[]\n",
    "    for line in input_file:\n",
    "        #print(line)\n",
    "        line = line.strip()     # strip 함수 이용해서 화이트 스페이스 문자(공백 문자, 탭 문자, 줄바꿈 문자) 없앰\n",
    "        #print(line)   \n",
    "        #line = line.split(\". \")# 일반적으로 문장 구분은 마침표(.) 기준으로 이뤄지기 때문에 split 함수를 마침표를 기준으로 분절함.\n",
    "        words = line.split(\". \")\n",
    "        #print(line)\n",
    "        #print(words)\n",
    "        text_words += words      # 빈 리스트였던 text_words 를 최종적인 words 가 들어간 리스트로 만들어줌 \n",
    "\n",
    "print(len(text_words))\n",
    "print(text_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116\n",
      "[\"'눈이\", \"부시게'가\", '가뿐하게', '지상파', '월화극을', '따돌리며', '6%를', '돌파했다.', '27일', '시청률', '조사회사', '닐슨', '코리아에', '따르면', '26일', '방송된', 'JTBC', '월화극', \"'눈이\", \"부시게'는\", '6.567%(전국', '유료가구', '기준)의', '시청률을', '기록했다.', '5회', '연속', '자체', '최고', '시청률을', '찍으며', '멈출', '줄', '모르는', '상승세를', '이어가고', '있다.', '동시에', '첫', '6%대', '돌파였다.', '동', '시간대', '방송된', '지상파', '3사', '월화극', 'SBS', \"'해치'\", 'KBS', '2TV', \"'동네변호사\", '조들호2:죄와', \"벌'\", 'MBC', \"'아이템'을\", '따돌리고', '우위를', '점했다.', 'tvN', \"'왕이\", '된', \"남자'(9.5%)를\", '잇는', '월화극', '전체', '2위에', '이름을', '올렸다.', \"'왕이\", '된', \"남자'의\", '경우', '종영을', '앞두고', '있기에', \"'눈이\", \"부시게'가\", '어디까지', '상승할', '수', '있을지', '주목된다.', '이날', '방송에는', '김혜자(김혜자)가', '방송', '말미', '시간을', '되돌리는', '시계를', '발견하는', '모습이', '그려졌다.', '전무송이', '이', '시계를', '차고', '있었고', '시계를', '본', '후', '눈빛이', '심하게', '흔들린', '김혜자의', '모습을', '통해', '다시금', '시간을', '되돌릴', '수', '있을지', '여부에', '관심이', '쏠렸다.']\n"
     ]
    }
   ],
   "source": [
    "# Step 3 split 이용    공백 기준으로 값 분리 \n",
    "input_file_name = r\"textdata.txt\"\n",
    "\n",
    "with open(input_file_name, \"r\", encoding=\"EUC-KR\") as input_file:\n",
    "    text_words=[]\n",
    "    for line in input_file:\n",
    "        #print(line)\n",
    "        line = line.strip()      # strip 함수 이용해서 화이트 스페이스 문자(공백 문자, 탭 문자, 줄바꿈 문자) 없앰\n",
    "        #print(line)   \n",
    "        words = line.split() # 일반적으로 문장 구분은 마침표(.) 기준으로 이뤄지기 때문에 split 함수를 마침표를 기준으로 분절함.\n",
    "        #print(line)\n",
    "        #print(words)\n",
    "        text_words += words      # 빈 리스트였던 text_words 를 최종적인 words 가 들어간 리스트로 만들어줌 \n",
    "\n",
    "print(len(text_words))\n",
    "print(text_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## splitlines() 메소드 사용\n",
    "`splitlines()` 메소드는 `줄바꿈`을 기준으로 문자열을 여러 개로 나뉜뒤 리스트의 요소로 반환한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "죽는 날까지 하늘을 우러러\n",
      "한 점 부끄럼이 없기를,\n",
      "잎새에 이는 바람에도\n",
      "나는 괴로워했다.\n",
      "\n",
      "죽는 날까지 하늘을 우러러\n",
      "한 점 부끄럼이 없기를,\n",
      "잎새에 이는 바람에도\n",
      "나는 괴로워했다.\n",
      "죽는 날까지 하늘을 우러러 한 점 부끄럼이 없기를, 잎새에 이는 바람에도 나는 괴로워했다.\n",
      "========================================================================================\n",
      "['죽는 날까지 하늘을 우러러', '한 점 부끄럼이 없기를,', '잎새에 이는 바람에도', '나는 괴로워했다.']\n",
      "4\n",
      "['죽는 날까지 하늘을 우러러', '한 점 부끄럼이 없기를,', '잎새에 이는 바람에도', '나는 괴로워했다.']\n",
      "4\n",
      "['죽는 날까지 하늘을 우러러 한 점 부끄럼이 없기를, 잎새에 이는 바람에도 나는 괴로워했다.']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# 줄바꿈을 \\n 으로 해도 같은 결과가 나타난다 \n",
    "poem_input1 = \"\"\"죽는 날까지 하늘을 우러러\n",
    "한 점 부끄럼이 없기를,\n",
    "잎새에 이는 바람에도\n",
    "나는 괴로워했다.\n",
    "\"\"\"\n",
    "poem_input2 = \"\"\"죽는 날까지 하늘을 우러러\\n한 점 부끄럼이 없기를,\\n잎새에 이는 바람에도\\n나는 괴로워했다.\"\"\"\n",
    "poem_input3 = \"\"\"죽는 날까지 하늘을 우러러 한 점 부끄럼이 없기를, 잎새에 이는 바람에도 나는 괴로워했다.\"\"\"\n",
    "print(poem_input1)\n",
    "print(poem_input2)\n",
    "print(poem_input3)\n",
    "print(\"========================================================================================\")\n",
    "\n",
    "poem1 = poem_input1.splitlines()\n",
    "print(poem1);print(len(poem1))\n",
    "poem2 = poem_input2.splitlines()\n",
    "print(poem2);print(len(poem2))\n",
    "poem3 = poem_input3.splitlines()\n",
    "print(poem3);print(len(poem3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문장 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">단어 토큰화 전에 문서를 문장으로 분절하는 것은 영어나 한국어나 동일한 과정이다.\n",
    "\n",
    ">이제까지 마침표(.)를 기준으로 문서의 문장들을 분절했으나, 마침표 외에도 마침표(.), 물음표(?), 느낌표(!)와 같은 문장 구분자들이 존재한다.\n",
    "\n",
    ">그런데 이들 구분자가 때로 문장의 종결이 아닌 곳에서도 사용될 수 있기 때문에 이들 부호에 공백 문자가 연이어진 경우를 문장의 구분이 이루어지는 것으로 보는 것이 안전하다. \n",
    "- 실제 구현에 있어서는 문장의 구분을 줄의 구분과 일치시켜서 문장의 구분이 텍스트 파일의 외현적 구조에 반영되도록 하는 것이 편리하다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'눈이 부시게'가 가뿐하게 지상파 월화극을 따돌리며 6%를 돌파했다.\", \"27일 시청률 조사회사 닐슨 코리아에 따르면 26일 방송된 JTBC 월화극 '눈이 부시게'는 6.567%(전국 유료가구 기준)의 시청률을 기록? 5회 연속 자체 최고 시청률을 찍으며 멈출 줄 모르는 상승세를 이어가고 있다!\", '동시에 첫 6%대 돌파였다', \"동 시간대 방송된 지상파 3사 월화극 SBS '해치' KBS 2TV '동네변호사 조들호2:죄와 벌' MBC '아이템'을 따돌리고 우위를 점했다\", \"tvN '왕이 된 남자'(9.5%)를 잇는 월화극 전체 2위에 이름을 올렸다\", \"'왕이 된 남자'의 경우 종영을 앞두고 있기에 '눈이 부시게'가 어디까지 상승할 수 있을지 주목된다!\", '이날 방송에는 김혜자(김혜자)가 방송 말미 시간을 되돌리는 시계를 발견하는 모습이 그려졌다? 전무송이 이 시계를 차고 있었고 시계를 본 후 눈빛이 심하게 흔들린 김혜자의 모습을 통해 다시금 시간을 되돌릴 수 있을지 여부에 관심이 쏠렸다.']\n",
      "7\n",
      "=======================================================================\n",
      "[\"'눈이 부시게'가 가뿐하게 지상파 월화극을 따돌리며 6%를 돌파했다.\", \"27일 시청률 조사회사 닐슨 코리아에 따르면 26일 방송된 JTBC 월화극 '눈이 부시게'는 6.567%(전국 유료가구 기준)의 시청률을 기록?\", '5회 연속 자체 최고 시청률을 찍으며 멈출 줄 모르는 상승세를 이어가고 있다!', '동시에 첫 6%대 돌파였다', \"동 시간대 방송된 지상파 3사 월화극 SBS '해치' KBS 2TV '동네변호사 조들호2:죄와 벌' MBC '아이템'을 따돌리고 우위를 점했다\", \"tvN '왕이 된 남자'(9.5%)를 잇는 월화극 전체 2위에 이름을 올렸다\", \"'왕이 된 남자'의 경우 종영을 앞두고 있기에 '눈이 부시게'가 어디까지 상승할 수 있을지 주목된다!\", '이날 방송에는 김혜자(김혜자)가 방송 말미 시간을 되돌리는 시계를 발견하는 모습이 그려졌다?', '전무송이 이 시계를 차고 있었고 시계를 본 후 눈빛이 심하게 흔들린 김혜자의 모습을 통해 다시금 시간을 되돌릴 수 있을지 여부에 관심이 쏠렸다.']\n",
      "9\n",
      "=======================================================================\n",
      "[\"'눈이 부시게'가 가뿐하게 지상파 월화극을 따돌리며 6%를 돌파했다.\", \"27일 시청률 조사회사 닐슨 코리아에 따르면 26일 방송된 JTBC 월화극 '눈이 부시게'는 6.567%(전국 유료가구 기준)의 시청률을 기록?\", '5회 연속 자체 최고 시청률을 찍으며 멈출 줄 모르는 상승세를 이어가고 있다!', '동시에 첫 6%대 돌파였다.', \"동 시간대 방송된 지상파 3사 월화극 SBS '해치' KBS 2TV '동네변호사 조들호2:죄와 벌' MBC '아이템'을 따돌리고 우위를 점했다.\", \"tvN '왕이 된 남자'(9.5%)를 잇는 월화극 전체 2위에 이름을 올렸다.\", \"'왕이 된 남자'의 경우 종영을 앞두고 있기에 '눈이 부시게'가 어디까지 상승할 수 있을지 주목된다!\", '이날 방송에는 김혜자(김혜자)가 방송 말미 시간을 되돌리는 시계를 발견하는 모습이 그려졌다?', '전무송이 이 시계를 차고 있었고 시계를 본 후 눈빛이 심하게 흔들린 김혜자의 모습을 통해 다시금 시간을 되돌릴 수 있을지 여부에 관심이 쏠렸다.']\n",
      "9\n",
      "=======================================================================\n",
      "[\"'눈이 부시게'가 가뿐하게 지상파 월화극을 따돌리며 6%를 돌파했다.\", \"27일 시청률 조사회사 닐슨 코리아에 따르면 26일 방송된 JTBC 월화극 '눈이 부시게'는 6.567%(전국 유료가구 기준)의 시청률을 기록?\", '5회 연속 자체 최고 시청률을 찍으며 멈출 줄 모르는 상승세를 이어가고 있다!', '동시에 첫 6%대 돌파였다.', \"동 시간대 방송된 지상파 3사 월화극 SBS '해치' KBS 2TV '동네변호사 조들호2:죄와 벌' MBC '아이템'을 따돌리고 우위를 점했다.\", \"tvN '왕이 된 남자'(9.5%)를 잇는 월화극 전체 2위에 이름을 올렸다.\", \"'왕이 된 남자'의 경우 종영을 앞두고 있기에 '눈이 부시게'가 어디까지 상승할 수 있을지 주목된다!\", '이날 방송에는 김혜자(김혜자)가 방송 말미 시간을 되돌리는 시계를 발견하는 모습이 그려졌다?', '전무송이 이 시계를 차고 있었고 시계를 본 후 눈빛이 심하게 흔들린 김혜자의 모습을 통해 다시금 시간을 되돌릴 수 있을지 여부에 관심이 쏠렸다.']\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "### 일반적으로 . 구분자 이용해서 문장을 split 하기 \n",
    "input_file_name = r\"textdata_add.txt\"\n",
    "\n",
    "text_sentences = []\n",
    "with open(input_file_name, \"r\", encoding=\"EUC-KR\") as input_file:\n",
    "    for line in input_file:\n",
    "        line = line.strip()\n",
    "        sentences = line.split(\". \")   # 일반적으로 문장 구분은 마침표(.) 기준으로 이뤄지기 때문에 마침표를 기준으로 분절함.\n",
    "        text_sentences +=sentences\n",
    "        \n",
    "print(text_sentences)\n",
    "print(len(text_sentences))\n",
    "\n",
    "print(\"=======================================================================\")\n",
    "\n",
    "### 방법1 . 구분자 뿐만 아니라 ? ! 이용해서 문장을 split 하기 \n",
    "input_file_name = r\"textdata_add.txt\"\n",
    "text_sentences=[]\n",
    "with open(input_file_name, \"r\", encoding=\"EUC-KR\") as input_file:\n",
    "    for line in input_file:\n",
    "        line = line.strip()\n",
    "        line = line.replace(\". \", \"\\n\")\n",
    "        line = line.replace(\"? \", \"?\\n\")\n",
    "        line = line.replace(\"! \", \"!\\n\")   \n",
    "        sub_sentences = line.splitlines() # splitlines()메소드는 아래에서 설명\n",
    "        text_sentences +=sub_sentences\n",
    "        \n",
    "print(text_sentences)\n",
    "print(len(text_sentences))\n",
    "\n",
    "print(\"=======================================================================\")\n",
    "\n",
    "### 방법2. 구분자 뿐만 아니라 ? ! 이용해서 문장을 split 하기 \n",
    "def split_sentences(text):\n",
    "    text = text.strip().replace(\". \", \".\\n\").replace(\"? \", \"?\\n\").replace(\"! \", \"!\\n\")\n",
    "    sentences = text.splitlines()\n",
    "    return sentences\n",
    "\n",
    "input_file_name = r\"textdata_add.txt\"\n",
    "text_sentences=[]\n",
    "with open(input_file_name, \"r\", encoding=\"EUC-KR\") as input_file:\n",
    "    for line in input_file:\n",
    "        sub_sentences = split_sentences(line)  # 앞서 정의한 사용자 함수 def split_sentences를 호출해 매개변수에 line을 입력\n",
    "        text_sentences += sub_sentences\n",
    "        \n",
    "print(text_sentences)\n",
    "print(len(text_sentences))\n",
    "\n",
    "print(\"=======================================================================\")\n",
    "\n",
    "### 방법3 re 모듈 사용해 정규식을 이용한 문장 토큰화\n",
    "import re   # re 모듈 호출\n",
    "def split_sentences(text):\n",
    "    sentences = re.split(\"(?<=[.?!])\\s+\", text.strip())\n",
    "    return sentences\n",
    "\n",
    "input_file_name = r\"textdata_add.txt\"\n",
    "text_sentences = []\n",
    "with open(input_file_name, \"r\", encoding=\"EUC-KR\") as input_file:\n",
    "    for line in input_file:\n",
    "        sub_sentences = split_sentences(line)  # 앞서 정의한 사용자 함수 def split_sentences를 호출해 매개변수에 line을 입력\n",
    "        text_sentences += sub_sentences\n",
    "        \n",
    "print(text_sentences)\n",
    "print(len(text_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
