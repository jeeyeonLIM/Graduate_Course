{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 형태소분석과 한국어 텍스트의 전처리 , 형태소 분석\n",
    "\n",
    ">지난시간에는 한국어 텍스트에 대한 단어 토큰화를 위해 띄어쓰기 기준으로 문장 내 단어의 의미를 분절했다.\n",
    "- 그러나 이번 시간에 배웠듯이 영어는 띄어쓰기를 기준으로 해도 단어의 최소 의미와 비교적 일치하나 한국어의 경우 교착어 특징을 갖기 때문에 띄어쓰기가 아닌 \"형태소\"가 의미의 최소 단위라고 설명했다.\n",
    "- 따라서 한국어 텍스트에 대한 토큰화는 \"띄어쓰기 기준\"이 아닌 `형태소`가 된다.\n",
    "- 이러한 이유로 한국어 텍스트 분석에서 `형태소분석`이 중요한 것은 전처리 단계 중 단어의 \"토큰화\"가 \"형태소\"에 해당되기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Twitter 가 품사가 직관적이기 때문에 선택함 , 분석 목적에 따라 형태소 분석기 적절하게 선택해서 사용해야 함\n",
    "- pos 함수 : 리스트 자료로 반환해줌 \n",
    "- counter 함수 : 사전형(dict) 자료로 반환해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeeyeon\\Anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('죽는', 'Verb'), ('날', 'Noun'), ('까지', 'Josa'), ('하늘', 'Noun'), ('을', 'Josa'), ('우러러', 'Noun'), ('한', 'Verb'), ('점', 'Noun'), ('부끄럼', 'Noun'), ('이', 'Josa'), ('없기를', 'Adjective'), (',', 'Punctuation'), ('잎새', 'Noun'), ('에', 'Josa'), ('이는', 'Verb'), ('바람', 'Noun'), ('에도', 'Josa'), ('나', 'Noun'), ('는', 'Josa'), ('괴로워', 'Adjective'), ('했다', 'Verb'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "#pos 함수쓰면 알아서 떼줘서 옆에 품사를 붙여준다 튜플들을 리스트 형태로 묶어놓는다\n",
    "\n",
    "from konlpy.tag import Twitter  \n",
    "    \n",
    "twitter = Twitter()             # 변수설정 \n",
    "text = \"죽는 날까지 하늘을 우러러 한 점 부끄럼이 없기를, 잎새에 이는 바람에도 나는 괴로워했다.\"\n",
    "text_pos = twitter.pos(text)    # Twitter 모듈안에 있는 pos 함수를 호출\n",
    "print(text_pos)                 # 리스트 내 튜플 형태(단어, 품사)로 형태소에 품사가 부착되서 출력 값 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('죽는', 'Verb')\n",
      "('날', 'Noun')\n",
      "('까지', 'Josa')\n",
      "('하늘', 'Noun')\n",
      "('을', 'Josa')\n",
      "('우러러', 'Noun')\n",
      "('한', 'Verb')\n",
      "('점', 'Noun')\n",
      "('부끄럼', 'Noun')\n",
      "('이', 'Josa')\n",
      "('없기를', 'Adjective')\n",
      "(',', 'Punctuation')\n",
      "('잎새', 'Noun')\n",
      "('에', 'Josa')\n",
      "('이는', 'Verb')\n",
      "('바람', 'Noun')\n",
      "('에도', 'Josa')\n",
      "('나', 'Noun')\n",
      "('는', 'Josa')\n",
      "('괴로워', 'Adjective')\n",
      "('했다', 'Verb')\n",
      "('.', 'Punctuation')\n"
     ]
    }
   ],
   "source": [
    "# 리스트 값을 하나씩 출력하겠다 \n",
    "\n",
    "for word_pos in text_pos:\n",
    "    print(word_pos)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "죽는\tVerb\n",
      "날\tNoun\n",
      "까지\tJosa\n",
      "하늘\tNoun\n",
      "을\tJosa\n",
      "우러러\tNoun\n",
      "한\tVerb\n",
      "점\tNoun\n",
      "부끄럼\tNoun\n",
      "이\tJosa\n",
      "없기를\tAdjective\n",
      ",\tPunctuation\n",
      "잎새\tNoun\n",
      "에\tJosa\n",
      "이는\tVerb\n",
      "바람\tNoun\n",
      "에도\tJosa\n",
      "나\tNoun\n",
      "는\tJosa\n",
      "괴로워\tAdjective\n",
      "했다\tVerb\n",
      ".\tPunctuation\n"
     ]
    }
   ],
   "source": [
    "for word, pos in text_pos:  \n",
    "    print(\"{}\\t{}\".format(word, pos))       # 튜플의 자동 언패킹 기능이 실행되고 있음을 확인 가능 \n",
    "                                            # {}\\t{} 이렇게 해주면 첫번째, 두번째 값이 각각 {}안에 들어가고 중간에는 tab으로 구분 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 튜플의 자동 언패킹(unpacking) 기능 활용법\n",
    "\n",
    "- 튜플은 () 이렇게 생긴 자료형 \n",
    "- 리스트의 원소인 튜플을 하나씩 접근한 후, 개별 튜플의 원소들을 각각 출력하기 위해서는 다음과 같이 for문을 사용하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 1)\n",
      "('b', 2)\n",
      "('c', 3)\n",
      "('d', 4)\n",
      "('e', 5)\n",
      "===================\n",
      "a 1\n",
      "b 2\n",
      "c 3\n",
      "d 4\n",
      "e 5\n",
      "===================\n",
      "a 1\n",
      "b 2\n",
      "c 3\n",
      "d 4\n",
      "e 5\n"
     ]
    }
   ],
   "source": [
    "A = [('a', 1), ('b', 2), ('c', 3), ('d', 4), ('e', 5)]  # 각 원소들이 묶여있음 , 단위로 \n",
    "\n",
    "for i in A:\n",
    "    print(i)      # 원소 하나하나 지정 안해주면 묶여서 보여줌  \n",
    "    \n",
    "print(\"===================\")\n",
    "\n",
    "for i in A:\n",
    "    print(i[0], i[1])     # 튜플을 index 사용해서 뗴놓기 \n",
    "    \n",
    "print(\"===================\")\n",
    "\n",
    "for first, second in A:\n",
    "    print(first, second)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `morphs() 메소드` : 문장에서분절된 형태소를 리스트 내 원소로 반환\n",
    "\n",
    "- `nouns()  메소드` : 문장에서 분절된 형태소들 가운데 명사만을 각각 리스트 내 원소로 반환 (텍스트 분석에서 가장 많이 사용되는 품사가 명사이기 때문에 명사만 따로 메소드 있음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('죽는', 'Verb'), ('날', 'Noun'), ('까지', 'Josa'), ('하늘', 'Noun'), ('을', 'Josa'), ('우러러', 'Noun'), ('한', 'Verb'), ('점', 'Noun'), ('부끄럼', 'Noun'), ('이', 'Josa'), ('없기를', 'Adjective'), (',', 'Punctuation'), ('잎새', 'Noun'), ('에', 'Josa'), ('이는', 'Verb'), ('바람', 'Noun'), ('에도', 'Josa'), ('나', 'Noun'), ('는', 'Josa'), ('괴로워', 'Adjective'), ('했다', 'Verb'), ('.', 'Punctuation')]\n",
      "  \n",
      "['죽는', '날', '까지', '하늘', '을', '우러러', '한', '점', '부끄럼', '이', '없기를', ',', '잎새', '에', '이는', '바람', '에도', '나', '는', '괴로워', '했다', '.']\n",
      "  \n",
      "['날', '하늘', '우러러', '점', '부끄럼', '잎새', '바람', '나']\n"
     ]
    }
   ],
   "source": [
    "text = \"죽는 날까지 하늘을 우러러 한 점 부끄럼이 없기를, 잎새에 이는 바람에도 나는 괴로워했다.\"\n",
    "\n",
    "text_pos = twitter.pos(text)            # pos 함수는 단어를 각각 뗴어서 품사까지 옆에 붙여줌\n",
    "text_words = twitter.morphs(text)       # tmorphs 함수는 형태소만 출력\n",
    "text_nouns = twitter.nouns(text)        # nouns(명사)의 품사를 가진 형태소만 출력\n",
    "\n",
    "print(text_pos)\n",
    "print(\"  \")\n",
    "print(text_words)\n",
    "print(\"  \")\n",
    "print(text_nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 텍스트를 셀 내에서 직접 입력한 후 형태소분석 수행\n",
    "\n",
    "아래 코드는 튜플의 자동 언패킹 기능을 사용하지 않고, 튜플 형태로 (형태소, 품사)를 출력하도록 했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이', '화', '여', '자', '대', '학', '교']\n",
      "['이', '화', '여', '자', '대', '학', '교']\n"
     ]
    }
   ],
   "source": [
    "# 복습 예제먼저 \n",
    "a= \"이 화 여 자 대 학 교\"\n",
    "a1 = a.split(\" \")\n",
    "print(a1)\n",
    "\n",
    "a2 = []\n",
    "for word in a1:\n",
    "    #print(word)\n",
    "    a2.append(word)\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(\"'\", 'Punctuation'), ('눈', 'Noun'), ('이', 'Josa'), ('부시', 'Noun'), ('게', 'Josa'), (\"'\", 'Punctuation'), ('가', 'Verb'), ('가뿐하게', 'Adjective'), ('지상파', 'Noun'), ('월화', 'Noun'), ('극', 'Suffix'), ('을', 'Josa'), ('따돌리며', 'Verb'), ('6%', 'Number'), ('를', 'Noun'), ('돌파', 'Noun'), ('했다', 'Verb'), ('.', 'Punctuation')]]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Twitter\n",
    "\n",
    "# 문장 종결 특수문자를 이용해 줄바꿈 기준으로 해서 문장 단위로 분리해 주는 사용자 함수 생성\n",
    "def split_sentences(text):     \n",
    "    text = text.strip().replace(\". \", \".\\n\").replace(\"? \", \"?\\n\").replace(\"! \", \"!\\n\")\n",
    "    sentences = text.splitlines()\n",
    "    return sentences\n",
    "\n",
    "# twitter 형태소 분석기를 사용해 문장 단위로 분리 후 형태소 분석 실행\n",
    "def get_pos(text): \n",
    "    morph_anals = []\n",
    "    sentences = split_sentences(text)  # 사용자 함수 내에서 다른 사용자 함수 호출 가능, 문장단위로 분리 \n",
    "    for sentence in sentences:\n",
    "        #print(sentence) # 문장단위로 잘 분리되었군 \n",
    "        morph_anal = twitter.pos(sentence)       # pos 함수 사용해서 형태소분석 \n",
    "        morph_anals.append(morph_anal)           # morph_anal의 출력 값 = [(word, pos)] \n",
    "    return morph_anals\n",
    "\n",
    "\n",
    "# main \n",
    "# 로컬입력시킴\n",
    "textdata = \"\"\" \n",
    "'눈이 부시게'가 가뿐하게 지상파 월화극을 따돌리며 6%를 돌파했다. \n",
    "27일 시청률 조사회사 닐슨 코리아에 따르면 26일 방송된 JTBC 월화극 '눈이 부시게'는 6.567%(전국 유료가구 기준)의 시청률을 기록? 5회 연속 자체 최고 시청률을 찍으며 멈출 줄 모르는 상승세를 이어가고 있다!\n",
    "동시에 첫 6%대 돌파였다. 동 시간대 방송된 지상파 3사 월화극 SBS '해치' KBS 2TV '동네변호사 조들호2:죄와 벌' MBC '아이템'을 따돌리고 우위를 점했다. tvN '왕이 된 남자'(9.5%)를 잇는 월화극 전체 2위에 이름을 올렸다. '왕이 된 남자'의 경우 종영을 앞두고 있기에 '눈이 부시게'가 어디까지 상승할 수 있을지 주목된다! \n",
    "이날 방송에는 김혜자(김혜자)가 방송 말미 시간을 되돌리는 시계를 발견하는 모습이 그려졌다? 전무송이 이 시계를 차고 있었고 시계를 본 후 눈빛이 심하게 흔들린 김혜자의 모습을 통해 다시금 시간을 되돌릴 수 있을지 여부에 관심이 쏠렸다. \n",
    "\"\"\"\n",
    "\n",
    "twitter = Twitter() # 이건 왜 해준거지 ? \n",
    "textdata_pos = get_pos(textdata)\n",
    "print(textdata_pos[:1])  # 첫 번째 문장을 형태소분석 해본 거 출력\n",
    "#print(textdata_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 외부 텍스트 파일을 불러와 형태소분석 수행\n",
    "\n",
    "아래 코드는 튜플의 자동 언패킹 기능을 사용하지 않고, 튜플 형태로 (형태소, 품사)를 출력하도록 했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[(\"'\", 'Punctuation'), ('눈', 'Noun'), ('이', 'Josa'), ('부시', 'Noun'), ('게', 'Josa'), (\"'\", 'Punctuation'), ('가', 'Verb'), ('가뿐하게', 'Adjective'), ('지상파', 'Noun'), ('월화', 'Noun'), ('극', 'Suffix'), ('을', 'Josa'), ('따돌리며', 'Verb'), ('6%', 'Number'), ('를', 'Noun'), ('돌파', 'Noun'), ('했다', 'Verb'), ('.', 'Punctuation')]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeeyeon\\Anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Twitter\n",
    "\n",
    "# 문장 종결 특수문자를 이용해 문장 단위로 분리 사용자 함수 생성\n",
    "def split_sentences(text):\n",
    "    text = text.strip().replace(\". \", \".\\n\").replace(\"? \", \"?\\n\").replace(\"! \", \"!\\n\")\n",
    "    sentences = text.splitlines() \n",
    "    return sentences\n",
    "\n",
    "# 형태소 분석기를 analyzer로 지정해준 후 사용해 문장 단위로 분리 후 형태소 분석 실행\n",
    "def get_pos(analyzer, line):\n",
    "    morph_anals = []\n",
    "    sentences = split_sentences(line)          # 위에서 정의한 def split_sentences 호출 \n",
    "    for sentence in sentences: \n",
    "        #print(sentence)\n",
    "        morph_anal = analyzer.pos(sentence)    # analyzer라는 형태소 분석기 사용 # morph_anal의 출력 값 = [(word, pos)] \n",
    "        morph_anals.append(morph_anal)      \n",
    "    return morph_anals                         # 형태소분석기를 지정해줄 뿐 결과는 하나밖에 안나오지않나\n",
    "\n",
    "\n",
    "# main \n",
    "input_file_name = r\"textdata_add.txt\"\n",
    "twitter = Twitter()\n",
    "textdata_pos = []\n",
    "with open(input_file_name, \"r\", encoding=\"EUC-KR\") as input_file:\n",
    "    for line in input_file:\n",
    "        #print(line)\n",
    "        words_pos = get_pos(twitter, line)  # 앞서 정의한 사용자 함수 def split_sentences를 호출해 매개변수에 line을 입력\n",
    "        textdata_pos.append(words_pos)     \n",
    "\n",
    "print(textdata_pos[:1])  # 첫 번째 문장을 형태소분석 해본 거 출력\n",
    "#print(textdata_pos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
