{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 여기서는 TF(Term Frequency) 기준으로 출력한 것 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV 파일 불러와서 TSV 파일 변환\n",
    "\n",
    "* CSV 파일이란? `쉼표`로 항목이 구분된 파일을 CSV(Comma-Separated Values)라고 부르며 스프레드시트, 혹은 데이터베이스에 저장된 데이터를 텍스트 파일로 변환할 때에 널리 사용된다.\n",
    "\n",
    "* TSV 파일이란? 현장에서는 구분자로 사용되는 쉼표가 텍스트의 일부에도 사용될 수 있기 때문에 쉼표 대신에 탭 문자를 구분자로 사용하는 경우가 많다. `탭 문자`가 구분자로 사용된 파일을 TSV(Tab-Separated Values라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 방법 (1) : for 문을 이용해 줄단위로 읽기\n",
    "- 아래 코드는 CSV 파일로 저장된 데이터를 TSV파일로 변환하는 과정\n",
    "- `with 문에 open() 함수`를 두 번 포함하여 입력 파일과 출력 파일을 동시에 열 수 있다.\n",
    "- 소스 코드의 한 줄이 너무 길어지면 역사선(back slash) 문자로 줄바꿈을 명시적으로 표시하고 줄바꿈을 할 수 있다. \n",
    "- 이 때 줄바꿈이 되어 넘어간 내용은 들여쓰기를 한 단계 더 하여 실제 들여쓰기 된 코드와 구분하는 것이 가독성이 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (<ipython-input-1-e3208b7586be>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-e3208b7586be>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    with open(input_file_name, \"r\", encoding=\"utf-8-sig\") as input_file, \\\u001b[0m\n\u001b[1;37m                                                                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "input_file_name = r\"movie.csv\"          # 노트패드로 보면 utf-8-bom 이라고 되어있는데 이건 곧 utf-8-sig라고 encoding 을 지정해 줘야 한다 \n",
    "output_file_name = r\"movie1.tsv\"\n",
    "\n",
    "# 'r'(read할거)  ,여기서 \\ 는 왜 해주는가: 가독성 높이기 위해서   # 'w'(write할거) , encoding 바꿔줄거야 \n",
    "\n",
    "with open(input_file_name, \"r\", encoding=\"utf-8-sig\") as input_file, \\  \n",
    "        open(output_file_name, \"w\", encoding=\"utf-8\") as output_file:   \n",
    "    for line in input_file:             \n",
    "        line = line.strip()              \n",
    "        line = line.replace(\"\\n\\n\", \" \")   # 자동 줄바꿈해주는 개행문제 삭제 ,외부 데이터를 불러올 경우 줄바꿈이 들어있기 때문에 삭제하려고\n",
    "        line = line.replace(\",\", \"\\t\")     # 개형문자 두개 들어가 있을 수도 있어서 처리해줌 \n",
    "        line = line.replace(\"\\n\", \" \")     # csv 파일이기 때문에 ,로 구분되어있으니깐 tsv 로 바꾸기 위해서 tab으로 바꿔줌 \n",
    "        print(line, file= output_file)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파일 저장 경로 확인 & 디렉토리 경로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeeyeon\\Desktop\\텍마\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd()) # 현재 경로 확인 \n",
    "\n",
    "# os.chdir(r'C:\\Users\\jeeyeon\\Desktop\\텍마1')  # 새로운 경로로 변경하고 싶을 떄 os.chdir\n",
    "#os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 방법 (2) : CSV 모듈 사용 \n",
    "- csv 모듈 공부하기 전에 enumerate , join 함수를 알아보자 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### enumerate() 함수 사용 \n",
    "\n",
    "- `enumerate()` 함수는 순서가 있는 자료형(리스트, 튜플, 문자열)을 입력으로 받아 인덱스 값을 포함하는 enumerate 객체를 리턴한다. \n",
    "- 보통 enumerate 함수는 아래 예제처럼 for문과 함께 자주 사용된다.\n",
    "\n",
    "- 순서가 있는 자료형에 대해서만 사용할 수 있기 때문에 사전형 자료에 대해서는 사용할 수 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10\n",
      "1 20\n",
      "2 30\n",
      "3 40\n",
      "4 50\n",
      "5 60\n",
      "6 70\n",
      "7 80\n",
      "8 90\n",
      "9 100\n",
      "===================\n",
      "0 body\n",
      "1 foo\n",
      "2 bar\n",
      "===================\n",
      "0 0\n",
      "1 20\n",
      "2 40\n",
      "3 60\n",
      "4 80\n",
      "5 100\n"
     ]
    }
   ],
   "source": [
    "a = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "\n",
    "for i,n in enumerate(a):\n",
    "    print(i,n)\n",
    "\n",
    "print(\"===================\")\n",
    "for i, name in enumerate(['body', 'foo', 'bar']):\n",
    "    print(i, name) # 각각의 리스트 값을 enumerate 함수를 이용해 0 1 2 이렇게 인덱스 생성해줌\n",
    "    \n",
    "print(\"===================\")\n",
    "for i, n in enumerate(range(0, 101, 20)):  # range(0,101,20) : 0 부터 1 까지 20 간격으로, 즉 0,20,40,60,80,100 이렇게 값 출력 \n",
    "    print(i, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join 메소드 사용법\n",
    "- `join() 메소드`는 문자열의 메소드로 해당 문자열을 구분자(delimeter)로 하여 주어진 문자열의 리스트를 합쳐서 하나의 문자열을 만든다.\n",
    "-  join과 split는 상반된 역할을 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 화 여 자 대 학 교\n",
      "['이', '화', '여', '자', '대', '학', '교']\n",
      "이 화 여 자 대 학 교\n",
      "이\t \t화\t \t여\t \t자\t \t대\t \t학\t \t교\n"
     ]
    }
   ],
   "source": [
    "a= \"이 화 여 자 대 학 교\"\n",
    "a1= a.split(\" \")\n",
    "a2= \"\".join(a)     # \"\" 기준으로 join 하시오\n",
    "a3= \"\\t\".join(a)   # tab 기준으로 join 하시오\n",
    "\n",
    "print(a)\n",
    "print(a1)\n",
    "print(a2)\n",
    "print(a3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 본격적으로 살펴보자 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "input_file_name = r\"movie.csv\"\n",
    "output_file_name = r\"movie2.tsv\"\n",
    "\n",
    "with open(input_file_name, \"r\", encoding=\"utf-8-sig\", newline=\"\") as csv_file, \\\n",
    "        open(output_file_name, \"w\", encoding=\"utf-8\") as tsv_file:\n",
    "    \n",
    "    csv_reader = csv.reader(csv_file)       # csv 모듈 내에 reader 함수 호출\n",
    "    \n",
    "    for i, line in enumerate(csv_reader):   # enumerate 함수로 i 생성해서 사용 i는 0,1,2, 이렇게 생성이 됨 \n",
    "        #print(i,line)\n",
    "        if i == 0:                          # 0 ['doc', 'review', 'label'] 이건 csv 파일의 header 니까 건너뛰기위해 if문사용\n",
    "            continue                        # i=0이면 넘겨라,, 라는 의미 \n",
    "        \n",
    "        doc_no, review, label = line        # 각각의 값을 직접 할당해 줌 \n",
    "        body = review.replace(\"\\n\\n\", \" \")  # reveiew 전처리 하기 위해 body에서 연속된 줄바꿈 문자를 하나의 공백 문자로 변환\n",
    "        body = review.replace(\"\\n\", \" \")    # 혹시 연속되지 않은 줄바꿈 문자가 있으면 하나의 공백 문자로 변환\n",
    "        outputs =  [doc_no, body, label]    # review 를 전처리해준 body 로 바꿔서 새로운 output 생성\n",
    "        \n",
    "        #print(outputs)\n",
    "        #print(\"\\t\".join(outputs), file = tsv_file)   # outputs의 doc_no,body,label 값을 tab 기준으로 header가 없는 상태로 저장됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 외부파일 불러와 형태소 분석 후 TSV 파일 저장 \n",
    "\n",
    "아래 코드를 통해 파일을 저장하면, 파일의 줄(line)에 문서 기준이 아닌 형태소 기준으로 정보가 저장된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 먼저 튜플의 자동 언패킹 하는 걸 복습해보자 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "마지막 noun\n",
      "황정민 noun\n",
      "날린 verb\n",
      "스토리 noun\n",
      "=================\n",
      "마지막 noun\n",
      "황정민 noun\n",
      "날린 verb\n",
      "스토리 noun\n"
     ]
    }
   ],
   "source": [
    "pos_anal = [('마지막', 'noun'), ('황정민', 'noun'), ('날린', 'verb'), ('스토리', 'noun')]\n",
    "\n",
    "for i in pos_anal:\n",
    "    print(i[0], i[1]) \n",
    "\n",
    "print(\"=================\")\n",
    "for word, pos in pos_anal:\n",
    "    print(word, pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zip() 함수 \n",
    "\n",
    "- 여러 개의 리스트 자료를 불러와 새로운 사전형 자료를 생성하기 위해 `zip()` 함수를 사용한다.\n",
    "- zip() 함수는 여러 개의 리스트의 항목을 for  …  in 구문으로 한꺼번에 접근할 때에 활용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5}\n"
     ]
    }
   ],
   "source": [
    "# zip()함수를 이용하여 append 와 다르게 사전형자료를 만드는 방법 \n",
    "\n",
    "l1 = ['a', 'b', 'c', 'd', 'e']\n",
    "l2 = [1, 2, 3, 4, 5]\n",
    "\n",
    "new = {}\n",
    "for n1, n2 in zip(l1, l2):\n",
    "    #print(n1,n2)\n",
    "    new[n1] = n2\n",
    " \n",
    "print(new) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 1 A\n",
      "b 2 B\n",
      "c 3 C\n",
      "d 4 D\n",
      "e 5 E\n"
     ]
    }
   ],
   "source": [
    "# zip() 함수를 이용하여 3개의 리스트를 합칠 수도 있음 \n",
    "\n",
    "l1 = [1, 2, 3, 4, 5]\n",
    "l2 = ['a', 'b', 'c', 'd', 'e']\n",
    "l3 = ['A', 'B', 'C', 'D', 'E']\n",
    "\n",
    "for n1, n2, n3 in zip(l1, l2, l3):\n",
    "    print(n2, n1, n3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeeyeon\\Anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "# 본격적으로 분석해보자\n",
    "\n",
    "from konlpy.tag import Twitter    # konlpy.tag 에서 Twitter 함수를 불러오겠다 \n",
    "\n",
    "def split_sentences(text): # split 해주는 함수 \n",
    "    text = text.strip().replace(\". \", \".\\n\").replace(\"? \", \"?\\n\").replace(\"! \", \"!\\n\")\n",
    "    sentences = text.splitlines()\n",
    "    return sentences\n",
    "\n",
    "def get_pos(analyzer, text):      # POS(Part-Of-Speech) 분석해주는함수\n",
    "    morph_anals = []\n",
    "    sentences = split_sentences(text)                       # 위에서 정의한 def split_sentences 호출 \n",
    "    for sentence in sentences:\n",
    "        morph_anal = analyzer.pos(sentence)                 # morph_anal의 출력 값 = [(word, pos)] \n",
    "        morph_anals.append(morph_anal)\n",
    "    return morph_anals\n",
    "\n",
    "\n",
    "# main \n",
    "def main(): # 호출할 때 main() 하면 들어있는 거 전부 다 호출 \n",
    "    tsv_file_name = r\"movie2.tsv\"          \n",
    "    pos_tsv_file_name = r\"pos1_movie.tsv\" # output\n",
    "    twitter = Twitter()\n",
    "\n",
    "    with open(tsv_file_name, \"r\", encoding=\"utf-8\") as tsv_file, \\\n",
    "            open(pos_tsv_file_name, \"w\", encoding=\"utf-8\") as pos_tsv_file:\n",
    "        \n",
    "        for line in tsv_file:\n",
    "            line = line.strip().split(\"\\t\")           # tsv 파일이니까 나누는 걸 tab 기준으로 \n",
    "            doc_no, review, label = line              # 나눠서 지정할거야 review에 대해서만관심있어서 \n",
    "            pos_anal = get_pos(twitter, review)              # morph_anal의 출력 값을 pos_anal에 할당\n",
    "            #print(pos_anal)                         # pos_anal의 출력 값 = [(word, pos)] , 즉 (형태소,품사)형태로 묶여있는 상태 \n",
    "            \n",
    "            for word_pos in pos_anal:                        \n",
    "                for word, pos in word_pos:                       # tuple 의 자동 unpacking \n",
    "                    outputs = [doc_no, word, pos, label]         # 중간 review 값이 word, pos 로 바뀜! !\n",
    "                    print(\"\\t\".join(outputs), file=pos_tsv_file) # tab 기준으로 새로운 파일로 저장함  \n",
    "                    \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON  파일 형식\n",
    "\n",
    "최근 현업에서 많이 쓰이는 데이터 저장 형식은 JSON(JavaScript Object Notation) 방식이다.\n",
    "json 파일 저장을 위해 `import ujson` 모듈을 호출한다.\n",
    "\n",
    "> `왜 쓰는가 ?`\n",
    "- 기존에 위에서처럼 list에 list 사용하면 호출하면 시간이 오래걸리기 때문에 앞으로는 JSON 파일 형태로 파일을 저장해 놓을 것이다. \n",
    "- key 값과 value 로 나눠 저장하기 때문에 다루기가 편리하다.\n",
    " \n",
    "> `JSON 형식 파일 생성`\n",
    "- 데이터를 저장하기 위한 사전형 타입으로 객체를 생성한다. \n",
    "- movie_data 개체는 JSON 파일로 변환되기 위한 객체이고, series 객체는 series key 에 들어가기 위한 앨범 목록을 저장하는 객체이다.\n",
    "- (이 값들은 “key”: value 형식으로 저장) -> 그 결과 series 는 사전 속 사전 타입의 자료형이 생성된다.\n",
    "\n",
    "> `절차`\n",
    "- STEP 1. “name”이라는 키로 “어벤져스”라는 값을 입력하고 “hero”라는 키로 어벤져스 멤버 이름이 저장된 배열을 입력\n",
    "- STEP 2. 그리고 10번 줄부터 앨범 목록을 순서대로 하나씩 추가하고, 이를 group_data에 “albums”라는 키에다가 추가\n",
    "- STEP 3. 모든 데이터의 추가가 끝났으면 이를 출력한다.\n",
    "\n",
    "> `json.dumps 함수 옵션`\n",
    "- json.dumps 함수는 Python Object 를 문자열로 변환해 주는 역할을 함 \n",
    "- \n",
    "- ensure_ascii=False 옵션은 유니코드가 아스키 문자로 변환되지 않게 한다. \n",
    "- indent=\"\\t\" 옵션을 줘서 탭 문자로 들여쓰기를 함으로써 가독성을 높인다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\t\"name\": \"어벤져스\",\n",
      "\t\"hero\": [\n",
      "\t\t\"아이언맨\",\n",
      "\t\t\"토르\",\n",
      "\t\t\"헐크\",\n",
      "\t\t\"캡틴\",\n",
      "\t\t\"블랙위도우\",\n",
      "\t\t\"호크아이\"\n",
      "\t],\n",
      "\t\"series\": {\n",
      "\t\t\"어벤져스1\": \"첫편\",\n",
      "\t\t\"어벤져스2\": \"에이지 오브 울트론\",\n",
      "\t\t\"어벤져스3\": \"인피니티 워\",\n",
      "\t\t\"어벤져스4\": \"어벤져스: 엔드게임\"\n",
      "\t}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import ujson\n",
    "import json\n",
    "\n",
    "# Ready for data\n",
    "movie_data = {}\n",
    "series = {}\n",
    " \n",
    "# 값 입력 \n",
    "movie_data[\"name\"] = \"어벤져스\"\n",
    "movie_data[\"hero\"] = [\"아이언맨\", \"토르\", \"헐크\", \"캡틴\", \"블랙위도우\", \"호크아이\"]\n",
    " \n",
    "series[\"어벤져스1\"] = \"첫편\"\n",
    "series[\"어벤져스2\"] = \"에이지 오브 울트론\"\n",
    "series[\"어벤져스3\"] = \"인피니티 워\"\n",
    "series[\"어벤져스4\"] = \"어벤져스: 엔드게임\"\n",
    "movie_data[\"series\"] = series  # 위에서 설정해준 series 라는 리스트를 다시 movie 라는 리스트에 넣어줌 \n",
    " \n",
    "\n",
    "# print(json.dumps(movie_data) )\n",
    "#print(json.dumps(movie_data, ensure_ascii=False) )\n",
    "print(json.dumps(movie_data, ensure_ascii=False, indent=\"\\t\") )\n",
    "    # ensure_ascii : unicode가 ascii 코드로 변환되지 않도록 하기 위한 옵션\n",
    "    #indent=\"\\t\"   : 탭 문자로 들여쓰기를 함으로써 가독성을 높인다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON 형식 파일 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Avengers.json', 'w', encoding=\"utf-8\") as make_file: \\\n",
    "    json.dump(movie_data, make_file, ensure_ascii=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON 파일 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': '어벤져스', 'hero': ['아이언맨', '토르', '헐크', '캡틴', '블랙위도우', '호크아이'], 'series': {'어벤져스1': '첫편', '어벤져스2': '에이지 오브 울트론', '어벤져스3': '인피니티 워', '어벤져스4': '어벤져스: 엔드게임'}}\n"
     ]
    }
   ],
   "source": [
    "with open('Avengers.json', 'r', encoding=\"utf-8\") as read_file:\n",
    "    for line in read_file:\n",
    "        print(ujson.loads(line))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 외부파일 불러와 형태소 분석 후 JSON 파일 형태로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeeyeon\\Anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "\"\"\"형태소 분석 후 JSON 형태로 파일 저장\"\"\"\n",
    "\n",
    "import csv\n",
    "import re\n",
    "import ujson\n",
    "from konlpy.tag import Twitter\n",
    "\n",
    "\"\"\"문서를 문장 단위로 토큰화\"\"\"\n",
    "def split_sentences(text):\n",
    "    text = text.strip().replace(\". \", \".\\n\").replace(\"? \", \"?\\n\").replace(\"! \", \"!\\n\")\n",
    "    sentences = text.splitlines()\n",
    "    return sentences\n",
    "\n",
    "\"\"\" 형태소 분석 실행 \"\"\"\n",
    "def get_pos(analyzer, text): \n",
    "    morph_anals = []\n",
    "    sentences = split_sentences(text)       # 위에서 정의한 def split_sentences 호출 \n",
    "    for sentence in sentences:\n",
    "        morph_anal = analyzer.pos(sentence) # morph_anal의 출력 값 = [(word, pos)] \n",
    "        morph_anals.append(morph_anal)\n",
    "    return morph_anals\n",
    "\n",
    "\"\"\" def(1): CSV 파일 불러들이기 \"\"\"\n",
    "# 결과적으로 영화 리뷰 한사람이 쓴 게 한 줄에 들어감 아래예시보면 \n",
    "# 형태 : {'doc_no': '1', 'review': '마지막에 황정민vs유아인 격투....였다. 진짜... ,'label': 'pos'}\n",
    "def read_text(input_file_name):\n",
    "    key_names = ['doc_no', 'review', 'label']\n",
    "    data = []\n",
    "    with open(input_file_name, \"r\", encoding=\"utf-8-sig\",newline=\"\") as input_file:\n",
    "        reader = csv.reader(input_file)\n",
    "        for row_num, row in enumerate(reader): \n",
    "            #print(row)        \n",
    "            if row_num == 0:  # 첫번째줄 skip\n",
    "                continue\n",
    "\n",
    "            reviews = {}\n",
    "            for key_name, val in zip(key_names, row):\n",
    "                #print(key_name, val)   # doc_no:1  review:리뷰내용 label:라벨값 이런식으로 들어감 \n",
    "                reviews[key_name] = val\n",
    "            data.append(reviews)        # list 의 사전형자료 들어가도록 \n",
    "    return data                         # def read_text 파일의 최종출력값은 사전형 자료를 내포한 리스트\n",
    "\n",
    "\"\"\"def(2): 주어진 상품평 파일에서 \"review\"에 대해서만 형태소 분석\"\"\" \n",
    "# data 넣어주면 data 값에 review_pos 라는 변수 추가해서 데이터셋 출력해줌 \n",
    "# 형태 : [{'doc_no': '1', 'review': '마지막에 황정민vs유아인 격투....였다. 진짜... ,', 'label': 'pos', 'review_pos': [[('마지막', 'Noun'), \n",
    "def pos_review(data): \n",
    "    data_pos = []\n",
    "    twitter = Twitter()\n",
    "    for reviews in data:      # data doc_no,review,label가 있는데 reviews 는 일단 이 전체를 의미함 곧 data = reviews의 한줄한줄 \n",
    "        body = reviews[\"review\"]                   # 위의 3게 변수 중 review 값만 불러와라 \n",
    "        #print(body)\n",
    "        review_pos = get_pos(twitter, body)        # pos 이용한 형태소분석 \n",
    "        reviews[\"review_pos\"] = review_pos         # 새로운 review_pos 변수명을 가진 변수 추가 \n",
    "        data_pos.append(reviews)                   # 즉 여기서 data_pos 데이터에는 doc_no, review, label,review_pos 변수가 포함됨\n",
    "        #print(data_pos)                          \n",
    "    return data_pos                                # 최종리턴값        \n",
    "\n",
    "\"\"\"def(3): json 형태로 파일 저장\"\"\"\n",
    "# def(2)에서 형태소분석까지 한 최종 return 값인 data_pos를 JSON 파일로 내보내줌\n",
    "def write_pos_review(output_file_name, data_pos):\n",
    "    with open(output_file_name, \"w\", encoding=\"utf-8\") as output_file:   # output file 써서 내보낼거얌\n",
    "        for review_pos in data_pos:\n",
    "            review_str = ujson.dumps(review_pos, ensure_ascii=False)\n",
    "            print(review_str, file=output_file)\n",
    "\n",
    "\"\"\" 앞에서 정의해 준 파일 넣어주면 형태소분석 포함된 변수를 JSON 파일로 저장해줌 \"\"\"\n",
    "def main():   # input, output 설정한다음에 하나씩 부르기 \n",
    "    \n",
    "    input_file_name = r\"movie.csv\"\n",
    "    output_file_name = r\"pos_movie.txt\"\n",
    "    \n",
    "    data = read_text(input_file_name)                                     # def (1) read_text 함수 호출        \n",
    "    data_pos = pos_review(data)                                           # def (2) pos_review 함수 호출\n",
    "    write_pos_review(output_file_name, data_pos)                          # def (3) write_pos_review  함수 호출\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 형태소 분석 결과!!!\n",
    "위 분석 결과, 문서 번호 140의 상품평에 대한 형태소 분석 결과 예시이다.\n",
    "\n",
    ">상품평에 대한 형태소 분석 결과인 review_pos 리스트 안에 리스트가 한 겹 더 있는 것은 입력 텍스트가 여러 개의 문장으로 분절될 경우 문장별 형태소 분석 결과를 분리하여 저장하기 위해서이다. \n",
    "따라서, review_pos는 입력 텍스트의 문장별 형태소 분석 결과를 원소로 하는 리스트이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_pos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-5d8ab73e16c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_pos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpos_review\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_pos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_pos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m139\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_pos' is not defined"
     ]
    }
   ],
   "source": [
    "data_pos = pos_review(data)\n",
    "print(data_pos[139])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_140= {\"doc_no\":\"140\",\"review\":\"최고다. 후속작도 너무 기대된다.. \",\"label\":\"pos\",\n",
    "          \"review_pos\":[[[\"최고다\",\"Noun\"],[\".\",\"Punctuation\"]],\n",
    "                       [[\"후속작\",\"Noun\"],[\"도\",\"Josa\"],[\"너무\",\"Adverb\"],[\"기대\",\"Noun\"],[\"된다\",\"Verb\"],[\"..\",\"Punctuation\"]]]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 형태소 빈도 계수\n",
    "- 형태소 분석까지 완료된 텍스트 데이터에 대해서 `Counter` 클래스를 이용하여 문서에서 등장한 각 형태소별 빈도를 계산할 수 있다.\n",
    "- R 에서 table()과 같은 함수, 빈도 계산해줌 \n",
    "\n",
    "### Counter 클래스 사용\n",
    "- 텍스트분석에서는 빈도 계수에 특화된 자료 구조로 collections 모듈에서 제공하는 `Counter 클래스`가 자주 이용된다.\n",
    "- collections 모듈은 이미 아나콘다에 포함되어 있으며 이 모듈에서 Counter 클래스를 불러온다. \n",
    "- Counter 는 collections 아래에 정의된 하위 클래스, 리스트나 튜플에서 각 데이터가 등장한 횟수를 계산한 뒤 사전 형식으로 돌려준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## using Counter -> 각 문자별 빈도표 출력 \n",
    "from collections import Counter\n",
    "colors = ['red', 'blue', 'red', 'green', 'blue', 'blue'] \n",
    "\n",
    "color_no = Counter(colors)\n",
    "print(color_no )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counter 클래스의 `most_common()` 메소드\n",
    "- 등장한 횟수를 내림차순으로 정리\n",
    "- most_common() 메소드를 사용하면 리스트로 반환해 주며, 따라서 for문을 이용해 리스트 내 원소를 차례로 꺼낼 수 있다. \n",
    "- 리스트(list)를 구성하는 요소들은 튜플(tuple)이며 각 튜플의 첫 번째 요소는 numbers의 숫자, 두 번째 요소는 각 숫자가 등장한 횟수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [100, 200, 300, 300, 400, 400, 400, 500, 500] \n",
    "num = Counter(numbers) \n",
    "\n",
    "num.most_common()   # Counter 의 장점!  (값, 그값에 대응되는 빈도)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각각 하나씩 원소를 꺼낼 수 있음 \n",
    "for number, freq in num.most_common():\n",
    "    print(number, freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key는 \"동물\", 나머지는 value (한줄임)\n",
    "animals = {\"동물\" : [[\"cat\",\"야옹\"],[\"cat\",\"야옹\"],[\"cat\",\"야옹\"],[\"dog\",\"멍멍\"],[\"dog\",\"멍멍\"],[\"pig\",\"꿀꿀\"]]}\n",
    "\n",
    "animal_freq = Counter(animals)\n",
    "animal_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animals = {\"동물\" : [[\"cat\",\"야옹\"],[\"cat\",\"야옹\"],[\"cat\",\"야옹\"],[\"dog\",\"멍멍\"],[\"dog\",\"멍멍\"],[\"pig\",\"꿀꿀\"]]}\n",
    "\n",
    "animal_freq = Counter()               # 빈리스트 만들어줌 \n",
    "for animal in animals[\"동물\"]: \n",
    "    #print(animal)                    # animal은 [,] 이런형태로 6개 리스트가 들어있음\n",
    "    (type, cry) = animal\n",
    "    #print(type,cry)\n",
    "    animal_freq.update([(type, cry)]) # update 즉 Counter().update([하나,하나])\n",
    "\n",
    "animal_freq           # Counter 사용해서 dictionary 형태로 돌려줌 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 형태소 분석 후 주요 품사의 형태소만 선택한 뒤 빈도 추출 \n",
    "- 아래는 상품 리뷰에 대해 형태소 분석 후 관심있는 주요 품사의 형태소만 선택하고 그 빈도를 알아봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import ujson\n",
    "\n",
    "POS_KEY = \"review_pos\"\n",
    "FEATURE_POS = [\"Noun\", \"Verb\", \"Adverb\", \"Adjective\", \"Exclamation\", \"Determiner\"]\n",
    "\n",
    "\"\"\"주어진 품사의 주요 품사 여부(FEATURE_POS)를 판단하여 돌려준다.\"\"\"\n",
    "# pos 값이 FEATURE_POS 안에 있으면 TRUE return\n",
    "def is_feature_pos(pos):  \n",
    "    if pos in FEATURE_POS:  \n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\"\"\"주어진 형태소 분석 결과 리스트에서 필요한 형태소만 골라서 돌려준다.\"\"\"\n",
    "# 위에서 정의한 is_feature_pos를 이용해 관심있는 pos 에 대해서만 해당 값의 word 와 함께 return \n",
    "def feature_pos(pos_anals):  \n",
    "    pos_anals = [(word, pos) for word, pos in pos_anals if is_feature_pos(pos)]\n",
    "    return pos_anals\n",
    "\n",
    "\"\"\"한국어 상품평의 주요 품사 형태소 빈도를 계수한다.\"\"\"\n",
    "def main():      \n",
    "    pos_freq = Counter()\n",
    "    input_file_name = r\"pos_movie.txt\"\n",
    "\n",
    "    with open(input_file_name, \"r\", encoding=\"utf-8\") as input_file:\n",
    "        for line in input_file:\n",
    "            doc = ujson.loads(line)  # line 정보를 이용해서 ujson파일 load 해서 doc에 저장하기 \n",
    "            #print(doc)               # doc_no review label review_pos 변수로 이루어진 딕셔너리가 저장됨 \n",
    "            for sent_ma in doc[POS_KEY]:         \n",
    "                #print(sent_ma)                  # 형태 : ['마지막','None']\n",
    "                sent_ma = feature_pos(sent_ma)   # 내가 관심있는 FEATURE_POS에 대해서만 뽑아줌\n",
    "                pos_freq.update(sent_ma)\n",
    "                # print(pos_freq)                # 형태 : Counter({('사이다', 'Noun'): 2, ('마지막', 'Noun'): 1\n",
    "            \n",
    "    for (word, pos), freq in pos_freq.most_common(20):   # pos_freq.most_common(20) :상위 빈도 20개만 출력 \n",
    "        print(\"{}\\t{}\\t{}\".format(word, pos, freq))\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위에서 정의된 is_feature_pos, feature_pos 함수를 합쳐보자 \n",
    "FEATURE_POS = [\"Noun\", \"Verb\", \"Adverb\", \"Adjective\", \"Exclamation\", \"Determiner\"]\n",
    "POS_KEY = \"review_pos\"\n",
    "\n",
    "\"\"\"주어진 형태소 분석 결과 리스트에서 필요한 형태소만 골라서 돌려준다.\"\"\"\n",
    "pos_anals = []\n",
    "def feature_pos(sent_ma):   \n",
    "    for word, pos in sent_ma:\n",
    "        if pos not in FEATURE_POS:\n",
    "            continue\n",
    "        i = (word, pos)\n",
    "        pos_anals.append(i)\n",
    "    return pos_anals\n",
    "\n",
    "\n",
    "# 이해쉽게하기위해 \n",
    "s= {\"doc_no\":\"140\",\"review\":\"최고다. 후속작도 너무 기대된다.. \",\"label\":\"pos\",  \n",
    "    \"review_pos\":[[[\"최고다\",\"Noun\"],[\".\",\"Punctuation\"]],[[\"후속작\",\"Noun\"],[\"도\",\"Josa\"],[\"너무\",\"Adverb\"],[\"기대\",\"Noun\"],[\"된다\",\"Verb\"],[\"..\",\"Punctuation\"]]]}\n",
    "\n",
    "pos_freq = Counter()\n",
    "for sent_ma in s[POS_KEY]: \n",
    "    sent_ma = feature_pos(sent_ma)   # 원하는 품사만 출력할거야 \n",
    "    pos_freq.update(sent_ma)         # Counter 함수 사용해서 갯수 세고 dic 형태로 저장할거야 \n",
    "pos_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 형태소 빈도 계수 후 파일 저장\n",
    "- 일단 itemgetter(), sort() 에 대해서 공부한 후 본격적인 분석에 들어가자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sort()메소드, itemgetter() 함수\n",
    "- 조건을 여러개 쓰기 위해서는 sorted 함수 쓰면 가나다순, sorted(값,reverse =True ) 해주면 가나다순의 역순으로 해줌\n",
    "- 앞서 배운 most_common() 메소드는 간단한 방법으로 빈도 계수 결과를 빈도 역순으로 얻을 수 있어서 매우 편리하다.\n",
    "- 빈도 계수 결과를 \"여러 개의 기준, 즉 다중 키\"로 정렬해야 할 때 `operator 모듈의 itemgetter() 함수`와  `sorted() 함수`를 함께 사용하면 편리하다. \n",
    "- itemgetter() 함수에는 인덱스를 복수로 지정할 수 있기 때문에 다중 키 사용이 가능하다. 이때, 정렬의 차순이 다를 경우에는 sorted() 함수를 연이어 사용해야 한다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['빨강', '파랑', '노랑', '하양', '검정']\n",
    "print(colors)\n",
    "\n",
    "print('------------Sort-------------------------')\n",
    "colors_s= sorted(colors) # 오름차순\n",
    "print(colors_s)\n",
    "print('------------Reverse sort-----------------')\n",
    "colors_t= sorted(colors, reverse=True) # 내림차순\n",
    "print(colors_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두 값이 튜플로 묶여있는 경우는 첫 번째 원소 기준으로 정렬됨\n",
    "colors = [('빨강', 2), ('파랑', 4), ('노랑', 10), ('하양', 7), ('검정', 1)]\n",
    "print(colors)\n",
    "\n",
    "print('------------Sort by 1st-----------------')\n",
    "colors_s= sorted(colors)              # 뒤에 있는 숫자가 아닌 앞에 있는 한글로 정렬해줌 \n",
    "print(colors_s)\n",
    "print('------------Reverse sort by 1st-----------------')\n",
    "colors_t= sorted(colors, reverse=True)\n",
    "print(colors_t)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "colors = [('빨강', 2), ('파랑', 4), ('노랑', 10), ('하양', 7), ('검정', 1)]\n",
    "print(colors)\n",
    "\n",
    "print('------------Sort by 2nd-----------------')\n",
    "colors_s= sorted(colors, key=itemgetter(1))     # 0: 첫번째원소'빨강'에 해당, 1:두번째원소 숫자기준으로, key는 순서를 나타내줌 \n",
    "print(colors_s)\n",
    "print('------------Reverse sort by 1nd-----------------')\n",
    "colors_t= sorted(colors, key=itemgetter(1), reverse=True)\n",
    "print(colors_t) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 이제 본격적으로 분석해보자\n",
    "- 1.상품 리뷰에 대해 형태소 분석 (pos)\n",
    "- 2.주요 품사의 형태소만 선택 (is_feature_pos)\n",
    "- 3.빈도 역순으로 정렬 (most_common)\n",
    "- 4.형태소, 단어, 빈도수 파일 저장 ( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import ujson\n",
    "from operator import itemgetter # 추가 \n",
    "\n",
    "POS_KEY = \"review_pos\"\n",
    "FEATURE_POS = [\"Noun\", \"Verb\", \"Adverb\", \"Adjective\", \"Exclamation\", \"Determiner\"]\n",
    "\n",
    "\"\"\"주어진 품사의 주요 품사 여부(FEATURE_POS)를 판단하여 돌려준다.\"\"\"\n",
    "# pos 값이 FEATURE_POS 안에 있으면 TRUE return\n",
    "def is_feature_pos(pos):  \n",
    "    if pos in FEATURE_POS:  \n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\"\"\"주어진 형태소 분석 결과 리스트에서 필요한 형태소만 골라서 돌려준다.\"\"\"\n",
    "# 위에서 정의한 is_feature_pos를 이용해 관심있는 pos 에 대해서만 해당 값의 word 와 함께 return \n",
    "def feature_pos(pos_anals):  \n",
    "    pos_anals = [(word, pos) for word, pos in pos_anals if is_feature_pos(pos)]\n",
    "    return pos_anals\n",
    "\n",
    "\"\"\"한국어 상품평의 주요 품사 형태소 빈도를 계수한다.\"\"\"\n",
    "def main():      \n",
    "    pos_freq = Counter()\n",
    "    input_file_name = r\"pos_movie.txt\"\n",
    "    output_file_name = r\"counts_pos_movie.txt\"\n",
    "\n",
    "    with open(input_file_name, \"r\", encoding=\"utf-8\") as input_file:\n",
    "        for line in input_file:\n",
    "            doc = ujson.loads(line)  # line 정보를 이용해서 ujson파일 load 해서 doc에 저장하기 \n",
    "            #print(doc)               # doc_no review label review_pos 변수로 이루어진 딕셔너리가 저장됨 \n",
    "            for sent_ma in doc[POS_KEY]:         \n",
    "                #print(sent_ma)                  # 형태 : ['마지막','None']\n",
    "                sent_ma = feature_pos(sent_ma)   # 내가 관심있는 FEATURE_POS에 대해서만 뽑아줌\n",
    "                pos_freq.update(sent_ma)\n",
    "                # print(pos_freq)                # 형태 : Counter({('사이다', 'Noun'): 2, ('마지막', 'Noun'): 1\n",
    "                \n",
    "    # ----------------------------------------------- 여기까지는 위와 같아             \n",
    "    #print(pos_freq.items())  # 곧 그냥 pos_freq를 의미함 items = key, value \n",
    "    pos_counts = [(word, pos, freq) for (word, pos), freq in pos_freq.items()]\n",
    "    #print(pos_counts)  # 앞에서 형태가 ('사이다', 'Noun'): 2 이렇게 되어있었기 때문에 ('사이다','Noun', 2 ) 이런 형태로 풀어줌 \n",
    "    sorted_pos_counts = sorted(pos_counts, key=itemgetter(2), reverse=True)     # 빈도 역순으로 정렬    \n",
    "                          #sorted() 함수와 itemgetter() 함수를 사용해 (빈도 역순 + 어휘 정순 + 품사 정순)의 다중 키 정렬을 수행!!\n",
    "    #print(sorted_pos_counts)  # 형태 : [('영화', 'Noun', 927), ('연기', 'Noun', 247), ...\n",
    "    \n",
    "    with open(output_file_name, \"w\", encoding=\"utf-8\") as output_file:\n",
    "            for word, pos, freq in sorted_pos_counts:                           # 모든 형태소, 품사, 빈도 저장\n",
    "                print(\"{}\\t{}\\t{}\".format(word, pos, freq), file = output_file)     \n",
    "        \n",
    "# 실행\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법1\n",
    "pos_counts = [(word, pos, freq) for (word, pos), freq in pos_freq.items()]\n",
    "\n",
    "# 방법2\n",
    "pos_counts = []\n",
    "for (word, pos), freq in pos_freq.items():\n",
    "    i = (word, pos, freq) \n",
    "    pos_counts.append(i)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 특징 변수 시각화: 워드클라우드\n",
    "\n",
    "문서에서 자주 등장하는 어휘는 그 문서의 의미 또는 속성을 대표하는 지표가 될 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 워드 클라우드\n",
    "가장 직관적으로 어휘 빈도를 시작화할 수 있는 방법으로 `워드 클라우드`가 있다. \n",
    ">파이썬을 이용하여 워드 클라우드를 그릴 수 있는 라이브러리가 여러 가지 존재하지만 `from wordcloud import WordCloud`를 이용해 워드클라우드를 그린다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import ujson\n",
    "from operator import itemgetter # 추가 \n",
    "\n",
    "POS_KEY = \"review_pos\"\n",
    "FEATURE_POS = [\"Noun\", \"Verb\", \"Adverb\", \"Adjective\", \"Exclamation\", \"Determiner\"]\n",
    "\n",
    "\"\"\"주어진 품사의 주요 품사 여부(FEATURE_POS)를 판단하여 돌려준다.\"\"\"\n",
    "# pos 값이 FEATURE_POS 안에 있으면 TRUE return\n",
    "def is_feature_pos(pos):  \n",
    "    if pos in FEATURE_POS:  \n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\"\"\"주어진 형태소 분석 결과 리스트에서 필요한 형태소만 골라서 돌려준다.\"\"\"\n",
    "# 위에서 정의한 is_feature_pos를 이용해 관심있는 pos 에 대해서만 해당 값의 word 와 함께 return \n",
    "def feature_pos(pos_anals):  \n",
    "    pos_anals = [(word, pos) for word, pos in pos_anals if is_feature_pos(pos)]\n",
    "    return pos_anals\n",
    "\n",
    "\"\"\"한국어 상품평의 주요 품사 형태소 빈도를 계수한다.\"\"\"\n",
    "def main():      \n",
    "    pos_freq = Counter()\n",
    "    input_file_name = r\"pos_movie.txt\"\n",
    "    output_file_name = r\"counts_pos_movie50.txt\"\n",
    "\n",
    "    with open(input_file_name, \"r\", encoding=\"utf-8\") as input_file:\n",
    "        for line in input_file:\n",
    "            doc = ujson.loads(line)  # line 정보를 이용해서 ujson파일 load 해서 doc에 저장하기 \n",
    "            #print(doc)               # doc_no review label review_pos 변수로 이루어진 딕셔너리가 저장됨 \n",
    "            for sent_ma in doc[POS_KEY]:         \n",
    "                #print(sent_ma)                  # 형태 : ['마지막','None']\n",
    "                sent_ma = feature_pos(sent_ma)   # 내가 관심있는 FEATURE_POS에 대해서만 뽑아줌\n",
    "                pos_freq.update(sent_ma)\n",
    "                # print(pos_freq)                # 형태 : Counter({('사이다', 'Noun'): 2, ('마지막', 'Noun'): 1\n",
    "                \n",
    "    # ----------------------------------------------- 여기까지는 위와 같아             \n",
    "    #print(pos_freq.items())  # 곧 그냥 pos_freq를 의미함 items = key, value \n",
    "    pos_counts = [(word, pos, freq) for (word, pos), freq in pos_freq.items()]\n",
    "    #print(pos_counts)  # 앞에서 형태가 ('사이다', 'Noun'): 2 이렇게 되어있었기 때문에 ('사이다','Noun', 2 ) 이런 형태로 풀어줌 \n",
    "    sorted_pos_counts = sorted(pos_counts, key=itemgetter(2), reverse=True)     # 빈도 역순으로 정렬    \n",
    "                          #sorted() 함수와 itemgetter() 함수를 사용해 (빈도 역순 + 어휘 정순 + 품사 정순)의 다중 키 정렬을 수행!!\n",
    "    #print(sorted_pos_counts)  # 형태 : [('영화', 'Noun', 927), ('연기', 'Noun', 247), ...\n",
    "    \n",
    "    with open(output_file_name, \"w\", encoding=\"utf-8\") as output_file:\n",
    "            for word, pos, freq in sorted_pos_counts[0:50]:              # 상위 빈도 50까지만 저장 # 모든 형태소, 품사, 빈도 저장\n",
    "                print(\"{}\\t{}\\t{}\".format(word, pos, freq), file = output_file)     \n",
    "        \n",
    "# 실행\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 내 컴퓨터에 저장된 폰트 뭐뭐있는지 보기 위해서 .ttf 확장자명 파일 다 보여줘라 \n",
    "\n",
    "import glob\n",
    "glob.glob(\"C:\\Windows\\Fonts\\*.ttf\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "input_file_name = r\"counts_pos_movie50.txt\"\n",
    "\n",
    "with open(input_file_name, \"r\", encoding=\"utf-8\") as input_file:\n",
    "    keywords = {} \n",
    "    for line in input_file:\n",
    "        line = line.rstrip().split(\"\\t\") # 문장 단위로 분리해주고\n",
    "        #print(line)\n",
    "        word, pos, freq = line           # word, freq 변수 만들어주고 \n",
    "        keywords[word] = int(freq)       # keywords 라는 dic 에는 {word() : frequence}\n",
    "        # print(keywords)                # 이건 왜 형태가 ???????????????????\n",
    "        \n",
    "#wordcloud = WordCloud()\n",
    "font_path = 'c:\\\\windows\\\\fonts\\\\NanumSquareRoundL.ttf'   # 폰트 경로 지정하기 \n",
    "#wordcloud = WordCloud(font_path = font_path,width = 800, height = 800)  #background default : Black\n",
    "wordcloud = WordCloud(font_path = font_path,width = 800, height = 800, background_color=\"white\")  \n",
    "\n",
    "wordcloud = wordcloud.generate_from_frequencies(keywords) # 형태소와 빈도로 지정된 사전형 자료를 넣어주고 \n",
    "\n",
    "# 그래프 그리기 ------------------------\n",
    "fig = plt.figure(figsize=(12,9))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()  # 그래프보여주시오 \n",
    "\n",
    "#fig.savefig('word_cloud_black.png')\n",
    "fig.savefig('word_cloud_white.png') # 파일저장하기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "워드클라우드 다양한 시각화하는 방법은 아래 사이트 참고\n",
    "\n",
    "> https://lovit.github.io/nlp/2018/04/17/word_cloud/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이제까지 TF(빈도기반)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
