{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selenium Location Elements\n",
    "\n",
    "- find_element_by_id\n",
    "- find_element_by_name\n",
    "- find_element_by_xpath\n",
    "- find_element_by_link_text\n",
    "- find_element_by_partial_link_text\n",
    "- find_element_by_tag_name\n",
    "- find_element_by_class_name\n",
    "- find_element_by_css_selector\n",
    "\n",
    "출처 : https://dejavuqa.tistory.com/108"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "## 아까 받은 chromedriver의 위치를 지정해준다.=>에러남\n",
    "#해결 : chromedriver.exe파일을 파이썬 스크립트 폴더안에 넣었음(파이썬 설치경로)\n",
    "##path = \"C:\\Users\\jeeyeon\\Downloads\\chromedriver_win32\\chromedriver.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "청담자이 160,000\n",
      "청담자이 280,000\n",
      "청담자이 160,000\n",
      "래미안개포루체하임 245,000\n",
      "청담래미안로이뷰 230,000\n",
      "청담자이 165,000\n",
      "청담자이 163,000\n",
      "청담자이 160,000\n",
      "청담래미안로이뷰 230,000\n",
      "청담자이 160,000\n",
      "청담자이 170,000\n",
      "청담자이 280,000\n",
      "래미안블레스티지 160,000\n",
      "래미안블레스티지 160,000\n",
      "청담자이 280,000\n",
      "청담자이 280,000\n",
      "청담자이 280,000\n",
      "개포주공4단지 140,000\n",
      "삼익 240,000\n",
      "개포주공1단지 202,000\n",
      "개포주공1단지 167,000\n",
      "대치풍림아이원1차 130,000\n",
      "개포주공1단지 167,000\n",
      "개포주공1단지 189,000\n",
      "개포주공1단지 145,000\n",
      "개포주공4단지 143,000\n",
      "청담래미안로이뷰 175,000\n",
      "개포주공4단지 150,000\n",
      "래미안블레스티지 210,000\n",
      "청구 163,000\n",
      "삼익 300,000\n",
      "청담자이 260,000\n",
      "청담자이 260,000\n",
      "래미안강남힐즈 155,000\n",
      "래미안강남힐즈 145,000\n",
      "청구 163,000\n",
      "개포주공4단지 149,000\n",
      "청담래미안로이뷰 225,000\n",
      "삼익 300,000\n",
      "래미안강남힐즈 145,000\n",
      "청담자이 145,000\n",
      "래미안대치팰리스1단지 280,000\n",
      "청담자이 150,000\n",
      "청담자이 295,000\n",
      "개포주공1단지 143,000\n",
      "래미안강남힐즈 145,000\n",
      "래미안강남힐즈 140,000\n",
      "래미안강남힐즈 155,000\n",
      "청담자이 295,000\n",
      "래미안강남힐즈 167,000\n"
     ]
    }
   ],
   "source": [
    "### 네이버 부동산 매물 예시로 맛보기\n",
    "import requests\n",
    "res = requests.get('http://land.naver.com/article/divisionInfo.nhn?rletTypeCd=A01&tradeTypeCd=A1&hscpTypeCd=A01%3AA03%3AA04&cortarNo=1168000000&articleOrderCode=&cpId=&minPrc=&maxPrc=&minWrrnt=&maxWrrnt=&minLease=&maxLease=&minSpc=&maxSpc=&subDist=&mviDate=&hsehCnt=&rltrId=&mnex=&siteOrderCode=&cmplYn=')\n",
    "\n",
    "soup = BeautifulSoup(res.content, 'html.parser')\n",
    "\n",
    "# a 태그이면서 href 속성 값이 특정한 값을 갖는 경우 탐색\n",
    "link_title = soup.select(\"#depth4Tab0Content > div > table > tbody > tr > td.align_l.name > div > a.sale_title\")\n",
    "link_price = soup.select(\"#depth4Tab0Content > div > table > tbody > tr > td.num.align_r > div > strong\")\n",
    "\n",
    "for num in range(len(link_price)):\n",
    "    print(link_title[num].get_text(), link_price[num].get_text()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   score                                             review\n",
      "0      5  저는 이 제품을 3번째 쓰는중인데너무 좋은거같아요 이 제품을 쓰기전에는 톤업도 안되...\n",
      "1      4  닥터지 딥 모이스트 사용중인데 언니가 사용하고 있는 제품이라 갈아 타 봤어요 우선 ...\n",
      "2      1  배송준비중이라 취소했는데 답변도 늦고 결국 배송은되고 제돈으로 반품해야한다니 그냥 ...\n",
      "3      4  발림성이 좋고 끈적이지 않아서 아주만족해요 백탁현상이 있는듯 했으나 브라이트라 환하...\n",
      "4      5  여름엔 이것 만한 게 없어요. 벌써 몇 년째 사용하고 있는지 모르겠어요. 지복합성 ...\n",
      "5      4  작년에 조카소개로 알게되어 써보고...다 떨어져서 재구매하네요~오~이번엔 작은쌔이즈...\n",
      "6      4  지성이 사용하기에 나쁘지 않은것 같아요 수분부족지성이라 기초를 유분보다 수분 위주로...\n",
      "7      4  친구껄 발라보고 너무 좋아서 구매하게 되었어요! 피부가 좀 까무잡잡한 편이라 자연스...\n",
      "8      5  선크림 유목민입니다. .로드샵여러군데꺼다써보다가실패하고. .쌓아놓코하다가 지인이 선...\n",
      "9      5  검색하다가 우연히 접하고 구매했습니다. 그동안 선크림, 선큐션, 선스틱 등 다양한 ...\n",
      "10     5  처음써봐요! 쓰던거는 백화점 유명브랜드였는데 닥터지가 훨좋네요ㅋ...원래 쓰던건 가...\n",
      "11     5  닥터지브라이트닝업선 오프라인이나 다른 쇼핑몰은할인율이 크지 않은데 검색해보니 H몰에...\n",
      "12     5  화해어플에서 상위에 있길래 그린마일드를 구매했다가 발림성도 좋고 무난하게 잘사용해서...\n",
      "13     1  처음 발랐을땐 톤업과 파데나비비 올렸을때 피부표현이 괜찮아서 만족했는데 그날 저녁 ...\n",
      "14     5  20m에는 있는 은박 마개가 50m에는 원래없는건가요?아직 사용전이긴한데 저는 닥터...\n",
      "15     5  비싼개기름과 모공때문에 작년부터  재구매 사용중입니다비싼 수입브랜드 이것저것 써봤지...\n",
      "16     5  바르고난후 빠르게 흡수되고 매트하고 보송한 느끼미 듭니다.저는 수분 부족형 지성이라...\n",
      "17     5  벌써 3번째 구매입니다 점보가 가성비가 좋은것 같아 구매했는데 예쁜 통에 담아주셨어...\n",
      "18     4  처음에 그린 마일드로 샀다가 얼굴 당김이 너무 심했고 심지어 원래 머리에서 땀이 많...\n",
      "19     5  예전에 사용하고 두번째 구매에요.우선 톤업효과 톡톡히 발휘하고 발림성이 좋고 여러모...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#1\\nfor i in range(len(reviews)):\\n    print(scores[i].get_text(), reviews[i].get_text()) \\n\\n#2  \\ndrz_review = {}\\nfor i in score:\\n    for j in review:\\n        drz_review[i] = j\\nprint(drz_review)\\n\\n#3\\nimport pandas as pd\\ndf = pd.concat([score,review],axis =1)\\nprint(df)\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##1 페이지에 대해서 !!!!! \n",
    "\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.implicitly_wait(10) # 암묵적으로 웹 자원 로드를 위해 10초까지 기다려 준다.\n",
    "#ulr접근\n",
    "driver.get('https://search.shopping.naver.com/detail/detail.nhn?nv_mid=11358374022&cat_id=50000445&frm=&query=&NaPm=ct%3Djw79pzoo%7Cci%3Dd075b68850622fe0fbdf8dd99921e77adce8f3e6%7Ctr%3Dslsl%7Csn%3D95694%7Chk%3D07b0251e19fc2778129b73f7c1e6befb680d165e')\n",
    "\n",
    "driver.find_element_by_xpath('//*[@id=\"_review_paging\"]/a['+str(1)+']').click() #페이지변화\n",
    "html = driver.page_source # page_source에 코드가 저장되어 있음\n",
    "soup = BeautifulSoup(html, 'html.parser') # html 파일 형식을 볼거라고 지정\n",
    "\n",
    "#reviews = soup.select('#_review_list > li.thumb_nail > div.atc_area > div.avg_area > a > span.curr_avg > strong')\n",
    "scores = soup.select('#_review_list > li > div > div > a > span > strong')\n",
    "reviews = soup.select('#_review_list > li > div > div.atc')\n",
    "\n",
    "score = []\n",
    "for i in scores:\n",
    "    score.append(i.text.strip())\n",
    "review = []\n",
    "for j in reviews:\n",
    "    review.append(j.text.strip())\n",
    "\n",
    "score=pd.Series(score)\n",
    "review=pd.Series(review)\n",
    "mymy = pd.DataFrame({'score':score,\n",
    "                     'review':review})\n",
    "print(mymy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'저는 이 제품을 3번째 쓰는중인데너무 좋은거같아요 이 제품을 쓰기전에는 톤업도 안되는 그냥 선크림을 썼었는데 친구가 이거를 쓰길래 괜찮아서 저도 그 뒤로는 계속 이것만 쓰고 있는데요 톤업도되고 끈적거림도 없고 발림성도 좋고 저는 개인적으로 아주 만족합니다'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymy.review[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 페이지수 반복햇서 크롤링해보자 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 처음 1~10페이지만 \n",
    "\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.implicitly_wait(10) # 암묵적으로 웹 자원 로드를 위해 10초까지 기다려 준다.\n",
    "#ulr접근\n",
    "#driver.get('https://search.shopping.naver.com/detail/detail.nhn?nv_mid=11358374022&cat_id=50000445&frm=&query=&NaPm=ct%3Djw79pzoo%7Cci%3Dd075b68850622fe0fbdf8dd99921e77adce8f3e6%7Ctr%3Dslsl%7Csn%3D95694%7Chk%3D07b0251e19fc2778129b73f7c1e6befb680d165e')\n",
    "driver.get('https://search.shopping.naver.com/detail/detail.nhn?nv_mid=2600099204&cat_id=50000445&frm=NVSHATC&query=%ED%97%A4%EB%9D%BC+%EC%84%A0+%EB%A9%94%EC%9D%B4%ED%8A%B8+%EB%8D%B0%EC%9D%BC%EB%A6%AC+70ml%28SPF35%29+%EC%B5%9C%EC%A0%8014%2C230%EC%9B%90&NaPm=ct%3Djwfyecuo%7Cci%3Dd6c1a707708c662f997b62a24ef0ef94ed6c599d%7Ctr%3Dslsl%7Csn%3D95694%7Chk%3D75d3373d42be546c61e6ffd81f7de1fdc0585505')\n",
    "\n",
    "scorebox=[]\n",
    "reviewbox=[]\n",
    "for i in range(1,11) : # 1~ 10까지 \n",
    "    driver.find_element_by_xpath('//*[@id=\"_review_paging\"]/a['+str(i)+']').click()\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    scores = soup.select('#_review_list > li > div > div > a > span > strong')\n",
    "    reviews = soup.select('#_review_list > li > div > div.atc')\n",
    "    \n",
    "    for idx,s in enumerate(scores) :\n",
    "        sc = s.text.strip()\n",
    "        scorebox.append(sc)\n",
    "    for r in reviews :\n",
    "        rv = r.text.strip()\n",
    "        reviewbox.append(rv)\n",
    "\n",
    "#df = pd.DataFrame({'review' : reviewbox, 'score' : scorebox})\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_css_selector('a.next').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "    for i in range(3,14) :\n",
    "        #driver.find_element_by_css_selector('#_review_paging > a:nth-child('+str(i)+')').click()\n",
    "        #driver.find_element_by_xpath('//*[@id=\"_review_paging\"]/a['+str(i)+']').click()\n",
    "        #driver.find_element_by_xpath('//*[@id=\"_review_paging\"]/a['+str(i)+']').click()\n",
    "\n",
    "        try:\n",
    "            if i % 13 == 0:\n",
    "                driver.find_element_by_css_selector('a.next').click()\n",
    "            else :\n",
    "                driver.find_element_by_xpath('//*[@id=\"_review_paging\"]/a['+str(i)+']').click()\n",
    "        except:\n",
    "            break\n",
    "            \n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        scores = soup.select('#_review_list > li > div > div > a > span > strong')\n",
    "        reviews = soup.select('#_review_list > li > div > div.atc')\n",
    "        \n",
    "        for idx,s in enumerate(scores) :\n",
    "            sc = s.text.strip()\n",
    "            scorebox.append(sc)\n",
    "        for r in reviews :\n",
    "            rv = r.text.strip()\n",
    "            reviewbox.append(rv)\n",
    "\n",
    "df = pd.DataFrame({'review' : reviewbox, 'score' : scorebox})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport pandas as pd\\nfrom selenium import webdriver\\nfrom bs4 import BeautifulSoup\\nimport re\\n\\ndriver = webdriver.Chrome()\\ndriver.implicitly_wait(10) # 암묵적으로 웹 자원 로드를 위해 10초까지 기다려 준다.\\n#ulr접근\\ndriver.get(\\'https://search.shopping.naver.com/detail/detail.nhn?nv_mid=11358374022&cat_id=50000445&frm=&query=&NaPm=ct%3Djw79pzoo%7Cci%3Dd075b68850622fe0fbdf8dd99921e77adce8f3e6%7Ctr%3Dslsl%7Csn%3D95694%7Chk%3D07b0251e19fc2778129b73f7c1e6befb680d165e\\')\\n\\nscorebox=[]\\nreviewbox=[]\\nfor i in range(1,25) :\\n\\n    try:\\n        #if i % 11 == 1: \\n        if i % 10 == 0:\\n            driver.find_element_by_xpath(\\'//*[@id=\"_review_paging\"]/a[\\'+str(i)+\\']\\').click()\\n            driver.find_element_by_css_selector(\\'a.next\\').click()\\n        else :\\n            driver.find_element_by_xpath(\\'//*[@id=\"_review_paging\"]/a[\\'+str(i)+\\']\\').click()\\n    except:\\n        break\\n    \\n    \\n    html = driver.page_source\\n    soup = BeautifulSoup(html, \\'html.parser\\')\\n    scores = soup.select(\\'#_review_list > li > div > div > a > span > strong\\')\\n    reviews = soup.select(\\'#_review_list > li > div > div.atc\\')\\n    \\n    for idx,s in enumerate(scores) :\\n        sc = s.text.strip()\\n        scorebox.append(sc)\\n    for r in reviews :\\n        rv = r.text.strip()\\n        reviewbox.append(rv)\\n\\ndf = pd.DataFrame({\\'review\\' : reviewbox, \\'score\\' : scorebox})\\ndf\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### 실패 \n",
    "'''\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.implicitly_wait(10) # 암묵적으로 웹 자원 로드를 위해 10초까지 기다려 준다.\n",
    "#ulr접근\n",
    "driver.get('https://search.shopping.naver.com/detail/detail.nhn?nv_mid=11358374022&cat_id=50000445&frm=&query=&NaPm=ct%3Djw79pzoo%7Cci%3Dd075b68850622fe0fbdf8dd99921e77adce8f3e6%7Ctr%3Dslsl%7Csn%3D95694%7Chk%3D07b0251e19fc2778129b73f7c1e6befb680d165e')\n",
    "\n",
    "scorebox=[]\n",
    "reviewbox=[]\n",
    "for i in range(1,25) :\n",
    "\n",
    "    try:\n",
    "        #if i % 11 == 1: \n",
    "        if i % 10 == 0:\n",
    "            driver.find_element_by_xpath('//*[@id=\"_review_paging\"]/a['+str(i)+']').click()\n",
    "            driver.find_element_by_css_selector('a.next').click()\n",
    "        else :\n",
    "            driver.find_element_by_xpath('//*[@id=\"_review_paging\"]/a['+str(i)+']').click()\n",
    "    except:\n",
    "        break\n",
    "    \n",
    "    \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    scores = soup.select('#_review_list > li > div > div > a > span > strong')\n",
    "    reviews = soup.select('#_review_list > li > div > div.atc')\n",
    "    \n",
    "    for idx,s in enumerate(scores) :\n",
    "        sc = s.text.strip()\n",
    "        scorebox.append(sc)\n",
    "    for r in reviews :\n",
    "        rv = r.text.strip()\n",
    "        reviewbox.append(rv)\n",
    "\n",
    "df = pd.DataFrame({'review' : reviewbox, 'score' : scorebox})\n",
    "df\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport pandas as pd\\nfrom selenium import webdriver\\nfrom bs4 import BeautifulSoup\\nimport re\\n\\ndriver = webdriver.Chrome()\\ndriver.implicitly_wait(10) # 암묵적으로 웹 자원 로드를 위해 10초까지 기다려 준다.\\n#ulr접근\\ndriver.get(\\'https://search.shopping.naver.com/detail/detail.nhn?nv_mid=11358374022&cat_id=50000445&frm=&query=&NaPm=ct%3Djw79pzoo%7Cci%3Dd075b68850622fe0fbdf8dd99921e77adce8f3e6%7Ctr%3Dslsl%7Csn%3D95694%7Chk%3D07b0251e19fc2778129b73f7c1e6befb680d165e\\')\\n\\nscorebox=[]\\nreviewbox=[]\\n\\nfor i in range(1,10) :\\n    #driver.find_element_by_xpath(\\'//*[@id=\"_review_paging\"]/a[\\'+str(i)+\\']\\').click()\\n    #driver.find_element_by_xpath(\\'#_review_paging > a:nth-child(\\'+str(i)+\\')\\').click()\\n    driver.find_element_by_css_selector(\\'#_review_paging > a:nth-child(\\'+str(i)+\\')\\').click()\\n    \\n    html = driver.page_source\\n    soup = BeautifulSoup(html, \\'html.parser\\')\\n    scores = soup.select(\\'#_review_list > li > div > div > a > span > strong\\')\\n    reviews = soup.select(\\'#_review_list > li > div > div.atc\\')\\n    \\n    for idx,s in enumerate(scores) :\\n        sc = s.text.strip()\\n        scorebox.append(sc)\\n    for r in reviews :\\n        rv = r.text.strip()\\n        reviewbox.append(rv)\\n        \\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.implicitly_wait(10) # 암묵적으로 웹 자원 로드를 위해 10초까지 기다려 준다.\n",
    "#ulr접근\n",
    "driver.get('https://search.shopping.naver.com/detail/detail.nhn?nv_mid=11358374022&cat_id=50000445&frm=&query=&NaPm=ct%3Djw79pzoo%7Cci%3Dd075b68850622fe0fbdf8dd99921e77adce8f3e6%7Ctr%3Dslsl%7Csn%3D95694%7Chk%3D07b0251e19fc2778129b73f7c1e6befb680d165e')\n",
    "\n",
    "scorebox=[]\n",
    "reviewbox=[]\n",
    "\n",
    "for i in range(1,10) :\n",
    "    #driver.find_element_by_xpath('//*[@id=\"_review_paging\"]/a['+str(i)+']').click()\n",
    "    #driver.find_element_by_xpath('#_review_paging > a:nth-child('+str(i)+')').click()\n",
    "    driver.find_element_by_css_selector('#_review_paging > a:nth-child('+str(i)+')').click()\n",
    "    \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    scores = soup.select('#_review_list > li > div > div > a > span > strong')\n",
    "    reviews = soup.select('#_review_list > li > div > div.atc')\n",
    "    \n",
    "    for idx,s in enumerate(scores) :\n",
    "        sc = s.text.strip()\n",
    "        scorebox.append(sc)\n",
    "    for r in reviews :\n",
    "        rv = r.text.strip()\n",
    "        reviewbox.append(rv)\n",
    "        \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# 민지 \\n\\nfrom selenium import webdriver\\ndriver = webdriver.Chrome()\\ndriver.implicitly_wait(10)\\ndriver.get('https://search.shopping.naver.com/detail/detail.nhn?nv_mid=7053369760&cat_id=50000445&frm=NVSHATC&query=%EC%96%B4%ED%93%A8+%ED%93%A8%EC%96%B4+%EB%B8%94%EB%A1%9D+%EB%82%B4%EC%B6%94%EB%9F%B4+%EB%8D%B0%EC%9D%BC%EB%A6%AC+%EC%84%A0%ED%81%AC%EB%A6%BC&NaPm=ct%3Djwe9t2m8%7Cci%3D3d29944aadcf2d9caf591794cf59ce30b163c930%7Ctr%3Dslsl%7Csn%3D95694%7Chk%3Da3e12d01204b0472c67401205f3f655ffa1702f9')\\nimport pandas as pd\\nscorebox2=[]\\nreviewbox2=[]\\nfrom selenium import webdriver\\nfrom bs4 import BeautifulSoup\\nimport re\\nfor i in range(1,12) :\\n    if i == 1 :\\n        driver.find_element_by_css_selector('#_review_paging > strong').click()\\n    else :\\n            driver.find_element_by_css_selector('#_review_paging > a:nth-child('+str(i)+')').click()\\n    html = driver.page_source\\n    soup = BeautifulSoup(html, 'html.parser')\\n    scores = soup.select('#_review_list > li > div > div > a > span > strong')\\n    reviews = soup.select('#_review_list > li > div > div.atc')\\n    for idx,s in enumerate(scores) :\\n        sc = s.text.strip()\\n        scorebox2.append(sc)\\n    for r in reviews :\\n        rv = r.text.strip()\\n        reviewbox2.append(rv)\\n#_review_paging > strong\\n\\nwhile (True) :\\n    for i in range(3,14) :\\n        if i == 3 :\\n            driver.find_element_by_css_selector('#_review_paging > strong').click()\\n        else :\\n                driver.find_element_by_css_selector('#_review_paging > a:nth-child('+str(i)+')').click()\\n        html = driver.page_source\\n        soup = BeautifulSoup(html, 'html.parser')\\n        scores = soup.select('#_review_list > li > div > div > a > span > strong')\\n        reviews = soup.select('#_review_list > li > div > div.atc')\\n        for s in scores :\\n            sc = s.text.strip()\\n            scorebox2.append(sc)\\n        for r in reviews :\\n            rv = r.text.strip()\\n            reviewbox2.append(rv)\\n            \\n\\ndf2 = pd.DataFrame({'review' : reviewbox2, 'score' : scorebox2})\\ndf2 = df2.drop_duplicates()\\ndf2\\n\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# 민지 \n",
    "\n",
    "from selenium import webdriver\n",
    "driver = webdriver.Chrome()\n",
    "driver.implicitly_wait(10)\n",
    "driver.get('https://search.shopping.naver.com/detail/detail.nhn?nv_mid=7053369760&cat_id=50000445&frm=NVSHATC&query=%EC%96%B4%ED%93%A8+%ED%93%A8%EC%96%B4+%EB%B8%94%EB%A1%9D+%EB%82%B4%EC%B6%94%EB%9F%B4+%EB%8D%B0%EC%9D%BC%EB%A6%AC+%EC%84%A0%ED%81%AC%EB%A6%BC&NaPm=ct%3Djwe9t2m8%7Cci%3D3d29944aadcf2d9caf591794cf59ce30b163c930%7Ctr%3Dslsl%7Csn%3D95694%7Chk%3Da3e12d01204b0472c67401205f3f655ffa1702f9')\n",
    "import pandas as pd\n",
    "scorebox2=[]\n",
    "reviewbox2=[]\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "for i in range(1,12) :\n",
    "    if i == 1 :\n",
    "        driver.find_element_by_css_selector('#_review_paging > strong').click()\n",
    "    else :\n",
    "            driver.find_element_by_css_selector('#_review_paging > a:nth-child('+str(i)+')').click()\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    scores = soup.select('#_review_list > li > div > div > a > span > strong')\n",
    "    reviews = soup.select('#_review_list > li > div > div.atc')\n",
    "    for idx,s in enumerate(scores) :\n",
    "        sc = s.text.strip()\n",
    "        scorebox2.append(sc)\n",
    "    for r in reviews :\n",
    "        rv = r.text.strip()\n",
    "        reviewbox2.append(rv)\n",
    "#_review_paging > strong\n",
    "\n",
    "while (True) :\n",
    "    for i in range(3,14) :\n",
    "        if i == 3 :\n",
    "            driver.find_element_by_css_selector('#_review_paging > strong').click()\n",
    "        else :\n",
    "                driver.find_element_by_css_selector('#_review_paging > a:nth-child('+str(i)+')').click()\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        scores = soup.select('#_review_list > li > div > div > a > span > strong')\n",
    "        reviews = soup.select('#_review_list > li > div > div.atc')\n",
    "        for s in scores :\n",
    "            sc = s.text.strip()\n",
    "            scorebox2.append(sc)\n",
    "        for r in reviews :\n",
    "            rv = r.text.strip()\n",
    "            reviewbox2.append(rv)\n",
    "            \n",
    "\n",
    "df2 = pd.DataFrame({'review' : reviewbox2, 'score' : scorebox2})\n",
    "df2 = df2.drop_duplicates()\n",
    "df2\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 다른 방법으로 리뷰 내용 불러오기\\n\\nfrom selenium import webdriver\\ndriver = webdriver.Chrome()\\ndriver.implicitly_wait(10)\\ndriver.get(\\'https://search.shopping.naver.com/detail/detail.nhn?nv_mid=11358374022&cat_id=50000445&frm=&query=&NaPm=ct%3Djw79pzoo%7Cci%3Dd075b68850622fe0fbdf8dd99921e77adce8f3e6%7Ctr%3Dslsl%7Csn%3D95694%7Chk%3D07b0251e19fc2778129b73f7c1e6befb680d165e\\')\\n\\ndriver.find_element_by_xpath(\\'//*[@id=\"_review_paging\"]/a[\\'+str(1)+\\']\\').click() #페이지변화\\nhtml = driver.page_source # page_source 모두 가져오기 \\nsoup = BeautifulSoup(html, \\'html.parser\\') # html 파일 형식을 볼거라고 지정\\nscores = soup.select(\\'span[class=curr_avg]\\')\\nreviews = soup.select(\\'div[class=atc]\\')\\n\\nfor i in reviews:\\n    print(i.text.strip())\\n#for j in scores:\\n#    print(j.text.strip())\\n\\n'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# 다른 방법으로 리뷰 내용 불러오기\n",
    "\n",
    "from selenium import webdriver\n",
    "driver = webdriver.Chrome()\n",
    "driver.implicitly_wait(10)\n",
    "driver.get('https://search.shopping.naver.com/detail/detail.nhn?nv_mid=11358374022&cat_id=50000445&frm=&query=&NaPm=ct%3Djw79pzoo%7Cci%3Dd075b68850622fe0fbdf8dd99921e77adce8f3e6%7Ctr%3Dslsl%7Csn%3D95694%7Chk%3D07b0251e19fc2778129b73f7c1e6befb680d165e')\n",
    "\n",
    "driver.find_element_by_xpath('//*[@id=\"_review_paging\"]/a['+str(1)+']').click() #페이지변화\n",
    "html = driver.page_source # page_source 모두 가져오기 \n",
    "soup = BeautifulSoup(html, 'html.parser') # html 파일 형식을 볼거라고 지정\n",
    "scores = soup.select('span[class=curr_avg]')\n",
    "reviews = soup.select('div[class=atc]')\n",
    "\n",
    "for i in reviews:\n",
    "    print(i.text.strip())\n",
    "#for j in scores:\n",
    "#    print(j.text.strip())\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom selenium import webdriver\\nfrom bs4 import BeautifulSoup\\n\\nfrom selenium import webdriver\\ndriver = webdriver.Chrome()\\ndriver.implicitly_wait(10)\\ndriver.get(\\'https://search.shopping.naver.com/detail/detail.nhn?nv_mid=11358374022&cat_id=50000445&frm=&query=&NaPm=ct%3Djw79pzoo%7Cci%3Dd075b68850622fe0fbdf8dd99921e77adce8f3e6%7Ctr%3Dslsl%7Csn%3D95694%7Chk%3D07b0251e19fc2778129b73f7c1e6befb680d165e\\')\\n\\nfor i in range(1,15) :\\n    driver.find_element_by_xpath(\\'//*[@id=\"_review_paging\"]/a[\\'+str(i)+\\']\\').click() #페이지변화하기\\n    html = driver.page_source # page_source에 코드가 저장되어 있음\\n    soup = BeautifulSoup(html, \\'html.parser\\') # html 파일열기\\n    #reviews = soup.select(\\'#_review_list\\')\\n    scores = soup.select(\\'span[class=curr_avg]\\')\\n    reviews = soup.select(\\'div[class=atc]\\')\\n    print(reviews)\\n    #score = []\\n    #review = []\\n    #for n in reviews:\\n    #    review.append(n.text.strip())\\n    \\n'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "driver = webdriver.Chrome()\n",
    "driver.implicitly_wait(10)\n",
    "driver.get('https://search.shopping.naver.com/detail/detail.nhn?nv_mid=11358374022&cat_id=50000445&frm=&query=&NaPm=ct%3Djw79pzoo%7Cci%3Dd075b68850622fe0fbdf8dd99921e77adce8f3e6%7Ctr%3Dslsl%7Csn%3D95694%7Chk%3D07b0251e19fc2778129b73f7c1e6befb680d165e')\n",
    "\n",
    "for i in range(1,15) :\n",
    "    driver.find_element_by_xpath('//*[@id=\"_review_paging\"]/a['+str(i)+']').click() #페이지변화하기\n",
    "    html = driver.page_source # page_source에 코드가 저장되어 있음\n",
    "    soup = BeautifulSoup(html, 'html.parser') # html 파일열기\n",
    "    #reviews = soup.select('#_review_list')\n",
    "    scores = soup.select('span[class=curr_avg]')\n",
    "    reviews = soup.select('div[class=atc]')\n",
    "    print(reviews)\n",
    "    #score = []\n",
    "    #review = []\n",
    "    #for n in reviews:\n",
    "    #    review.append(n.text.strip())\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 구조형 잘 몰라서 이것저것 해봄  \n",
    "'''\n",
    "#1\n",
    "for i in range(len(reviews)):\n",
    "    print(scores[i].get_text(), reviews[i].get_text()) \n",
    "\n",
    "#2  \n",
    "drz_review = {}\n",
    "for i in score:\n",
    "    for j in review:\n",
    "        drz_review[i] = j\n",
    "print(drz_review)\n",
    "\n",
    "#3\n",
    "import pandas as pd\n",
    "df = pd.concat([score,review],axis =1)\n",
    "print(df)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
